{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e21bf4",
   "metadata": {},
   "source": [
    "# Populate MinIO Bucket with a list of URLs to Partition Text Datasets using Unstructured-IO\n",
    "\n",
    "The script automates fetching text from URLs, processes it, and uploads to a MinIO bucket. It sanitizes URLs for storage, partitions web content to structured text, and manages temporary files for clean-up. Error handling and feedback are included for process transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7497688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from minio import Minio\n",
    "import os\n",
    "import tempfile\n",
    "import re\n",
    "from unstructured.partition.auto import partition\n",
    "import io\n",
    "\n",
    "def sanitize_url_to_object_name(url):\n",
    "    clean_url = re.sub(r'^https?://', '', url)\n",
    "    clean_url = re.sub(r'[^\\w\\-_\\.]', '_', clean_url)\n",
    "    return clean_url[:250] + '.txt'\n",
    "\n",
    "def prepare_text_for_tokenization(text):\n",
    "    # Simple placeholder for text cleaning logic\n",
    "    clean_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return clean_text\n",
    "\n",
    "minio_client = Minio(\"cda-DESKTOP:9000\", access_key=\"cda_cdaprod\", secret_key=\"cda_cdaprod\", secure=False)\n",
    "bucket_name = \"cda-datasets\"\n",
    "\n",
    "urls = [\n",
    "    \"https://nanonets.com/blog/langchain/amp/\",\n",
    "    \"https://www.sitepoint.com/langchain-python-complete-guide/\",\n",
    "    \"https://medium.com/@aisagescribe/langchain-101-a-comprehensive-introduction-guide-7a5db81afa49\",\n",
    "    \"https://blog.min.io/minio-langchain-tool\",\n",
    "    \"https://quickaitutorial.com/langgraph-create-your-hyper-ai-agent/\",\n",
    "    \"https://python.langchain.com/docs/langserve\",\n",
    "    \"https://python.langchain.com/docs/expression_language/interface\",\n",
    "    \"https://blog.min.io/minio-langchain-tool\",\n",
    "    \"https://python.langchain.com/docs/langgraph\",\n",
    "    \"https://www.33rdsquare.com/langchain/\",\n",
    "    \"https://medium.com/widle-studio/building-ai-solutions-with-langchain-and-node-js-a-comprehensive-guide-widle-studio-4812753aedff\", \"https://blog.min.io/\", \"https://sanity.cdaprod.dev/\"]\n",
    "\n",
    "\n",
    "if not minio_client.bucket_exists(bucket_name):\n",
    "    minio_client.make_bucket(bucket_name)\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises an error for bad status\n",
    "\n",
    "        html_content = io.BytesIO(response.content)\n",
    "        partitioned_elements = partition(file=html_content, content_type=\"text/html\")\n",
    "        combined_text = \"\"\n",
    "\n",
    "        for element in partitioned_elements:\n",
    "            if hasattr(element, 'text'):\n",
    "                combined_text += element.text + \"\\n\\n\"\n",
    "\n",
    "        combined_text = prepare_text_for_tokenization(combined_text)\n",
    "        object_name = sanitize_url_to_object_name(url)\n",
    "\n",
    "        # Using tempfile to automatically handle file creation and deletion\n",
    "        with tempfile.NamedTemporaryFile(delete=False, mode=\"w\", encoding=\"utf-8\", suffix=\".txt\") as tmp_file:\n",
    "            tmp_file.write(combined_text)\n",
    "            tmp_file_path = tmp_file.name\n",
    "            print(f\"OK - Successfully created {object_name}\")\n",
    "\n",
    "        minio_client.fput_object(bucket_name, object_name, tmp_file_path)\n",
    "        os.remove(tmp_file_path)  # Ensure deletion if delete=False\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error for {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74ed2f",
   "metadata": {},
   "source": [
    "# Hydrate Weaviate with Text Datasets stored in MinIO Bucket\n",
    "\n",
    "#### Key Points:\n",
    "\n",
    "- This script directly executes each step without defining classes or functions. It downloads PDF files from MinIO, processes them to extract text, and stores this text in Weaviate.\n",
    "- Error handling, logging, and detailed text extraction logic are omitted for brevity but should be considered in a production environment.\n",
    "- Adjust the bucket_name, MinIO, and Weaviate endpoints and credentials as necessary for your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c749b2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\weaviate\\warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from minio import Minio\n",
    "import weaviate\n",
    "from unstructured.partition.auto import partition\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# MinIO setup\n",
    "minio_client = Minio(\n",
    "    \"192.168.0.25:9000\",\n",
    "    access_key=\"cda_cdaprod\",\n",
    "    secret_key=\"cda_cdaprod\",\n",
    "    secure=False\n",
    ")\n",
    "bucket_name = \"cda-datasets\"\n",
    "\n",
    "# Weaviate setup\n",
    "client = weaviate.Client(\"http://192.168.0.25:8080\")\n",
    "\n",
    "# List and download PDFs from MinIO\n",
    "for obj in minio_client.list_objects(bucket_name, recursive=True):\n",
    "    if obj.object_name.endswith('.txt'):\n",
    "        file_path = f\"{obj.object_name}\"\n",
    "        minio_client.fget_object(bucket_name, obj.object_name, file_path)\n",
    "        \n",
    "        # Process each PDF with Unstructured\n",
    "        elements = partition(filename=file_path)\n",
    "        \n",
    "        # Extract text (simplified logic, replace with actual extraction logic)\n",
    "        extracted_texts = [e.text for e in elements if hasattr(e, 'text')]\n",
    "        text_content = \"\\n\".join(extracted_texts)\n",
    "        \n",
    "        # Store the extracted text in Weaviate (simplified to just storing the content as text)\n",
    "        data_object = {\n",
    "            \"source\": obj.object_name,\n",
    "            \"content\": text_content\n",
    "        }\n",
    "        \n",
    "        # Insert data into Weaviate, assuming 'Document' class exists\n",
    "        client.data_object.create(data_object, \"Document\")\n",
    "        \n",
    "        # Optional: Clean up by removing the downloaded PDF\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc1cfd",
   "metadata": {},
   "source": [
    "# Query the Weaviate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a451b12",
   "metadata": {},
   "source": [
    "#### Query #1\n",
    "This will return all documents in the Document class, displaying their source and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6341f48",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: blog.min.io_author_david-cannan_.txt\n",
      "Content: Topics All Architect's Guide Operator's Guide Best Practices AI/ML Modern Data Lakes Performance Kubernetes Integrations Benchmarks Security Multicloud Try the Erasure Code Calculator to configure your usable capacity Try Now Developing Langchain Agents with the MinIO SDK for LLM Tool-Use David Cannan David Cannan on AI/ML Explore Langchain’s LLM Tool-Use and leverage Langgraph for monitoring MinIO’s S3 Object Store. This guide walks you through developing custom conversational AI agents and creating powerful OpenAI LLM chains for efficient data management and enhanced application functionality. Read more... Powering AI/ML workflows with GitOps Automation David Cannan David Cannan on AI/ML Explore the fusion of GitOps, MinIO, Weaviate, and Python in AI development for unparalleled automation and innovation. This combination offers a solid foundation for creating scalable, efficient, and automated AI solutions, propelling projects from concept to reality with ease. Read more... Backing Up Weaviate with MinIO S3 Buckets David Cannan David Cannan on AI/ML Explore integrating MinIO with Weaviate using Docker Compose for AI-enhanced data management. Learn to back up Weaviate to MinIO S3 buckets, ensuring data integrity and scalability with practical Docker and Python examples. Streamline your AI-driven search and analysis with this robust setup. Read more...\n",
      "Innovating S3 Bucket Retrieval: Langchain Community S3 Loaders with OpenAI API David Cannan David Cannan on AI/ML Explore the synergy of MinIO, Langchain, and OpenAI in enhancing data storage and processing. This article illustrates MinIO’s integration for efficient document summarization using Langchain and OpenAI’s GPT, revolutionizing AI and ML data handling. Read more... Event-Driven Architecture: MinIO Event Notification Webhooks using Flask David Cannan David Cannan on Events Explore deploying MinIO and Flask with Docker-compose for event-driven architecture. Master MinIO bucket events and Flask webhooks for efficient data workflows and robust applications. Dive into the synergy of cloud technologies. Read more... Streamlining Data Events with MinIO and PostgreSQL David Cannan David Cannan on Events Explore 'Streamlining Data Events with MinIO and PostgreSQL,' a guide for developers using Docker, MinIO, and PostgreSQL. Learn about using Docker Compose for real-time data events, enhancing data analytics, and developing robust, event-driven applications. Read more... Smooth Sailing from Docker to Localhost David Cannan David Cannan on Docker Explore the integration of Dockerized MinIO with localhost Flask apps. This guide addresses Docker networking challenges, ensuring seamless MinIO and Flask communication for a development environment that closely mirrors production. Dive into practical solutions for robust workflows. Read more...\n",
      "\n",
      "Source: www.33rdsquare.com_langchain_.txt\n",
      "Content: LangChain: The Complete Guide for AI Developers December 22, 2023 by Jordan Brown LangChain is an open-source Python framework that connects large language models to external data for building informed AI applications. This comprehensive guide covers what LangChain provides, underlying concepts, use cases, performance analysis, current limitations and more. Introduction to LangChain Released in 2022, LangChain allows developers to connect large language models like GPT-3, BLOOM and Codex to external knowledge sources like databases, documents and proprietary data. This facilitates: Data-aware answers: Answers that combine external knowledge with language model context and semantics. Agent intelligence: Building agents that execute actions based on user input, model output, and external data state. Informed workflows: Chains that incorporate models, data processing, proxies to other systems and more to generate responses, artifacts and enact real-world changes. Initial use cases demonstrate 2-4x gains in task success rate for data heavy applications by grounding language models, reducing hallucinated output and speculation [1]. LangChain moves LLM applications beyond solely conversational to being able to query knowledge bases and take pragmatic actions. Let‘s look under the hood at how this works.\n",
      "Core Concepts A LangChain application consists of 5 key components working in conjunction: LLM Wrappers… Prompts Chains Vector Stores Agents Evolution of LangChain LangChain originated from techniques detailed in academic papers published by researchers at Anthropic – an AI safety startup that open sourced the framework [2]. In April 2022, a paper titled \"Language Model Steering through External Knowledge Retrieval\" outlined methods to connect large language models to vector databases holding document embeddings [3]. This research described: Using vector similarity search for low latency document chunk retrieval directly queriable by neural networks. Interleaving model queries with database retrievals to improve output relevance and factual accuracy over long conversations. A proposed system architecture enabling fluid incorporation of external information to \"steer\" model responses. The techniques discussed form the backbone of LangChain today. Another paper in June 2022 expanded on dynamically ordering database retrievals to maximize useful signal to models [4]. These concepts were productized into the open source Python framework LangChain in August 2022. By late 2022, LangChain saw rapid community adoption with over 5400 GitHub stars and 200+ commits from 50+ developers [5]. Multiple startups like Anthropic and Scale AI build directly on LangChain for applications, demonstrating real-world commercial value.\n",
      "Usage spansQA systems, classification models, search engines, dialog agents and more with TrailDB, Elasticsearch, FAISS, Milvus and Pinecone as popular vector store choices. This research lineage and rapid adoption underscore the transformative potential LangChain brings in connecting LLMs with external signals – let‘s see why. Architectural Deep Dive Under the surface, a LangChain deployment comprises of several key components [6]: Vectorstore Serving layer that stores vector encodings of text (embeddings) for low latency retrieval. Optimized for similarity search across millions of high dimensional vectors. Popular options include Pinecone, FAISS, Milvus. Controller Queries the vectorstore for document embeddings based on user questions and model outputs. Manages tradeoffs between precision and latency. Sampler Samples the most useful document chunks from retrieved candidates to maximize new information provided to the LLM. Prevents overloading the LLM with redundant text across retrieval steps. Optimizer Handles dynamically adjusting the number of documents retrieved per cycle to fit time constraints. Also sets the sample size and other hyperparameters like similarity threshold. Together, these components facilitate efficient incorporation of relevant external knowledge into model predictions.\n",
      "The overall workflow looks like: User provides query Query embeddings retrieved from vectorstore Embeddings ranked and filtered for novelty Corresponding document chunks sampled Text snippets concatenated to prime LLM LLM provides updated response Further query cycles initiate if needed Now let‘s see an example of building a conversational agent with LangChain. Building a Conversational Agent A key use case for LangChain is developing conversational bots that can provide knowledgeable, consistent and factual responses. This is facilitated by hooking into external data sources. Let‘s walk through a simplified architecture: Our chatbot uses: Long Short Term Memory (LSTM) Networks – Processes dialogue history to track dialog state. Extracts intent and entities. Sentiment Classifier – Detects user sentiment to guide responses. Policy Manager – Decides next action for bot based on full context. Document Vector Store – Corpus of help articles to source answers from. GPT-3 / Codex – Generates natural language responses. User queries first get processed by the NLU and state tracking modules. The policy manager selects the next action – either ask clarifying question, pull from FAQ, search documents. Relevant articles get retrieved from the vectorstore and fed to GPT-3 with dialog context to get an informed response. Generated text gets ranked and returned. By mixing reactive and knowledgeable responses, this architecture facilitates consistent and high quality conversations.\n",
      "LangChain streamlines glueing the different parts together into an integrated pipeline. Next let‘s discuss some cutting edge research projects built using LangChain. Cutting Edge Research Papers The openness and flexibility of LangChain has facilitated exciting new research into pushing LLMs to new frontiers. Here are a few sample papers and techniques unlocked: Scalable agents for dialogue modeling [7] Proposes an evaluation methodology for open ended dialogue agents Built using LangChain for incorporating common sense knowledge Demonstrates higher consistency, engagingness over baseline agents Better-Few-Shot learning for instruction following [8] Pretrains an agent on instruction following tasks using LangChain Leverages external QA systems and demonstrations to outperform baseline on new tasks Improving story generation with information filtering [9] Generates fantasy stories with narratives grounded in facts Filters retrieved information to map it appropriately into creative fiction plotlines Human evaluations show higher quality, coherency, consistency over baseline Multitasking question answering model [10] Single model able to perform multiple QA datasets with different formats LangChain provides pipeline for scalable question ingestion, document store interaction Outperforms baselines while maintaining 95% parameters of single-task models These demonstrate how LangChain facilitates innovating on top of LLMs and knowledge retrieval to push boundaries.\n",
      "Let‘s analyze quantitative performance as well. Benchmarking Performance In their paper, authors Xia et al. benchmark LangChain against traditional pipeline methods on question answering, dialogue consistency and common sense reasoning tasks [11]. Some key metrics: System Conversation Consistency QA Accuracy Common Sense Accuracy Traditional pipeline 37.2% 68.1% 83.7% LangChain 48.3% 73.2% 89.1% Across tasks, LangChain shows significant gains by avoiding compounding errors across pipeline stages. Further benchmarks by Anthropic on sales conversation success rate show 2-4x improvements over baseline. On raw compute: System Latency Throughput Model Parameters Traditional pipeline 510ms 17 q/s 125M (T5-Small) LangChain 620ms 15 q/s 125M (T5-Small) We see a moderate latency tax for improved accuracy and consistency. Parameter counts stay similar given most compute still goes to the LLM. These metrics provide confidence that LangChain can deliver meaningful accuracy and capability gains without drastic efficiency tradeoffs for many applications. But limitations remain. Limitations and Challenges While promising, LangChain has challenges to scale to enterprise grade applications: Vectorstore retrieval latency increases with size, hampering real-time use at hundreds of millions of documents. No built-in monitoring, logging or diagnostics to debug systems. Changing LLMs like GPT requires updating indexing and embeddings.\n",
      "Queries formulated poorly can fail to retrieve relevant information. Scaling to 10,000+ QPS requires specialized systems design. There are also issues intrinsic to LLMs like GPT-3: Hallucination and fact fabrication still occur without supervision. Personal beliefs and toxicity leak into responses. Limited ability to correct wrong information without retraining. Queries cost money with commercial LLMs. While areas for improvement remain, the fundamentals enable new capabilities not possible previously. Combining scalable knowledge retrieval with conversational intelligence paves the way for more useful applications. Conclusion This guide covered LangChain – its background, architecture, use cases, performance and current shortcomings. The main takeaways are: LangChain enables connecting LLMs to external data for data-aware responses. It facilitates building intelligent agents that take informed actions. Cutting edge research demonstrates capabilities not possible with standalone models. Quantitative benchmarks prove meaningful accuracy and consistency gains over baselines. There remain open challenges around scalability, monitoring and model stability. With fundamentals established here, developers should feel equipped to start building with LangChain. The opportunities to create novel applications by tying together language models, knowledge bases and external services are immense. To learn more, visit the Documentation and GitHub repo.\n",
      "Share your creations with the growing community! How useful was this post? Click on a star to rate it! Average rating 3.1 / 5. Vote count: 7 No votes so far! Be the first to rate this post. Related You May Like to Read, GPTGO: Pioneering the Future of Search Through AI Conversations Lovo AI Review: The Game-Changing Voice Solution Gamma App: A Powerful AI Presentation Tool, But Not Without Limitations Quivr AI: The Definitive Technical Guide Pushing the Boundaries of Expression with ElevenLabs Speech-to-Speech FlowGPT: The Community Platform Unlocking AI‘s Potential Monica AI: Transforming Writing and Research with Intelligent Assistance AIPRM Prompts For Sales Funnel: Tripwire Tactics Guide\n",
      "\n",
      "Source: quickaitutorial.com_langgraph-create-your-hyper-ai-agent_.txt\n",
      "Content: Home News Technology All Data ScienceMachine LearningProgramming Llmlingua + LlamaIndex + RAG = Cheaper Chatbot AutoGen + LangChian + SQLite + Function Schema = Super AI Chabot Microsoft PHI-2 + Huggine Face + Langchain = Super Tiny Chatbot TaskWeaver + Planner + Plugin = Super AI Agent OpenHermes 2.5 Vs GPT-4 Vs LLama2 = The Winner AutoGen + LangChian + RAG + Function Call = Super AI Chabot Why OpenChat Model is So Much Better Than ChatGPT? How To Build a LLava chatbot How Powerful Step-Back Prompting Transforms LLM Performance Money Ai Automation Why OpenChat Model is So Much Better Than ChatGPT? No Result Home News Technology All Data ScienceMachine LearningProgramming Llmlingua + LlamaIndex + RAG = Cheaper Chatbot AutoGen + LangChian + SQLite + Function Schema = Super AI Chabot Microsoft PHI-2 + Huggine Face + Langchain = Super Tiny Chatbot TaskWeaver + Planner + Plugin = Super AI Agent OpenHermes 2.5 Vs GPT-4 Vs LLama2 = The Winner AutoGen + LangChian + RAG + Function Call = Super AI Chabot Why OpenChat Model is So Much Better Than ChatGPT? How To Build a LLava chatbot How Powerful Step-Back Prompting Transforms LLM Performance Money Ai Automation Why OpenChat Model is So Much Better Than ChatGPT? No Result No Result ADVERTISEMENT Home Blog LangGraph : Create Your Hyper AI Agent by Gao Dalie (高達烈) February 2, 2024 in Blog SHARES 1.6k VIEWS Share on Facebook Share on Twitter ADVERTISEMENT ADVERTISEMENT LangChain has been around for a year.\n",
      "As an open-source framework, providing the modules and tools needed to build AI applications based on large models just a few days LangChain officially announced the new library called LangGraph LangGraph builds upon LangChain and simplifies the process of creating and managing agents and their runtimes. in this Post, we will introduce a comprehensive of langGraph, what are agents and agent runtimes? what is the Feature of Langgraph, and how to build an agent executor in LangGraph, we going to explore the Chat Agent Executor in LangGraph and How to modify the Chat Agent Executor in LangGraph in humans in a loop and chat Table of Contents Toggle Before we start! 🦸🏻‍♀️ So, what are agents and agent runtimes? A Key Feature how to build Agent Executor Exploring Chat Agent Executor How to modify humans in a loop Modify Managing Agent Steps Force-calling a Tool Conclusion :References : Before we start! 🦸🏻‍♀️ If you like this topic and you want to support me: Follow me on Medium and subscribe to get my latest article🫶 If you prefer video tutorials, please subscribe to my YouTube channel where I started to convert most of my articles to visual demonstrations. So, what are agents and agent runtimes? In LangChain, an agent is a system driven by a language model that makes decisions about actions to take. An agent runtime is what keeps this system running, continually deciding on actions, recording observations, and maintaining this cycle until the agent’s task is completed.\n",
      "LangChain has made agent customization easy with its expression language. LangGraph takes this further by allowing more flexible and dynamic customization of the agent runtime. The traditional agent runtime was the Agent EX class, but now with LangGraph, there’s more variety and adaptability. A Key Feature A key feature of LangGraph is the addition of cycles to the agent runtime. Unlike non-cyclical frameworks, LangGraph enables these repetitive loops essential for agent operation. We’re starting with two main agent runtimes in LangGraph: The Agent Executor is similar to LangChain’s but rebuilt in LangGraph. The Chat Agent Executor handles agent states as a list of messages — perfect for chat-based models that use messages for function calls and responses. how to build Agent Executor building an agent executor in LangGraph, similar to the one in LangChain. This process is surprisingly straightforward, so let’s dive in! First things first, we’ll need to set up our environment by installing a few packages: LangChain, LangChain OpenAI, and Tavily Python. These will help us utilize existing LangChain agent classes, power our agent with OpenAI’s language models, and use the Tavily Python package for search functionality. !pip install --quiet -U langchain langchain_openai tavily-python Next, we’ll set up our API keys for OpenAI, Tavily, and LingSmith. LingSmith is particularly important for logging and observability, but it’s currently in private beta.\n",
      "If you need access, feel free to reach out to Them. os import getpass os.environ[ \"OPENAI_API_KEY\"] = getpass.getpass( \"OpenAI API Key:\") os.environ[ \"TAVILY_API_KEY\"] = getpass.getpass( \"Tavily API Key:\") os.environ[ \"LANGCHAIN_TRACING_V2\"] = \"true\" os.environ[ \"LANGCHAIN_API_KEY\"] = getpass.getpass( \"LangSmith API Key:\") Our first step in the notebook is to create a LangChain agent. This involves selecting a language model, creating a search tool, and establishing our agent. For detailed information on this, you can refer to the LangChain documentation. from langchain import hub from langchain.agents import create_openai_functions_agent from langchain_openai.chat_models import ChatOpenAI from langchain_community.tools.tavily_search import TavilySearchResults tools = [TavilySearchResults(max_results= 1)] # Get the prompt to use - you can modify this! prompt = hub.pull( \"hwchase17/openai-functions-agent\") # Choose the LLM that will drive the agent llm = ChatOpenAI(model= \"gpt-3.5-turbo-1106\", streaming= True) # Construct the OpenAI Functions agent agent_runnable = create_openai_functions_agent(llm, tools, prompt) We then define the state of our graph, which tracks changes over time. This state allows each node in our graph to update the overall state, saving us the hassle of passing it around constantly. We’ll also decide how these updates will be applied, whether by overriding existing data or adding to it.\n",
      "from typing import TypedDict, Annotated, List, Union from langchain_core.agents import AgentAction, AgentFinish from langchain_core.messages import BaseMessage import operator class AgentState( TypedDict): # The input string input: str # The list of previous messages in the conversation chat_history: list[BaseMessage] # The outcome of a given call to the agent # Needs `None` as a valid type, since this is what this will start as agent_outcome: Union[AgentAction, AgentFinish, None] # List of actions and corresponding observations # Here we annotate this with `operator.add` to indicate that operations to # this state should be ADDED to the existing values (not overwrite it) intermediate_steps: Annotated[ list[ tuple[AgentAction, str]], operator.add] After setting up our state, we focus on defining nodes and edges in our graph. We need two primary nodes: one to run the agent and another to execute tools based on the agent’s decisions. Edges in our graph are of two types: conditional and normal. Conditional edges allow for branching paths based on previous results, while normal edges represent a fixed sequence of actions. We’ll look into specifics like the ‘run agent’ node, which invokes the agent, and the ‘execute tools’ function, which executes the tool chosen by the agent. We’ll also add a ‘should continue’ function to determine the next course of action.\n",
      "from langchain_core.agents import AgentFinish from langgraph.prebuilt.tool_executor import ToolExecutor # This a helper class we have that is useful for running tools # It takes in an agent action and calls that tool and returns the result tool_executor = ToolExecutor(tools) # Define the agent def run_agent( data): agent_outcome = agent_runnable.invoke(data) return { \"agent_outcome\": agent_outcome} # Define the function to execute tools def execute_tools( data): # Get the most recent agent_outcome - this is the key added in the `agent` above agent_action = data[ 'agent_outcome'] output = tool_executor.invoke(agent_action) return { \"intermediate_steps\": [(agent_action, str(output))]} # Define logic that will be used to determine which conditional edge to go down def should_continue( data): # If the agent outcome is an AgentFinish, then we return `exit` string # This will be used when setting up the graph to define the flow if isinstance(data[ 'agent_outcome'], AgentFinish): return \"end\" # Otherwise, an AgentAction is returned # Here we return `continue` string # This will be used when setting up the graph to define the flow else: return \"continue\" Finally, we construct our graph. We define it, add our nodes, set an entry point, and establish our edges — both conditional and normal. After compiling the graph, it’s ready to be used just like any LangChain runnable.\n",
      "from langgraph.graph import END, StateGraph # Define a new graph workflow = StateGraph(AgentState) # Define the two nodes we will cycle between workflow.add_node( \"agent\", run_agent) workflow.add_node( \"action\", execute_tools) # Set the entrypoint as `agent` # This means that this node is the first one called workflow.set_entry_point( \"agent\") # We now add a conditional edge workflow.add_conditional_edges( # First, we define the start node. We use `agent`. # This means these are the edges taken after the `agent` node is called. \"agent\", # Next, we pass in the function that will determine which node is called next. should_continue, # Finally we pass in a mapping. # The keys are strings, and the values are other nodes. # END is a special node marking that the graph should finish. # What will happen is we will call `should_continue`, and then the output of that # will be matched against the keys in this mapping. # Based on which one it matches, that node will then be called. { # If `tools`, then we call the tool node. \"continue\": \"action\", # Otherwise we finish. \"end\": END } ) # We now add a normal edge from `tools` to `agent`. # This means that after `tools` is called, `agent` node is called next. workflow.add_edge( 'action', 'agent') # Finally, we compile it! # This compiles it into a LangChain Runnable, # meaning you can use it as you would any other runnable app = workflow. compile() We’ll run our executor with some input data to see our executor in action.\n",
      "This process involves streaming the results of each node, allowing us to observe the agent’s decisions, the tools executed, and the overall state at each step. \"input\": \"what is the weather in sf\", \"chat_history\": []} for s in app.stream(inputs): print(list(s.values())[0]) print( \"----\") For a more visual understanding, we can explore these processes in LingSmith, which provides a detailed view of each step, including the prompts and responses involved in the execution. 'agent_outcome': AgentActionMessageLog(tool= 'tavily_search_results_json', tool_input={ 'query': 'weather in San Francisco'}, log= \"\\nInvoking: `tavily_search_results_json` with `{'query': 'weather in San Francisco'}`\\n\\n\\n\", message_log=[AIMessage(content= '', additional_kwargs={ 'function_call': { 'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}})])} ---- { 'intermediate_steps': [(AgentActionMessageLog(tool= 'tavily_search_results_json', tool_input={ 'query': 'weather in San Francisco'}, log= \"\\nInvoking: `tavily_search_results_json` with `{'query': 'weather in San Francisco'}`\\n\\n\\n\", message_log=[AIMessage(content= '', additional_kwargs={ 'function_call': { 'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}})]), \"[{'url': 'https://www.whereandwhen.net/when/north-america/california/san-francisco-ca/january/', 'content': 'Best time to go to San Francisco?\n",
      "Weather in San Francisco in january 2024 How was the weather last january? Here is the day by day recorded weather in San Francisco in january 2023: Seasonal average climate and temperature of San Francisco in january 8% 46% 29% 12% 8% Evolution of daily average temperature and precipitation in San Francisco in januaryWeather in San Francisco in january 2024. The weather in San Francisco in january comes from statistical datas on the past years. You can view the weather statistics the entire month, but also by using the tabs for the beginning, the middle and the end of the month. ... 16-01-2023 45°F to 52°F. 17-01-2023 45°F to 54°F. 18-01-2023 47°F to ...'}]\")]} That’s how you create an agent executor in LangGraph, mirroring the functionality of LangChain’s executor. we’ll explore more about the interface of the state graph and different streaming methods to return results. Else See : AutoGen + LangChian + SQLite + Function Schema = Super AI Chabot Exploring Chat Agent Executor we’re going to explore the Chat Agent Executor in LangGraph, a tool designed to work with chat-based models. This executor is unique because it operates entirely on a list of input messages, updating the agent’s state over time by adding new messages to this list. Let’s dive into the setup process: Installing Packages: We need the LangChain package, LangChain OpenAI for the model, and the Tavily package for the search tool. Setting API keys for these services is also necessary.\n",
      "!pip install --quiet -U langchain langchain_openai tavily-python os import getpass os.environ[ \"OPENAI_API_KEY\"] = getpass.getpass( \"OpenAI API Key:\") os.environ[ \"TAVILY_API_KEY\"] = getpass.getpass( \"Tavily API Key:\") os.environ[ \"LANGCHAIN_TRACING_V2\"] = \"true\" os.environ[ \"LANGCHAIN_API_KEY\"] = getpass.getpass( \"LangSmith API Key:\") Setting Up Tools and the Model: We’ll use Tavily Search as our tool, and set up a tool executor to invoke these tools. For the model, we’ll use the Chat OpenAI model from the LangChain integration, ensuring it’s initialized with streaming enabled. This enables us to stream back tokens and attach the functions we want the model to call. from langchain_community.tools.tavily_search import TavilySearchResults from langchain_openai import ChatOpenAI from langgraph.prebuilt import ToolExecutor from langchain.tools.render import format_tool_to_openai_function tools = [TavilySearchResults(max_results= 1)] tool_executor = ToolExecutor(tools) # We will set streaming=True so that we can stream tokens # See the streaming section for more information on this. model = ChatOpenAI(temperature= 0, streaming= True) functions = [format_tool_to_openai_function(t) for t in tools] model = model.bind_functions(functions) Defining Agent State: The agent state is a simple dictionary with a key for a list of messages. We’ll use an ‘add to’ annotation so that any updates from nodes to this messages list will accumulate over time.\n",
      "from typing import TypedDict, Annotated, Sequence import operator from langchain_core.messages import BaseMessage class AgentState( TypedDict): messages: Annotated[ Sequence[BaseMessage], operator.add] Creating Nodes and Edges: Nodes do the work, and edges connect them. We need an agent node to call the language model and get a response, an action node to see if there are any tools to be called, and a function to determine if we should proceed to tool calling or finish.\n",
      "from langgraph.prebuilt import ToolInvocation import json from langchain_core.messages import FunctionMessage # Define the function that determines whether to continue or not def should_continue( state): messages = state[ 'messages'] last_message = messages[- 1] # If there is no function call, then we finish if \"function_call\" not in last_message.additional_kwargs: return \"end\" # Otherwise if there is, we continue else: return \"continue\" # Define the function that calls the model def call_model( state): messages = state[ 'messages'] response = model.invoke(messages) # We return a list, because this will get added to the existing list return { \"messages\": [response]} # Define the function to execute tools def call_tool( state): messages = state[ 'messages'] # Based on the continue\n",
      "condition # we know the last message involves a function call last_message = messages[- 1] # We construct an ToolInvocation from the function_call action = ToolInvocation( tool=last_message.additional_kwargs[ \"function_call\"][ \"name\"], tool_input=json.loads(last_message.additional_kwargs[ \"function_call\"][ \"arguments\"]), ) # We call the tool_executor and get back a response response = tool_executor.invoke(action) # We use the response to create a FunctionMessage function_message = FunctionMessage(content= str(response), name=action.tool) # We return a list, because this will get added to the existing list return { \"messages\": [function_message]} Building the Graph: We create a graph with the agent state, add nodes for the agent and action, and set the entry point to the agent node. Conditional edges are added based on whether the agent should continue or end, and a normal edge always leads back to the agent after an action. from langgraph.graph import StateGraph, END # Define a new graph workflow = StateGraph(AgentState) # Define the two nodes we will cycle between workflow.add_node( \"agent\", call_model) workflow.add_node( \"action\", call_tool) # Set the entrypoint as `agent` # This means that this node is the first one called workflow.set_entry_point( \"agent\") # We now add a conditional edge workflow.add_conditional_edges( # First, we define the start node. We use `agent`. # This means these are the edges taken after the `agent` node is called.\n",
      "\"agent\", # Next, we pass in the function that will determine which node is called next. should_continue, # Finally we pass in a mapping. # The keys are strings, and the values are other nodes. # END is a special node marking that the graph should finish. # What will happen is we will call `should_continue`, and then the output of that # will be matched against the keys in this mapping. # Based on which one it matches, that node will then be called. { # If `tools`, then we call the tool node. \"continue\": \"action\", # Otherwise we finish. \"end\": END } ) # We now add a normal edge from `tools` to `agent`. # This means that after `tools` is called, `agent` node is called next. workflow.add_edge( 'action', 'agent') # Finally, we compile it! # This compiles it into a LangChain Runnable, # meaning you can use it as you would any other runnable app = workflow. compile() Compiling and Using the Graph: After compiling the graph, we create an input dictionary with a messages key. Running the graph will process these messages, adding AI responses, function results, and final outputs to the list of messages. from langchain_core.messages import HumanMessage inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]} app.invoke(inputs) Observing Under the Hood: Using LangSmith, we can see the detailed steps taken by our agent, including the calls made to OpenAI and the resulting outputs.\n",
      "Streaming Capabilities: LangGraph also offers streaming capabilities, which we’ll explore in more detail in video. How to modify humans in a loop let’s modify the Chat Agent Executor in LangGraph to include a ‘human in the loop’ component. This addition allows for human validation of tool actions before they are executed. We’ll build on the base notebook we’ve previously worked on. If you haven’t gone through that notebook, I recommend reviewing it first, as this video will mainly focus on the modifications we make to it. Setting Up: The initial setup remains the same. There are no additional installations needed. We’ll create our tool, set up the tool executor, prepare our model, bind tools to the model, and define the agent state — all as we did in the previous session. Key Modification — Call Tool Function: The major change comes in the call tool function. We’ve added a step where the system prompts the user (that’s you!) in the interactive IDE, asking whether to proceed with a particular action. If the user responds ‘no’, an error is thrown, and the process stops. This is our human validation step.\n",
      "# Define the function to execute tools def call_tool( state): messages = state[ 'messages'] # Based on the continue condition # we know the last message involves a function call last_message = messages[- 1] # We construct an ToolInvocation from the function_call action = ToolInvocation( tool=last_message.additional_kwargs[ \"function_call\"][ \"name\"], tool_input=json.loads(last_message.additional_kwargs[ \"function_call\"][ \"arguments\"]), ) response = input(prompt= f\"[y/n] continue with: {action}?\") if response == \"n\": raise ValueError # We call the tool_executor and get back a response response = tool_executor.invoke(action) # We use the response to create a FunctionMessage function_message = FunctionMessage(content= str(response), name=action.tool) # We return a list, because this will get added to the existing list return { \"messages\": [function_message]} Using the Modified Executor: When we run this modified executor, it will ask for approval before executing any tool action. If we approve by saying ‘yes’, it proceeds as normal. However, if we say ‘no’, it raises an error and halts the process.\n",
      "utput from node 'agent': --- { 'messages': [ AIMessage(content='', additional_kwargs={ 'function_call': { 'arguments': '{\\n \"query\": \"weather in San Francisco\"\\n}', 'name': 'tavily_search_results_json'}} )]} --- --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[10], line from langchain_core.messages import HumanMessage inputs = { \"messages\": [ HumanMessage(content=\"what is the weather in sf\")]} ----> for output in app.stream(inputs): # stream() yields dictionaries with output keyed by node name for key, value in output.items(): print(f\"Output from node '{key}' :\") This is a basic implementation. In a real-world scenario, you might want to replace the error with a more sophisticated response and use a more user-friendly interface instead of a Jupyter Notebook. But this gives a clear idea of how you can add a simple yet effective human-in-the-loop component to your LangGraph agents Modify Managing Agent Steps let’s take a look at modifying the Chat Agent Executor in LangGraph to manipulate the internal state of the agent as it processes messages. This tutorial builds on the basic Chat Agent Executor setup, so if you haven’t gone through the initial setup in the base notebook, please do that first. We’ll focus here only on the new modifications. Key Modification — Filtering Messages: The primary change we’re introducing is a way to filter the messages passed to the model.\n",
      "You can now customize which messages the agent considers. For instance: 'messages'][- 5:] response = model.invoke(messages) # We return a list, because this will get added to the existing list return { \"messages\": [response]} Selecting only the five most recent messages. Including the system message plus the five latest messages. Summarizing messages that are older than the five most recent ones. This modification is a minor but powerful addition, allowing you to control how the agent interacts with its message history and improves its decision-making process. Using the Modified Executor: The implementation is straightforward. You won’t see a difference with just one input message, but the essential part is that any logic you wish to apply to the agent’s steps can be inserted into this new modification section. This method is ideal for modifying the Chat Agent Executor, but the same principle applies if you’re working with a standard agent executor. Force-calling a Tool we’ll be making a simple but effective modification to the Chat Agent Executor in LangGraph, ensuring that a tool is always called first. This builds on the base Chat Agent Executor notebook, so make sure you’ve checked that out for background information. Key Modification — Forcing a Tool Call First: Our focus here is on setting up the chat agent to call a specific tool as its first action. To do this, we’ll add a new node, which we’ll name ‘first model node’.\n",
      "This node will be programmed to return a message instructing the agent to call a particular tool, such as the ‘Tavil search results Json’ tool, with the most recent message content as the query. # This is the new first - the first call of the model we want to explicitly hard-code some action from langchain_core.messages import AIMessage import json def first_model( state): human_input = state[ 'messages'][- 1].content return { \"messages\": [ AIMessage( content= \"\", additional_kwargs={ \"function_call\": { \"name\": \"tavily_search_results_json\", \"arguments\": json.dumps({ \"query\": human_input}) } } ) ] } Updating the Graph: We’ll modify our existing graph to include this new ‘first agent’ node as the entry point. This ensures that the first agent node is always called first, followed by the action node. We set up a conditional node from the agent to the action or end, and a direct node from the action back to the agent. The crucial addition is a new node from the first agent to action, guaranteeing that the tool call happens right at the start.\n",
      "from langgraph.graph import StateGraph, END # Define a new graph workflow = StateGraph(AgentState) # Define the new entrypoint workflow.add_node( \"first_agent\", first_model) # Define the two nodes we will cycle between workflow.add_node( \"agent\", call_model) workflow.add_node( \"action\", call_tool) # Set the entrypoint as `agent` # This means that this node is the first one called workflow.set_entry_point( \"first_agent\") # We now add a conditional edge workflow.add_conditional_edges( # First, we define the start node. We use `agent`. # This means these are the edges taken after the `agent` node is called. \"agent\", # Next, we pass in the function that will determine which node is called next. should_continue, # Finally we pass in a mapping. # The keys are strings, and the values are other nodes. # END is a special node marking that the graph should finish. # What will happen is we will call `should_continue`, and then the output of that # will be matched against the keys in this mapping. # Based on which one it matches, that node will then be called. { # If `tools`, then we call the tool node. \"continue\": \"action\", # Otherwise we finish. \"end\": END } ) # We now add a normal edge from `tools` to `agent`. # This means that after `tools` is called, `agent` node is called next. workflow.add_edge( 'action', 'agent') # After we call the first agent, we know we want to go to action workflow.add_edge( 'first_agent', 'action') # Finally, we compile it!\n",
      "# This compiles it into a LangChain Runnable, # meaning you can use it as you would any other runnable app = workflow. compile() Using the Modified Executor: When we run this updated executor, the first result comes back quickly because we bypass the initial language model call and go straight to invoking the tool. This is confirmed by observing the process in LangSmith, where we can see that the tool is the first thing invoked, followed by a language model call at the end. This modification is a simple yet powerful way to ensure that specific tools are utilized immediately in your chat agent’s workflow. Else See: CrewAi + Solor/Hermes + Langchain + Ollama = Super Ai Agent Conclusion : And that’s a wrap! you know how to build a hyper AI agent. I hope you have gained a cursory understanding of langGraph capabilities. As a next step, try exploring LangGraph to build more interesting applications.\n",
      "References : https://github.com/langchain-ai/langgraph/blob/main/examples/agent_executor/base.ipynb https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/base.ipynb https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/human-in-the-loop.ipynb https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/managing-agent-steps.ipynb https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/force-calling-a-tool-first.ipynb Previous Post CrewAi + Solar/Hermes + Langchain + Ollama = Super Ai Agent Next Post What Is Langchain 0.1.0 ? Explained Simply Gao Dalie (高達烈) Gao Dalie (高達烈) he is a data driven enthusiast deeply passioned about Data Science, Automation, and Artificial Intelligence. AS a top writer on Medium in the Artificial Intelligence category ,He is also the founder of QuickAITutorial. For more of his insights, follow him on Medium @mr.tarik098. Next Post What Is Langchain 0.1.0 ? Explained Simply Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment * Name * Email * Website Save my name, email, and website in this browser for the next time I comment.\n",
      "You might also like LangGraph + Corrective RAG + Local LLM = Powerful Rag Chatbot 2 weeks ago Five Technique : VLLM + Torch + Flash_Attention =Super Local LLM 3 weeks ago [ollama libraries 🦙] Run Any Chatbot Free Locally on Your Computer 3 weeks ago LangGraph + Gemini Pro + Custom Tool + Streamlit = Multi-Agent Application Development 4 weeks ago What Is Langchain 0.1.0 ? Explained Simply 1 month ago LangGraph : Create Your Hyper AI Agent 1 month ago Quick AI Tutorial is your gateway to the world of Data Science, Machine Learning, and Artificial Intelligence. Dive into our expertly crafted articles to gain insights, master cutting-edge techniques, and discover lucrative opportunities in the digital realm. Stay Connected More… About Us Contact Us Policy Privacy Cookies Policy Home News Technology Money Ai Automation © 2023 quickaitutorial - Premium WordPress news & magazine theme by quickaitutorial. No Result Home News Technology Money Ai Automation © 2023 quickaitutorial - Premium WordPress news & magazine theme by quickaitutorial.\n",
      "\n",
      "Source: blog.min.io_minio-langchain-tool.txt\n",
      "Content: Topics All Architect's Guide Operator's Guide Best Practices AI/ML Modern Data Lakes Performance Kubernetes Integrations Benchmarks Security Multicloud Try the Erasure Code Calculator to configure your usable capacity Try Now Developing Langchain Agents with the MinIO SDK for LLM Tool-Use David Cannan David Cannan on AI/ML Share: Linkedin X (Twitter) Reddit Copy Article Link Email Article Follow: LinkedIn Reddit In my previous article on Langchain, I explored the use of the “community S3 loaders”, while useful, offer limited functionality. Here we delve into the development of customized tools (focusing on MinIO object upload for this demonstration) and their integration with Large Language Models (LLMs) through Langchain Agents and Executors. This demonstration showcases the process of uploading objects to a bucket, leveraging the MinIO Python SDK and Langchain as the foundational tools. This exploration opens up new possibilities, moving beyond basic loaders to a more versatile and powerful implementation that enhances the capabilities of language model-driven applications. This strategic move towards combining sophisticated language models with robust data storage capabilities, is an evolution that enhances the functionality of language-driven applications, providing a pathway to advanced tool utilization with Large Language Models (LLMs).\n",
      "Utilizing MinIO's secure and scalable object storage in conjunction with Langchain's ability to leverage the full capabilities of LLMs like OpenAI's GPT, developers can create systems that not only mimic human text generation but also carry out complex tasks, elevating the efficiency of data management to new heights. Langchain serves as the critical interface that translates human instructions into the operational language of machine intelligence. Envision it as a sophisticated intermediary, capable of interpreting user commands and orchestrating a range of activities from data organization within MinIO's structures to the detailed analysis of data sets. This capability effectively converts the theoretical prowess of LLMs into tangible, functional assets within the developer's toolkit, allowing for the crafting of advanced solutions that were once considered futuristic. In this guide, we prioritize practicality, steering towards actionable development practices. We invite developers to embrace the potential that lies in the union of Langchain and MinIO SDK, to not only innovate but also to redefine the boundaries of what can be achieved with the intelligent automation of today's digital tools. The source code and detailed documentation to accompany this exploration can be found here.\n",
      "This notebook provides the minimal and necessary resources to get started on your journey with Langchain and MinIO, offering a hands-on experience to deepen your understanding and skills in creating intelligent, data-driven applications. Enhancing Conversational Agents with Contextual Memory and Advanced Data Handling Integrating memory management into Langchain applications significantly elevates their ability to deliver responses that are not only relevant but also deeply context-aware. This advancement permits an agent to draw upon past dialogues, providing a richer, more layered understanding of each interaction. The real power of this feature lies in its ability to tailor responses based on the accumulated history of the user's session, transforming standard interactions into personalized experiences that resonate more deeply with users. The inclusion of memory capabilities, especially when combined with the functionality to expose object stores as agent tools, revolutionizes the landscape of AI-driven conversational agents. Developers are bestowed with the tools to create agents that not only execute tasks with unparalleled accuracy but also evolve and adapt to users' needs through ongoing interactions. This adaptability marks a leap forward in developing interactive applications, where the agent not only responds to but anticipates user requirements, crafting a truly interactive and intuitive user experience.\n",
      "Moreover, this approach lays down a comprehensive blueprint for seamlessly merging MinIO's robust data management capabilities with Langchain's advanced processing power, offering a meticulously detailed guide for enhancing conversational agents. The result is a harmonious integration that leverages the strengths of MinIO and Langchain, offering developers a rich palette for creating applications that are as technically profound as they are user-centric. Setting Up the Environment It's crucial to begin by setting up the development environment with all necessary packages. This ensures that you have all the required libraries and dependencies installed. First install two key packages: the MinIO Python SDK and Langchain. The MinIO SDK is a powerful tool that allows us to interact with MinIO buckets, enabling operations such as file uploads, downloads, and bucket management directly from our Python scripts. On the other hand, Langchain is an innovative framework that enables the creation of applications combining large language models with specific tasks, such as file management in this case. Together, these packages form the backbone of our tool, allowing us to leverage the strengths of both MinIO's robust storage solutions and the advanced natural language processing capabilities of large language models.\n",
      "To install these packages, run the following command in your terminal: Install package dependencies This command installs the latest versions of the MinIO client and Langchain, along with all optional dependencies required for Langchain. Integrating Langsmith for Process Monitoring and Tracing (Optional) A key aspect of developing with Langchain is the ability to monitor and trace the execution of tasks, especially when integrating complex functionalities like object storage operations with MinIO. Langsmith offers an intuitive platform to visualize these processes, providing real-time insights into the performance and efficiency of your Langchain applications. Below, we’ve included screenshots from Langsmith that highlight the seamless execution of tasks, from invoking LLMs to performing specific actions such as file uploads and data processing. These visual aids not only serve to demystify the underlying processes but also showcase the practical application of Langchain and MinIO SDK in creating sophisticated, AI-driven tools. Through Langsmith, developers can gain a deeper understanding of their application’s behavior, making it easier to optimize and refine their solutions for better performance and user experience. Incorporating Langsmith into your development workflow not only enhances transparency but also empowers you to build more reliable and efficient Langchain applications.\n",
      "By leveraging these insights, you can fine-tune your applications, ensuring they meet the high standards required for production environments. To get started with Langsmith, follow these steps: 1. Create a Langsmith Project: Visit smith.langchain.com and sign up or log in to your account. Once logged in, create a new project by selecting the option to create a new project and name it “Langchain MinIO Tool”. This project will be the central hub for monitoring the interactions and performance of your Langchain integrations. 2. Generate an API Key: After creating your project, navigate to the project settings to generate a new API key. This key will authenticate your application's requests to Langsmith, ensuring secure communication between your tool and the Langsmith service. 3. Configure Environment Variables: Langsmith requires several environment variables to be set up in your development environment. These variables enable your application to communicate with Langsmith's API and send tracing data. An example of these variables includes: Exporting environment variables for Langsmith Replace <your-api-key> with the actual API key generated in the previous step. These environment variables enable the Langchain SDK in your application to send tracing and monitoring data to your Langsmith project, providing you with real-time insights into the operation and performance of your Langchain integrations.\n",
      "Initializing OpenAI and MinIO Clients for File Management The foundation of building a Langchain tool that integrates with MinIO for file uploads involves setting up the clients for both OpenAI and MinIO. This setup allows your application to communicate with OpenAI's powerful language models and MinIO's efficient file storage system. Here's how you can initialize these crucial components in Python: Setting Up the Language Model with OpenAI First, we need to initialize the language model using OpenAI's API. This step involves using the langchain_openai package to create an instance of ChatOpenAI, which will serve as our interface to OpenAI's language models. This requires an API key from OpenAI, which you can obtain from your OpenAI account. Setup llm using ChatOpenAI Replace the empty string in (api_key=\"\") with your actual OpenAI API key. This key enables authenticated requests to OpenAI, allowing you to leverage the language model for processing and generating text. Importing Necessary Libraries Before proceeding, ensure you import the necessary libraries. These imports include io for handling byte streams and tool from langchain.agents, which is a decorator used to register functions as tools that can be utilized by Langchain agents. Importing io and langchain.agents tool Initializing the MinIO Client Next, we initialize the MinIO client, which allows our application to interact with MinIO buckets for operations like uploading, downloading, and listing files.\n",
      "The MinIO client is initialized with the server endpoint, access key, secret key, and a flag to indicate whether to use a secure connection (HTTPS). Setting the minio_client In this example, we're using MinIO's play server (play.min.io:443) with the default credentials (minioadmin for both access and secret keys). In a production environment, you should replace these with your MinIO server details and credentials. By initializing the OpenAI and MinIO clients, you set the stage for developing advanced tools that can interact with natural language processing models and manage files in MinIO buckets, opening a wide range of possibilities for automating and enhancing file management tasks. Managing Bucket Availability in MinIO An essential part of working with MinIO involves managing buckets, which are the basic containers that hold your data. Before uploading files, it's important to ensure that the target bucket exists. This process involves checking the existence of a bucket and creating it if it doesn't exist. This approach not only prepares your environment for file operations but also avoids errors related to non-existent buckets. Here's a simple yet effective helper function and code snippet for managing the availability of a bucket in MinIO: Bucket helper function This code performs the following operations: 1. Define the Bucket Name: It starts by specifying the name of the bucket you want to check or create, in this case, \"test\". 2.\n",
      "Check for Bucket Existence: It uses the bucket_exists method of the MinIO client to check if the bucket already exists. 3. Create the Bucket if Necessary: If the bucket does not exist, the make_bucket method is called to create a new bucket with the specified name. 4. Handle Errors: The operation is wrapped in a “try-except block” to catch and handle any S3Error exceptions that may occur during the process, such as permission issues or network errors. By ensuring the bucket's existence before performing file operations, you can make your applications more robust and user-friendly, providing clear feedback and avoiding common pitfalls associated with bucket management in MinIO. Implementing File Upload Functionality Once the environment is configured and the necessary checks are in place to ensure that the target bucket exists, the next step is to implement the core functionality of uploading files to MinIO. This involves creating a function that takes the bucket name, the object name (file name within the bucket), and the file's binary data as inputs, and then uses the MinIO client to upload the file to the specified bucket. Here's how you can define this upload function: Python “upload” function with Langchain’s @tool decorator. Key Components of the Upload Function: Function Decorator (@tool): This decorator is used to register the function as a tool within the Langchain framework, making it callable as part of a Langchain workflow or process.\n",
      "It enables the function to be integrated seamlessly with Langchain agents and executors. Parameters: The function takes three parameters: bucket_name: The name of the MinIO bucket where the file will be uploaded. object_name: The name you wish to assign to the file within the bucket. data_bytes: The binary data of the file to be uploaded. Creating a Byte Stream: The binary data (data_bytes) is wrapped in a BytesIO stream. This is necessary because the MinIO client's put_object method expects a stream of data rather than raw bytes. Uploading the File: The put_object method of the MinIO client is called with the bucket name, object name, data stream, and the length of the data. This method handles the upload process, storing the file in the specified bucket under the given object name. Return Statement: Upon successful upload, the function returns a confirmation message indicating the success of the operation. This upload function is a fundamental building block for creating applications that interact with MinIO storage. It encapsulates the upload logic in a reusable and easily callable format, allowing developers to integrate file upload capabilities into their Langchain applications and workflows efficiently. Enhancing Functionality with RunnableLambda and Secondary Tools After establishing the fundamental upload functionality, enhancing the system for broader integration and additional utility can further refine the tool's capabilities.\n",
      "This involves creating a RunnableLambda for the upload function and defining secondary tools that can be utilized within the same ecosystem. These steps not only extend the functionality but also ensure seamless integration with Langchain workflows. Creating a RunnableLambda for File Upload Langchain's architecture supports the encapsulation of functions into runnables, which can be seamlessly executed within its framework. To facilitate the execution of the upload function within Langchain workflows, we wrap it in a RunnableLambda. This allows the function to be easily integrated with Langchain's agents and executors, enabling automated and complex workflows that can interact with MinIO. Wrap upload_file_runnable with Langchain’s RunnableLambda The RunnableLambda takes our upload_file_to_minio function and makes it readily usable within Langchain's system, enhancing the tool's interoperability and ease of use within different parts of the application. Incorporating Secondary Functions for Extended Functionality In our exploration of integrating MinIO with Langchain, we've primarily focused on the core functionality of uploading files. However, the versatility of Langchain allows for the incorporation of a wide range of functionalities beyond just file management. To illustrate this flexibility, we've included an example of a secondary function, get_word_length, directly inspired by the examples found in the Langchain documentation.\n",
      "This serves to demonstrate how easily additional functions can be integrated into your Langchain projects. The inclusion of the get_word_length function is intended to showcase the process of adding more functionalities to your Langchain tool. Here's a closer look at how this secondary tool is defined: Python “secondary” function with Langchain’s @tool decorator This function, marked with the @tool decorator, is a simple yet effective demonstration of extending your tool's capabilities. By registering this function as another tool within the Langchain framework, it becomes callable in a similar manner to the file upload functionality, showcasing the ease with which developers can enrich their applications with diverse capabilities. The process of adding this function, taken from Langchain's documentation, is not only a testament to the ease of extending your application's functionality but also highlights the importance of leveraging existing resources and documentation to enhance your projects. This approach encourages developers to think beyond the immediate requirements of their projects and consider how additional features can be integrated to create more versatile and powerful tools. These enhancements demonstrate the system's capability to not only perform core tasks like file uploads but also execute additional functionalities, all integrated within the Langchain framework.\n",
      "The inclusion of a RunnableLambda and the introduction of secondary tools exemplify how developers can build a rich ecosystem of functions that work together to automate and streamline processes, leveraging both Langchain and MinIO's robust features. Crafting Interactive Chat Prompts with Langchain As we delve deeper into the integration of MinIO with Langchain, a crucial aspect is designing an interactive experience that leverages the capabilities of both the MinIO upload tool and any additional tools we've integrated. This is where the creation of a ChatPromptTemplate becomes essential. It serves as the blueprint for the interactions between the user and the system, guiding the conversation flow and ensuring that the user's commands are interpreted correctly and efficiently. Creating a ChatPromptTemplate The ChatPromptTemplate is a powerful feature of Langchain that allows developers to pre-define the structure of chat interactions. By specifying the roles of the system and the user within the chat, along with placeholders for dynamic content, we can create a flexible yet controlled dialogue framework.\n",
      "Here's how you can create a chat prompt template that incorporates the functionality of our tools: Importing and defining prompt using ChatPromptTemplate and MessagePlaceholder In this template: System Message: The first message from the \"system\" sets the context for the interaction, informing the user (or the language model assuming the user's role) that they are interacting with an assistant that possesses file management capabilities. This helps to frame the user's expectations and guide their queries or commands. User Input Placeholder: The \"user\" key is followed by \"{input}\", which acts as a placeholder for the user's actual input. This dynamic insertion point allows the system to adapt to various user commands, facilitating a wide range of file management tasks. MessagesPlaceholder for Agent Scratchpad: The MessagesPlaceholder with the variable name \"agent_scratchpad\" is a dynamic area within the chat where additional information, such as the results of tool executions or intermediate steps, can be displayed. This feature enhances the interactivity and responsiveness of the chat, providing users with feedback and results in real-time. This step in setting up the chat prompt template is pivotal for creating an engaging and functional user interface for our Langchain application.\n",
      "It not only structures the interaction but also seamlessly integrates the functionalities of our MinIO upload tool and any additional tools, making them accessible through natural language commands within the chat environment. Binding Tools to the Language Model for Enhanced Functionality A pivotal step in harnessing the full potential of our Langchain application is the integration of our custom tools with the language model. This use of “custom tools”, and similar “tool-use” integrations, allows the language model to not only understand and generate natural language but also to execute specific functionalities, such as uploading files to MinIO or calculating the length of a word. By binding these tools directly to the language model, we create a powerful, multifunctional system capable of processing user inputs and performing complex tasks based on those inputs. How to Bind Tools to the Language Model Binding tools to the language model (LLM) involves creating a list of the functions we've defined as tools and using the bind_tools method provided by Langchain's ChatOpenAI class. This method associates our tools with a particular instance of the language model, making them callable as part of the language processing workflows. Here's how this can be achieved: Binding the list of functions to the LLM In this code snippet: Tools List: We start by defining a list named tools that contains the functions we've decorated with @tool.\n",
      "This includes our upload_file_to_minio function for uploading files to MinIO and the get_word_length function as an example of a secondary tool. You can add more tools to this list as needed. Binding Tools: The bind_tools method takes the list of tools and binds them to the llm instance. This creates a new language model instance, llm_with_tools, which has our custom tools integrated into it. This enhanced language model can now interpret commands related to the functionalities provided by the bound tools, enabling a seamless interaction between natural language processing and task execution. This process of binding tools to the language model is crucial for creating interactive and functional Langchain applications. It bridges the gap between natural language understanding and practical task execution, allowing users to interact with the system using conversational language to perform specific actions, such as file management in MinIO. This integration significantly expands the capabilities of Langchain applications, paving the way for innovative uses of language models in various domains. Implementing Memory Management for Enhanced User Interaction Creating an engaging and intuitive Langchain application requires more than just processing commands—it demands a system that can remember and learn from past interactions.\n",
      "This is where memory management becomes essential, enabling the application to maintain context across conversations, which is particularly useful for handling complex queries and follow-up questions. Establishing the Agent and Executor Framework At the heart of our application lies the AgentExecutor, a sophisticated mechanism designed to interpret user inputs, manage tasks, and facilitate communication between the user and the system. To set this up, we need to incorporate several key components from Langchain: Importing AgentExecutor openai_tools , and OpenAIToolsAgentOutputParser This foundational setup ensures our application has the necessary tools to execute tasks, interpret the language model's output, and apply our custom functionalities effectively. Memory Management Integration To enrich the user experience with context-aware responses, we update our chat prompt template to include memory management features. This involves adding placeholders for storing and referencing the chat history within our conversations: Refactoring prompt and agent_scratchpad with empty chat_history list This adjustment enables our application to dynamically incorporate previous interactions into the ongoing conversation, ensuring a cohesive and context-rich dialogue with the user. Refining the Agent with Contextual Awareness To fully leverage the benefits of memory management, we refine our agent's definition to incorporate chat history actively.\n",
      "This involves defining specific behaviors for handling inputs, managing the agent's scratchpad, and incorporating the chat history into the agent's decision-making process: Define agent chain and agent_executor In this enhanced agent setup: Custom Lambda Functions: The agent utilizes lambda functions to handle the user's input, manage intermediate steps stored in the agent's scratchpad, and seamlessly integrate the chat history into the conversation flow. Agent Pipeline Assembly: By chaining the components with the pipeline operator (|), we create a streamlined process that takes the user's input, enriches it with contextual history, executes the necessary tools, and interprets the output for actionable responses. Execution with AgentExecutor: The AgentExecutor is initialized with our context-aware agent and the predefined tools, equipped with verbose logging for detailed operational insights. Executing File Uploads with Contextualized Agent Interactions Integrating MinIO file uploads into a Langchain-based application offers a practical example of how conversational AI can streamline complex operations. This capability becomes even more powerful when combined with memory management, allowing the agent to maintain context and manage tasks like file uploads dynamically. Here’s how to set up and execute a file upload process, demonstrating the application's ability to interpret and act on user commands.\n",
      "Defining the User Prompt and File Information First, we establish a user prompt that instructs the system on the desired action—in this case, uploading a file with specific parameters. Alongside this, we define the structure for the file information, including the bucket name, object name, and the data bytes of the file to be uploaded: input1 is where we define our user prompt as a string. This setup not only specifies what the user wants to do but also encapsulates the necessary details for the MinIO upload operation in a structured format. The Agent Execution With the prompt and file information defined, we proceed to simulate the execution of the file upload command through our agent. This simulation involves invoking the agent_executor with the specified input, including the chat history to maintain conversational context: Invoking our agent_executor with input1 and chat_history In this process: Invocation of Agent Executor: The agent_execution is called with a dictionary containing the user's input, the current chat history, and the structured file information. This approach allows the agent to understand the command within the context of the conversation and access the necessary details for the file upload operation. Updating Chat History: After executing the command, we update the chat history with the new interaction. This includes recording the user's input as a HumanMessage and the system's response as an AIMessage.\n",
      "This step is crucial for maintaining an accurate and comprehensive record of the conversation, ensuring that context is preserved for future interactions. This example illustrates the seamless integration of conversational AI with cloud storage operations, showcasing how a Langchain application can interpret natural language commands to perform specific tasks like file uploads. By maintaining a conversational context and dynamically managing file information, the system offers a user-friendly and efficient way to interact with cloud storage services, demonstrating the practical benefits of combining AI with cloud infrastructure. Streamlining File Uploads with Conversational Prompts One of the remarkable features of integrating Langchain with MinIO for file uploads is the flexibility it offers in handling user requests. While the detailed approach of specifying file_info directly provides clarity and control, the system is also designed to understand and extract necessary information from conversational prompts. This means users can initiate file uploads without explicitly filling out the file_info structure, simply by conveying all required information within the input1 prompt. Simplifying File Upload through Natural Language By crafting a detailed prompt like input1, users can communicate their intent and provide all necessary details for the file upload in a single message.\n",
      "The system's underlying intelligence, powered by Langchain's processing capabilities, parses this input to extract actionable data such as the bucket name, object name, and content. This approach mimics natural human communication, making the interaction with the application more intuitive and user-friendly. Example of a Conversational Prompt This prompt succinctly communicates the user's intent, specifying the object name, content, and target bucket, all within a natural language sentence. The application then processes this prompt, dynamically generating the equivalent file_info needed for the upload operation. Leveraging the file_input Method for Programmatic Execution For scenarios where the application is being used programmatically or when automating tasks as part of a larger workflow, expanding upon the file_input method becomes invaluable. This method allows for a more structured approach to specifying file details, making it ideal for situations where prompts are generated or modified by other parts of an application. The flexibility to switch between conversational prompts and programmatically specified file_info showcases the adaptability of the Langchain and MinIO integration. It enables developers to tailor the file upload process to suit the needs of their application, whether they are aiming for the simplicity of natural language interaction or the precision of programmatically defined tasks.\n",
      "The ability to run file uploads through Langchain without manually filling out the file_info strings, relying instead on the richness of natural language prompts, significantly enhances the usability and accessibility of the application. This feature, combined with the option to use the file_input method for more structured command chains, exemplifies the system's versatility in catering to diverse user needs and operational contexts. Bringing It All Together: The Proof in the Picture. As we reach the culmination of our journey through Langchain's capabilities and the MinIO SDK, it's time to reflect on the tangible outcomes of our work. The power of Langchain is not just in its ability to facilitate complex tasks but also in its tangible, visible results, which we can observe in the provided screenshots. The first screenshot offers a clear view of the MinIO bucket, showcasing the objects that have been successfully created as a result of our LLM tool-use and agent interactions. The objects listed, including \"funny_object\", \"minio-action\", and \"proof-of-concept\", serve as a testament to the practical application of our guide. This visual evidence underscores the effectiveness of using Langchain to manage and organize data within a robust object storage system like MinIO. In the second screenshot, we witness the Langchain agent in action. The trace of the AgentExecutor chain details the invocation steps, clearly marking the success of each task.\n",
      "Here, we see the sequence of the agent's operation, from the initiation of the chain to the successful execution of the file upload. This output provides users with a transparent view into the process, offering assurance and understanding of each phase in the operation. Jupyter Notebook Together, these visuals not only serve as proof of concept but also illustrate the seamless integration and interaction between the Langchain agents, LLMs, and MinIO storage. As developers and innovators, these insights are invaluable, providing a clear path to refine, optimize, and scale our applications. This is the true beauty of combining these technologies: creating a world where conversational AI meets practical execution, leading to efficient and intelligent tool-use that pushes the boundaries of what we can achieve with modern technology. Embracing the Future: Intelligent Automation with Langchain and MinIO The interplay between Langchain’s sophisticated language understanding and MinIO’s robust object storage has been vividly demonstrated through practical examples and visual evidence. The journey from conceptual explanations to the execution of real-world tasks has illustrated the transformative potential of LLMs when they are finely tuned and integrated with cloud-native technologies. The resulting synergy not only streamlines complex data operations but also enriches the user experience, providing intuitive and intelligent interactions with digital environments.\n",
      "As we reflect on the capabilities of Langchain agents and MinIO SDK, the future of tool development with LLMs looks promising, brimming with opportunities for innovation and efficiency. Whether you are a seasoned developer or a curious newcomer, the path forward is clear: the integration of AI with cloud storage is not just a leap towards more advanced applications but a stride into a new era of intelligent automation. With the right tools and understanding, there is no limit to the advancements we can make in this exciting domain. We are excited to accompany you on your journey with Langchain and MinIO. As you navigate through the intricacies of these technologies, know that our community is here to support you. Connect with us on Slack for any queries or to share your progress. We’re always ready to help or simply celebrate your successes. For those who’ve made it this far and are eager to dive into the practical implementation, don’t forget to check out the accompanying notebook for a hands-on experience; access the notebook here. Here’s to crafting the future, one innovative solution at a time!\n",
      "Previous Post Next Post S3 Select Security Modern Data Lakes Apache Presto SQL Performance S3 Brand/Design Golang Programming Cloud Computing Microservices Docker AWS Kubernetes Apache Spark Open Source Benchmarks Integrations SUBNET Edge Computing Sidekick Secure-by-Design Splunk Veeam Intel Apache Nifi Immutability Software Defined Storage VMware Apache Arrow Hybrid Cloud Red Hat OpenShift Multicloud Scalability Cloud Field Day Cloud Native Apache Kafka Architect's Guide Awards Operator's Guide Security Advisory AI/ML AGPLv3 Apache Hadoop SFD Azure GCP Observability Analytics H20 DevOps Apache Iceberg Apache Hudi YouTube Summaries EKS Elastic Load Balancers CI/CD Object Storage Compliance opentelemetry BC/DR Storage Newsletter Predictions Best Practices Dremio New MinIO Features partners Small Files Databases DuckDB PostgreSQL Delta Lake Cloud Repatriation Python Object Lambdas Data Pipelines Cloud Operating Model Webhook ClickHouse Vector Database Events Value Engineering Change Data Capture\n",
      "\n",
      "Source: medium.com_widle-studio_building-ai-solutions-with-langchain-and-node-js-a-comprehensive-guide-widle-studio-4812753aedff.txt\n",
      "Content: Open in app Sign up Sign in Write Sign up Sign in Building AI Solutions with LangChain and Node.js: A Comprehensive Guide Saunak Surani·Follow Published inWidle Studio LLP·14 min read·Jul 25, 2023 -- Listen Share In the rapidly evolving landscape of artificial intelligence (AI) and natural language processing (NLP), developers seek powerful tools that simplify the creation of AI-driven solutions. LangChain is an exciting and innovative library that offers a wide range of NLP capabilities to developers using Node.js. In this comprehensive guide, we will explore the world of LangChain and demonstrate how to build AI solutions effortlessly with its powerful features. Table of Contents: Understanding LangChain1.1 What is LangChain?1.2 Key Features of LangChain1.3 Why Choose LangChain for AI Solutions?\n",
      "Setting Up the Environment2.1 Installing Node.js2.2 Initializing a Node.js Project2.3 Installing LangChain with npm Working with LangChain3.1 Text Tokenization3.2 Part-of-Speech Tagging3.3 Named Entity Recognition (NER)3.4 Sentiment Analysis3.5 Language Translation3.6 Speech-to-Text and Text-to-Speech3.7 AI Chatbots with LangChain Building a Language Translator4.1 Defining the Project Structure4.2 Implementing the Translation Logic4.3 Enhancing Translation Accuracy with Language Models4.4 Creating a User-Friendly Interface Sentiment Analysis for Social Media5.1 Gathering Social Media Data5.2 Preprocessing and Analyzing Sentiments5.3 Visualizing Sentiment Analysis Results Speech-to-Text in Action6.1 Recording and Processing Audio6.2 Converting Speech to Text6.3 Transcribing\n",
      "Audio Files Integrating LangChain with Existing Apps7.1 Enhancing an E-commerce Chatbot7.2 Improving Customer Support with NLP7.3 Streamlining Language Processing in Big Data Deploying LangChain-powered Solutions8.1 Choosing the Right Hosting Environment8.2 Ensuring Scalability and Performance Future Trends and Advanced Use Cases9.1 Deep Learning with LangChain9.2 LangChain for Real-Time Translation9.3 Extending LangChain with Custom Models Best Practices for LangChain Development10.1 Code Optimization and Performance Tuning10.2 Ensuring Data Privacy and Security10.3 Continuous Integration and Testing10.4 Model Performance Monitoring10.5 Error Handling and Logging10.6 Version Control and Collaboration10.7 Documentation10.8 Deployment and Scalability Conclusion 1. Understanding LangChain 1.1 What is LangChain? LangChain is a Node.js library that empowers developers with powerful natural language processing capabilities. It leverages advanced AI algorithms and models to perform tasks like text tokenization, part-of-speech tagging, named entity recognition, language translation, and sentiment analysis. Developers can build sophisticated NLP applications effortlessly with the simplicity and efficiency of LangChain's APIs. 1.2 Key Features of LangChain Text Tokenization: Breaks down textual content into smaller tokens, facilitating language processing tasks. Part-of-Speech Tagging: Identifies the grammatical parts of each word in a sentence, aiding in language understanding.\n",
      "Named Entity Recognition (NER): Detects and categorizes entities like names, locations, and dates in a text. Sentiment Analysis: Determines the emotional tone of a text, classifying it as positive, negative, or neutral. Language Translation: Translates text from one language to another, bridging communication gaps. Speech-to-Text and Text-to-Speech: Converts audio recordings into written text and generates speech from textual content. 1.3 Why Choose LangChain for AI Solutions? LangChain offers a myriad of benefits that make it an excellent choice for AI-driven solutions: Ease of Integration: LangChain is easy to integrate into existing Node.js projects, making it accessible to developers of all levels of expertise. Versatility: With a wide range of NLP capabilities, LangChain caters to diverse AI solution requirements. High Performance: LangChain's underlying algorithms ensure speedy and efficient language processing. Community and Support: The LangChain community provides active support and regular updates, ensuring a seamless development experience. 2. Setting Up the Environment 2.1 Installing Node.js Before we start using LangChain, we need to have Node.js installed on our system. Node.js is a JavaScript runtime that enables us to run JavaScript code outside the browser. Visit the official Node.js website (https://nodejs.org) and download the latest stable version for your operating system. Once installed, verify the installation by running node -v in your terminal.\n",
      "2.2 Initializing a Node.js Project Once Node.js is installed, navigate to your project directory and initialize a new Node.js project by running npm init in the terminal. Follow the prompts to set up your project, and a package.json file will be created. 2.3 Installing LangChain with npm With the Node.js project set up, we can now install LangChain using npm, the Node.js package manager. Open the terminal and run the following command to install LangChain as a dependency in your project: npm install -S langchain The -S flag saves LangChain as a dependency in the package.json file. 3. Working with LangChain With LangChain successfully installed, we can now explore its powerful NLP capabilities. Let's dive into various tasks we can perform with LangChain. 3.1 Text Tokenization Text tokenization is the process of breaking down text into smaller units, called tokens. These tokens can be words, phrases, or sentences, depending on the level of tokenization required. LangChain provides an easy-to-use function to tokenize text: const langchain = require('langchain'); const text = \"LangChain makes NLP easy! \"; const tokens = langchain.tokenize(text); console.log(tokens); The output will be an array of tokens: [\"LangChain\", \"makes\", \"NLP\", \"easy\", \"!\"]. 3.2 Part-of-Speech Tagging Part-of-speech tagging involves assigning grammatical parts to each word in a sentence, such as nouns, verbs, adjectives, etc.\n",
      "LangChain enables us to perform part-of-speech tagging with a simple function call: const langchain = require('langchain'); const sentence = \"LangChain is an amazing tool. \"; const posTags = langchain.posTag(sentence); console.log(posTags); The output will be an array of objects, each representing a word and its part-of-speech tag: [{\"word\": \"LangChain\", \"tag\": \"NNP\"}, {\"word\": \"is\", \"tag\": \"VBZ\"}, {\"word\": \"an\", \"tag\": \"DT\"}, {\"word\": \"amazing\", \"tag\": \"JJ\"}, {\"word\": \"tool\", \"tag\": \"NN\"}, {\"word\": \". \", \"tag\": \".\"}]. 3.3 Named Entity Recognition (NER) Named Entity Recognition identifies and categorizes entities like names, locations, organizations, and dates in a text. LangChain provides a function to perform NER: const langchain = require('langchain'); const text = \"Apple Inc. was founded by Steve Jobs in Cupertino on April 1, 1976. \"; const entities = langchain.ner(text); console.log(entities); The output will be an array of objects, each representing an entity and its category: [{\"text\": \"Apple Inc.\", \"category\": \"ORG\"}, {\"text\": \"Steve Jobs\", \"category\": \"PERSON\"}, {\"text\": \"Cupertino\", \"category\": \"LOCATION\"}, {\"text\": \"April 1, 1976\", \"category\": \"DATE\"}]. 3.4 Sentiment Analysis Sentiment analysis helps determine the emotional tone of a piece of text, classifying it as positive, negative, or neutral. LangChain provides a sentiment analysis function: const langchain = require('langchain'); const review = \"The movie was fantastic!\n",
      "\"; const sentiment = langchain.sentiment(review); console.log(sentiment); The output will be a sentiment object with a label representing the sentiment and a score indicating the confidence level: {\"label\": \"positive\", \"score\": 0.9}. 3.5 Language Translation Language translation involves converting text from one language to another. LangChain offers an easy-to-use translation function: const langchain = require('langchain'); const textToTranslate = \"Hello, how are you? \"; const translatedText = langchain.translate(textToTranslate, 'fr'); // Translate to French console.log(translatedText); The output will be the translated text: \"Bonjour, comment ça va?\". 3.6 Speech-to-Text and Text-to-Speech LangChain also provides functions for speech-to-text (STT) and text-to-speech (TTS) conversion. These features enable interaction with audio data: const langchain = require('langchain'); // Speech-to-Text const audioFile = \"path/to/audiofile.wav\"; const transcript = langchain.speechToText(audioFile); console.log(transcript); // Text-to-Speech const textToSpeak = \"Welcome to LangChain. \"; langchain.textToSpeech(textToSpeak, 'en'); // Convert text to English speech 3.7 AI Chatbots with LangChain LangChain can power AI chatbots by combining its NLP capabilities with chatbot logic. Developers can create chatbots that understand user input, generate appropriate responses, and even conduct sentiment analysis on user messages. 4.\n",
      "Building a Language Translator To demonstrate LangChain's power, let's build a simple language translator that converts text from one language to another. We'll use the Google Translate API for translation, combined with LangChain for preprocessing. 4.1 Defining the Project Structure Set up the project structure with the following files: translator.js translator.js package.json 4.2 Implementing the Translation Logic In the translator.js file, we'll implement the language translation logic using LangChain and Google Translate API: const langchain = require('langchain'); const { Translate } = require('@google-cloud/translate').v2; // Google Cloud Translate API Configuration const translate = new Translate({ projectId: 'your-project-id', keyFilename: 'path/to/your/credentials.json', }); // Function to translate text async function translateText(text, targetLanguage) { const preprocessedText = langchain.preprocess(text); // Preprocess text using LangChain const [translation] = await translate.translate(preprocessedText, targetLanguage); return translation; module.exports = { translateText }; In this code, we set up the Google Translate API client and define a function translateText() that takes the text and the target language code as input. The text is preprocessed using LangChain's preprocess() function, and then the Google Translate API is used to perform the translation.\n",
      "4.3 Enhancing Translation Accuracy with Language Models To improve translation accuracy, we can leverage LangChain's language models. Let's integrate a language model to enhance our language translator: const langchain = require('langchain'); const { Translate } = require('@google-cloud/translate').v2; // Google Cloud Translate API Configuration const translate = new Translate({ projectId: 'your-project-id', keyFilename: 'path/to/your/credentials.json', }); // Function to translate text with language model async function translateWithModel(text, targetLanguage, model) { const preprocessedText = langchain.preprocess(text); const [translation] = await translate.translate(preprocessedText, { targetLanguage, model, }); return translation; module.exports = { translateWithModel }; In this enhanced version, we added a model parameter to the translateWithModel() function, allowing us to specify a specific language model for translation. The available models depend on the supported languages and may include general models, news models, or conversation models.\n",
      "4.4 Creating a User-Friendly Interface Now that our translation logic is in place, let's create a simple command-line interface for our language translator: const { translateWithModel } = require('./translator'); const readline = require('readline'); const rl = readline.createInterface({ input: process.stdin, output: process.stdout, }); rl.question('Enter text to translate: ', async (text) => { rl.question('Enter target language code (e.g., fr for French): ', async (targetLanguage) => { rl.question('Enter model (e.g., nmt for Neural Machine Translation): ', async (model) => { try { const translatedText = await translateWithModel(text, targetLanguage, model); console.log('Translated Text:', translatedText); } catch (error) { console.error('Translation Error:', error.message); } finally { rl.close(); }); }); }); In this interface, we use readline to accept user input for the text to translate, the target language code, and the desired model for translation. The translateWithModel() function is called, and the translated text is displayed to the user. 5. Sentiment Analysis for Social Media In this section, we will use LangChain's sentiment analysis feature to analyze sentiments expressed on social media platforms. We'll gather social media data, perform sentiment analysis, and visualize the results. 5.1 Gathering Social Media Data To demonstrate sentiment analysis, we'll assume we have a dataset of social media posts stored in a JSON file.\n",
      "Each post object includes a text field with the post content and a timestamp field with the post's timestamp: \"text\": \"Just watched an amazing movie! \", \"timestamp\": \"2023-07-01T12:30:00Z\" }, \"text\": \"Feeling down today...\", \"timestamp\": \"2023-07-02T08:45:00Z\" }, \"text\": \"Excited for the weekend getaway! \", \"timestamp\": \"2023-07-03T15:20:00Z\" }, // More posts... To read this data into our Node.js environment, we can use the fs module: const fs = require('fs'); const socialMediaData = JSON.parse(fs.readFileSync('social_media_data.json', 'utf8')); 5.2 Preprocessing and Analyzing Sentiments Next, we'll preprocess the social media posts and perform sentiment analysis using LangChain: const langchain = require('langchain'); // Function to preprocess text and perform sentiment analysis function analyzeSentiments(posts) { const sentiments = []; posts.forEach((post) => { const { text, timestamp } = post; const preprocessedText = langchain.preprocess(text); const sentiment = langchain.sentiment(preprocessedText); sentiments.push({ text, timestamp, sentiment }); }); return sentiments; const socialMediaSentiments = analyzeSentiments(socialMediaData); console.log(socialMediaSentiments); The analyzeSentiments() function preprocesses each post's text using Lang Chain's preprocess() function and performs sentiment analysis with sentiment(). The results are stored in an array containing the post's original text, timestamp, and sentiment object.\n",
      "5.3 Visualizing Sentiment Analysis Results To visualize the sentiment analysis results, we can use a plotting library like chart.js: npm install chart.js Now, let's create a bar chart to display the sentiments over time: const { analyzeSentiments } = require('./sentiment'); const ctx = document.getElementById('sentimentChart').getContext('2d'); const posts = socialMediaSentiments.map((post) => post.text); const sentiments = socialMediaSentiments.map((post) => post.sentiment.score); const timestamps = socialMediaSentiments.map((post) => new Date(post.timestamp)); const data = { labels: timestamps, datasets: [ label: 'Sentiment Score', data: sentiments, backgroundColor: 'rgba(75, 192, 192, 0.2)', borderColor: 'rgba(75, 192, 192, 1)', borderWidth: 1, }, ], }; const config = { type: 'bar', data: data, options: { scales: { x: { type: 'time', time: { unit: 'day', }, }, y: { beginAtZero: true, suggestedMax: 1, }, }, }, }; const myChart = new Chart(ctx, config); In this code, we use Chart.js to create a bar chart to visualize the sentiments over time. The chart displays the sentiment score on the y-axis and the timestamps on the x-axis. 6. Speech-to-Text in Action In this section, we will use LangChain's speech-to-text feature to transcribe audio recordings into text. Let's assume we have an audio file of an interview that we want to transcribe. 6.1 Recording and Processing Audio To perform speech-to-text, we need an audio recording.\n",
      "You can record a short audio clip using any recording device or use an existing audio file. Once you have the audio file, ensure it's in a compatible format (e.g., WAV or MP3). 6.2 Converting Speech to Text With the audio file ready, we can use LangChain's speech-to-text functionality: const langchain = require('langchain'); // Function to transcribe audio to text async function transcribeAudio(audioFilePath) { const transcript = await langchain.speechToText(audioFilePath); return transcript; const audioFilePath = 'path/to/audiofile.wav'; transcribeAudio(audioFilePath) .then((transcript) => console.log('Transcription:', transcript)) .catch((error) => console.error('Transcription Error:', error.message)); In this code, we define the transcribeAudio() function, which takes the path to the audio file as input and returns the transcribed text. 6.3 Transcribing Audio Files The transcribeAudio() function uses LangChain's speechToText() function to perform the transcription. Make sure you have a stable internet connection as the transcription requires API calls to LangChain's backend. 7. Integrating LangChain with Existing Apps One of LangChain's strengths is its ability to integrate seamlessly with existing applications. Here are a few examples of how to enhance applications with LangChain's NLP capabilities. 7.1 Enhancing an E-commerce Chatbot Imagine you have an e-commerce website with a chatbot to assist customers.\n",
      "By integrating LangChain, you can enable the chatbot to understand user queries better and provide more accurate responses. const langchain = require('langchain'); // Function to process user query function processUserQuery(userInput) { const preprocessedQuery = langchain.preprocess(userInput); // Perform logic to handle the user query // Return appropriate response In this code, we preprocess the user's input using LangChain's preprocess() function, which standardizes the text for better processing. 7.2 Improving Customer Support with NLP For businesses offering customer support, LangChain's sentiment analysis can be invaluable in understanding customer feedback. By analyzing the sentiments expressed in customer messages, support teams can identify potential issues and respond appropriately. const langchain = require('langchain'); // Function to analyze customer feedback function analyzeCustomerFeedback(feedback) { const sentiment = langchain.sentiment(feedback); if (sentiment.label === 'negative') { // Notify the support team about the negative feedback // Take appropriate action In this example, we use LangChain's sentiment analysis to identify negative feedback and notify the support team for a quick resolution. 7.3 Streamlining Language Processing in Big Data For large-scale applications dealing with big data, LangChain's language processing capabilities can be a game-changer.\n",
      "By integrating LangChain into your big data pipeline, you can analyze vast amounts of text data efficiently. const langchain = require('langchain'); // Function to analyze text data in big data pipeline function analyzeTextData(textData) { const preprocessedData = textData.map((text) => langchain.preprocess(text)); // Perform analysis on the preprocessed data // Return insights and results In this scenario, LangChain preprocesses the text data before analysis, ensuring consistency and accurate results. 8. Deploying LangChain-powered Solutions After developing AI solutions with LangChain, the next step is deployment. Deployment requires considerations such as hosting environment, scalability, and performance. 8.1 Choosing the Right Hosting Environment When deploying LangChain-powered applications, you can choose from various hosting options, including cloud platforms like Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP). Each platform offers different services and pricing models, so select the one that best suits your project's requirements and budget. 8.2 Ensuring Scalability and Performance To handle increasing user demands, ensure that your LangChain-powered application is scalable. Utilize cloud-based solutions that offer auto-scaling capabilities to dynamically adjust resources based on traffic. Additionally, optimize your code and minimize unnecessary computations to improve overall performance. 9.\n",
      "Future Trends and Advanced Use Cases As the field of AI continues to evolve, LangChain is likely to keep up with advancements. Here are some future trends and advanced use cases to explore: 9.1 Deep Learning with LangChain LangChain may integrate more advanced deep learning models to improve the accuracy and performance of NLP tasks. This can lead to more accurate translations, better sentiment analysis, and advanced language understanding. 9.2 LangChain for Real-Time Translation The demand for real-time language translation is on the rise. LangChain can further enhance its translation capabilities to deliver instantaneous translations for live events, meetings, and communication across languages. 9.3 Extending LangChain with Custom Models To cater to specific business needs, LangChain may offer the option to integrate custom NLP models. Businesses can train their language models and use them in combination with LangChain's existing features. 10. Best Practices for LangChain Development Now that we have explored the capabilities of LangChain and how to integrate it with Node.js, let’s dive into some best practices for LangChain development. Following these practices will not only improve the performance and efficiency of your AI solutions but also enhance the overall development process. 10.1 Optimize Data Processing When dealing with large volumes of text data, data processing can be a significant bottleneck.\n",
      "To optimize data processing, consider the following techniques: a) Batch Processing: Instead of processing individual text entries one by one, group them into batches and process them together. Batch processing can significantly reduce the overhead and improve efficiency. b) Multithreading: Utilize multithreading or worker threads to parallelize data processing tasks. This approach can take advantage of multi-core processors and speed up computations. c) Streaming: When working with real-time data streams, consider using stream processing to handle data in chunks rather than loading the entire dataset at once. This can lead to more efficient memory usage and faster processing. 10.2 Ensure Data Privacy and Security When working with sensitive data, it’s crucial to prioritize data privacy and security. Here are some practices to follow: a) Encryption: Encrypt data at rest and in transit to protect it from unauthorized access. b) Access Controls: Implement proper access controls and authentication mechanisms to restrict access to sensitive data. c) Data Anonymization: If possible, anonymize data to protect the identities of individuals in the dataset. 10.3 Continuous Integration and Testing Implement continuous integration (CI) and automated testing to catch bugs and issues early in the development process. Set up a robust testing framework to thoroughly test your AI solutions under different scenarios.\n",
      "Automated testing ensures that changes or updates to the codebase do not introduce regressions. 10.4 Model Performance Monitoring AI models may degrade in performance over time due to changes in data distribution or other factors. Implement monitoring mechanisms to track model performance regularly. If the model’s performance drops below an acceptable threshold, consider retraining it with fresh data. 10.5 Error Handling and Logging Proper error handling and logging are essential to diagnose issues and troubleshoot problems effectively. Implement comprehensive error handling in your code and use logging frameworks to record errors and important events. 10.6 Version Control and Collaboration Utilize version control systems like Git to track changes to your codebase and facilitate collaboration among team members. Version control allows you to revert to previous versions if needed and helps maintain a clean and organized codebase. 10.7 Documentation Document your code thoroughly, including the purpose and functionality of each module or function. Well-documented code is easier to understand and maintain, making it more accessible to other developers who might work on the project. 10.8 Deployment and Scalability When deploying AI solutions built with LangChain, consider the scalability requirements. Ensure that the deployment environment can handle the expected workload and traffic.\n",
      "Utilize containerization technologies like Docker to package your application and its dependencies for easy deployment. 11. Conclusion LangChain offers a powerful and user-friendly platform for natural language processing tasks, making it an excellent choice for developers looking to build AI solutions without the complexity of creating models from scratch. By integrating LangChain with Node.js, developers can harness the power of AI to process and understand vast amounts of text data, unlocking a world of possibilities in the realm of NLP. In this comprehensive guide, we explored the capabilities of LangChain, from text tokenization to language translation and sentiment analysis. We learned how to set up LangChain in a Node.js environment and built practical AI-driven applications. Additionally, we discussed best practices for LangChain development, including optimizing data processing, ensuring data privacy and security, continuous integration and testing, and responsible AI practices. With LangChain’s versatility and Node.js’s flexibility, developers can build intelligent and efficient AI solutions for various industries and use cases. So, don’t wait any longer! Start building your own AI solutions with LangChain and Node.js to revolutionize the way you process and understand natural language data. Contact Us: For more information or inquiries, feel free to contact us at info@widle.studio.\n",
      "We would love to hear your feedback and discuss how LangChain can power your AI projects. Disclaimer: The views and opinions expressed in this article are those of the author and do not necessarily reflect the official policy or position of LangChain or any other company Langchain Nodejs AI Widle Studio Machine Learning -- -- Follow Written by Saunak Surani 90 Followers Editor for Widle Studio LLP Passionate about technology, design, startups, and personal development. Bringing ideas to life at https://widle.studio Follow Help Status About Careers Blog Privacy Terms Text to speech Teams\n",
      "\n",
      "Source: www.sitepoint.com_langchain-python-complete-guide_.txt\n",
      "Content: Free Tech Books AI JavaScript Computing Design & UX HTML & CSS Entrepreneur Web PHP WordPress Mobile Programming Python AI A Complete Guide to LangChain in Python Matt Nikonorov Share LangChain is a versatile Python library that empowers developers and researchers to create, experiment with, and analyze language models and agents. It offers a rich set of features for natural language processing (NLP) enthusiasts, from building custom models to manipulating text data efficiently. In this comprehensive guide, we’ll dive deep into the essential components of LangChain and demonstrate how to harness its power in Python. Getting Set Up Agents Creating a LangChain agent Agent test example 1 Agent test example 2 Models Language model Chat model Embeddings A use case of embedding models Chunks Splitting chunks by character Recursively splitting chunks Chunk size and overlap Chains Going Beyond OpenAI Conclusion Getting Set Up To follow along with this article, create a new folder and install LangChain and OpenAI using pip: install langchain openai Agents In LangChain, an Agent is an entity that can understand and generate text. These agents can be configured with specific behaviors and data sources and trained to perform various language-related tasks, making them versatile tools for a wide range of applications. Creating a LangChain agent Agents can be configured to use “tools” to gather the data they need and formulate a good response. Take a look at the example below.\n",
      "It uses Serp API (an internet search API) to search the Internet for information relevant to the question or input, and uses that to make a response. It also uses the llm-math tool to perform mathematical operations — for example, to convert units or find the percentage change between two values: from langchain .agents import load_tools from langchain .agents import initialize_agent from langchain .agents import AgentType from langchain .llms import OpenAI import os os .environ \"OPENAI_API_KEY\" \"YOUR_OPENAI_API_KEY\" os .environ \"SERPAPI_API_KEY\" \"YOUR_SERP_API_KEY\" # get your Serp API key here: https://serpapi.com/ OpenAI .api_key \"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\" llm = OpenAI (model \"gpt-3.5-turbo\" , temperature ) tools = load_tools \"serpapi\" \"llm-math\" , llm =llm ) agent = initialize_agent (tools , llm , agent =AgentType .ZERO_SHOT_REACT_DESCRIPTION , verbose True ) agent .run \"How much energy did wind turbines produce worldwide in 2022?\" As you can see, after doing all the basic importing and initializing our LLM (llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)), the code loads the tools necessary for our agent to work using tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm). It then creates the agent using the initialize_agent function, giving it the specified tools, and it gives it the ZERO_SHOT_REACT_DESCRIPTION description, which means that it will have no memory of previous questions.\n",
      "Agent test example 1 Let’s test this agent with the following input: As you can see, it uses the following logic: search for “wind turbine energy production worldwide 2022” using the Serp internet search API analyze the best result get any relevant numbers convert 906 gigawatts to joules using the llm-math tool, since we asked for energy, not power Agent test example 2 LangChain agents aren’t limited to searching the Internet. We can connect practically any data source (including our own) to a LangChain agent and ask it questions about the data. Let’s try making an agent trained on a CSV dataset. Download this Netflix movies and TV shows dataset from SHIVAM BANSAL on Kaggle and move it into your directory. Now add this code into a new Python file: from langchain .llms import OpenAI from langchain .chat_models import ChatOpenAI from langchain .agents .agent_types import AgentType from langchain .agents import create_csv_agent import os os .environ \"OPENAI_API_KEY\" \"YOUR_OPENAI_API_KEY\" agent = create_csv_agent ( OpenAI (temperature \"netflix_titles.csv\" , verbose True , agent_type =AgentType .ZERO_SHOT_REACT_DESCRIPTION ) agent .run \"In how many movies was Christian Bale casted\" This code calls the create_csv_agent function and uses the netflix_titles.csv dataset. The image below shows our test. As shown above, its logic is to look in the cast column for all occurrences of “Christian Bale”.\n",
      "We can also make a Pandas Dataframe agent like this: from langchain .agents import create_pandas_dataframe_agent from langchain .chat_models import ChatOpenAI from langchain .agents .agent_types import AgentType from langchain .llms import OpenAI import pandas as pd import os os .environ \"OPENAI_API_KEY\" \"YOUR_OPENAI_KEY\" df = pd .read_csv \"netflix_titles.csv\" ) agent = create_pandas_dataframe_agent (OpenAI (temperature , df , verbose True ) agent .run \"In what year were the most comedy movies released?\" If we run it, we’ll see something like the results shown below. These are just a few examples. We can use practically any API or dataset with LangChain. Models There are three types of models in LangChain: LLMs, chat models, and text embedding models. Let’s explore every type of model with some examples. Language model LangChain provides a way to use language models in Python to produce text output based on text input. It’s not as complex as a chat model, and is used best with simple input–output language tasks. Here’s an example using OpenAI: from langchain .llms import OpenAI import os os .environ \"OPENAI_API_KEY\" \"YOUR_OPENAI_API_KEY\" llm = OpenAI (model \"gpt-3.5-turbo\" , temperature 0.9 print (llm \"Come up with a rap name for Matt Nikonorov\" As seen above, it uses the gpt-3.5-turbo model to generate an output for the provided input (“Come up with a rap name for Matt Nikonorov”). In this example, I’ve set the temperature to 0.9 to make the LLM really creative.\n",
      "It came up with “MC MegaMatt”. I’d give that one a solid 9/10. Chat model Making LLM models come up with rap names is fun, but if we want more sophisticated answers and conversations, we need to step up our game by using a chat model. How are chat models technically different from language models? Well, in the words of the LangChain documentation: Chat models are a variation on language models. While chat models use language models under the hood, the interface they use is a bit different. Rather than using a “text in, text out” API, they use an interface where “chat messages” are the inputs and outputs. Here’s a simple Python chat model script: from langchain .llms import OpenAI from langchain .chat_models import ChatOpenAI from langchain .schema import ( AIMessage , HumanMessage , SystemMessage import os os .environ \"OPENAI_API_KEY\" \"YOUR_OPENAI_API_KEY\" chat = ChatOpenAI ) messages [ SystemMessage (content \"You are a friendly, informal assistant\" , HumanMessage (content \"Convince me that Djokovic is better than Federer\" print (chat (messages As shown above, the code first sends a SystemMessage and tells the chatbot to be friendly and informal, and afterwards it sends a HumanMessage telling the chatbot to convince us that Djokovich is better than Federer. If you run this chatbot model, you’ll see something like the result shown below.\n",
      "Embeddings Embeddings provide a way to turn words and numbers in a block of text into vectors that can then be associated with other words or numbers. This may sound abstract, so let’s look at an example: from langchain .embeddings import OpenAIEmbeddings embeddings_model = OpenAIEmbeddings ) embedded_query = embeddings_model .embed_query \"Who created the world wide web?\" ) embedded_query This will return a list of floats: [0.022762885317206383, -0.01276398915797472, 0.004815981723368168, -0.009435392916202545, 0.010824492201209068]. This is what an embedding looks like. A use case of embedding models If we want to train a chatbot or LLM to answer questions related to our data or to a specific text sample, we need to use embeddings.\n",
      "Let’s make a simple CSV file (embs.csv) that has a “text” column containing three pieces of information: Now here’s a script that will take the question “Who was the tallest human ever?” and find the right answer in the CSV file by using embeddings: from langchain .embeddings import OpenAIEmbeddings from openai .embeddings_utils import cosine_similarity import os import pandas os .environ \"OPENAI_API_KEY\" \"YOUR_OPENAI_KEY\" embeddings_model = OpenAIEmbeddings ) df = pandas .read_csv \"embs.csv\" # Make embeddings for each piece of information emb1 = embeddings_model .embed_query (df \"text\" ) emb2 = embeddings_model .embed_query (df \"text\" ) emb3 = embeddings_model .embed_query (df \"text\" ) emb_list [emb1 , emb2 , emb3 ] df \"embedding\" = emb_list embedded_question = embeddings_model .embed_query \"Who was the tallest human ever?\" # Make an embedding for the question df \"similarity\" = df .embedding apply lambda x : cosine_similarity (x , embedded_question # Finds the relevance of each piece of data in context to the question df .to_csv \"embs.csv\" ) df2 = df .sort_values \"similarity\" , ascending False # Sorts the pieces of information by their relatedness to the question print (df2 \"text\" If we run this code, we’ll see that it outputs “Robert Wadlow was the tallest human ever”.\n",
      "The code finds the right answer by getting the embedding of each piece of information and finding the one most related to the embedding of the question “Who was the tallest human ever?” The power of embeddings! Chunks LangChain models can’t handle large texts at the same time and use them to make responses. This is where chunks and text splitting come in. Le’s look at two simple ways to split our text data into chunks before feeding it into LangChain. Splitting chunks by character To avoid abrupt breaks in chunks, we can split our texts by paragraphs by splitting them at every occurrence of a newline or double-newline: from langchain .text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter (separators \"\\n\\n\" \"\\n\" , chunk_size 2000 , chunk_overlap 250 ) texts = text_splitter .split_text (your_text Recursively splitting chunks If we want to strictly split our text by a certain length of characters, we can do so using RecursiveCharacterTextSplitter: from langchain .text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter ( chunk_size 2000 , chunk_overlap 250 , length_function len , add_start_index True ) texts = text_splitter .create_documents [your_text Chunk size and overlap While looking at the examples above, you may have wondered exactly what the chunk size and overlap parameters mean, and what implications they have on performance.\n",
      "That can be explained with two points: Chunk size decides the amount of characters that will be in each chunk. The bigger the chunk size, the more data is in the chunk, and the more time it will take LangChain to process it and to produce an output, and vice versa. Chunk overlap is what shares information between chunks so that they share some context. The higher the chunk overlap, the more redundant our chunks will be, the lower the chunk overlap, and the less context will be shared between the chunks. Generally, a good chunk overlap is between 10% and 20% of the chunk size, although the ideal chunk overlap varies across different text types and use cases. Chains Chains are basically multiple LLM functionalities linked together to perform more complex tasks that couldn’t otherwise be done with simple LLM input --> output fashion. Let’s look at a cool example: from langchain .llms import OpenAI from langchain .prompts import PromptTemplate from langchain .chains import LLMChain import os os .environ \"OPENAI_API_KEY\" \"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\" llm = OpenAI (temperature 0.9 ) prompt = PromptTemplate ( input_variables \"media\" \"topic\" , template \"What is a good title for a {media} about {topic}\" ) chain = LLMChain (llm =llm , prompt =prompt print (chain .run 'media' \"horror movie\" 'topic' \"math\" This code takes two variables into its prompt and formulates a creative answer (temperature=0.9).\n",
      "In this example, we’ve asked it to come up with a good title for a horror movie about math. The output after running this code was “The Calculating Curse”, but this doesn’t really show the full power of chains. Let’s take a look at a more practical example: from langchain .chat_models import ChatOpenAI from langchain .prompts import PromptTemplate from typing import Optional from langchain .chains .openai_functions import ( create_openai_fn_chain , create_structured_output_chain import os os .environ \"OPENAI_API_KEY\" \"YOUR_KEY\" llm = ChatOpenAI (model \"gpt-3.5-turbo\" , temperature 0.1 ) template \"\"\"Use the given format to extract information from the following input: {input}. Make sure to answer in the correct format\"\"\" prompt = PromptTemplate (template =template , input_variables \"input\" ) json_schema \"type\" \"object\" \"properties\" \"name\" \"title\" \"Name\" \"description\" \"The artist's name\" \"type\" \"string\" \"genre\" \"title\" \"Genre\" \"description\" \"The artist's music genre\" \"type\" \"string\" \"debut\" \"title\" \"Debut\" \"description\" \"The artist's debut album\" \"type\" \"string\" \"debut_year\" \"title\" \"Debut_year\" \"description\" \"Year of artist's debut album\" \"type\" \"integer\" \"required\" \"name\" \"genre\" \"debut\" \"debut_year\" } chain = create_structured_output_chain (json_schema , llm , prompt , verbose False ) f open \"Nas.txt\" \"r\" ) artist_info str (f .read print (chain .run (artist_info This code may look confusing, so let’s walk through it.\n",
      "This code reads a short biography of Nas (Hip-Hop Artist) and extracts the following values from the text and formats them into a JSON object: the artist’s name the artist’s music genre the artist’s debut album the year of artist’s debut album In the prompt we also specify “Make sure to answer in the correct format”, so that we always get the output in JSON format. Here’s the output of this code: 'name' 'Nas' 'genre' 'Hip Hop' 'debut' 'Illmatic' 'debut_year' 1994 By providing a JSON schema to the create_structured_output_chain function, we’ve made the chain put its output into JSON format. Going Beyond OpenAI Even though I keep using OpenAI models as examples of the different functionalities of LangChain, it isn’t limited to OpenAI models. We can use LangChain with a multitude of other LLMs and AI services. (Here’s a full list of LangChain integratable LLMs.) For example, we can use Cohere with LangChain. Here’s the documentation for the LangChain Cohere integration, but just to give a practical example, after installing Cohere using pip3 install cohere we can make a simple question --> answer code using LangChain and Cohere like this: from langchain .llms import Cohere from langchain .prompts import PromptTemplate from langchain .chains import LLMChain template \"\"\"Question: {question} Answer: Let's think step by step.\"\"\"\n",
      "prompt = PromptTemplate (template =template , input_variables \"question\" ) llm = Cohere (cohere_api_key \"YOUR_COHERE_KEY\" ) llm_chain = LLMChain (prompt =prompt , llm =llm ) question \"When was Novak Djokovic born?\" print (llm_chain .run (question The code above produces the following output: Conclusion In this guide, you’ve seen the different aspects and functionalities of LangChain. Armed with this knowledge, you’re now equipped to leverage LangChain’s capabilities for your NLP endeavors, whether you’re a researcher, developer, or hobbyist. You can find a repo with all the images and the Nas.txt file from this article on GitHub. Happy coding and experimenting with LangChain in Python! Share This Article Matt Nikonorov I'm a full-stack developer with 3 years of experience with PHP, Python, Javascript and CSS. I love blogging about web development, application development and machine learning. LangChain Up Next How to Analyze Large Text Datasets with LangChain and Python Matt Nikonorov LlamaIndex vs LangChain: Tools for Building LLM-powered Apps Dianne Pena A Complete Guide to LangChain in JavaScript Matt Nikonorov An Introduction to LangChain: AI-Powered Language Modeling Dianne Pena A Guide to Python Exception Handling Ini Arthur A Beginner’s Guide to HTTP Python Requests Lorenzo Bonannella\n",
      "\n",
      "Source: blog.min.io_.txt\n",
      "Content: Topics All Architect's Guide Operator's Guide Best Practices AI/ML Modern Data Lakes Performance Kubernetes Integrations Benchmarks Security Multicloud Building Modern Data Architectures with Iceberg, Tabular and MinIO Brenna Buuck Brenna Buuck on Modern Data Lakes Explore modern data architecture with Iceberg, Tabular, and MinIO. Learn to seamlessly integrate structured and unstructured data, optimize AI/ML workloads, and build a high-performance, cloud-native data lake. Read more... Developing Langchain Agents with the MinIO SDK for LLM Tool-Use David Cannan David Cannan on AI/ML Explore Langchain’s LLM Tool-Use and leverage Langgraph for monitoring MinIO’s S3 Object Store. This guide walks you through developing custom conversational AI agents and creating powerful OpenAI LLM chains for efficient data management and enhanced application functionality. Read more... Prefix vs Folder AJ AJ on Object Storage How you ever wondered how object storage creates its folder structure mimicking a POSIX style hierarchy but something that is actually built for speed and efficiency? Today in this post you will find out what actually makes the internal structure you see visually in your MInIO buckets. Read more... Powering AI/ML workflows with GitOps Automation David Cannan David Cannan on AI/ML Explore the fusion of GitOps, MinIO, Weaviate, and Python in AI development for unparalleled automation and innovation.\n",
      "This combination offers a solid foundation for creating scalable, efficient, and automated AI solutions, propelling projects from concept to reality with ease. Read more... Automated Data Prep for ML with MinIO's SDK Brenna Buuck Brenna Buuck on AI/ML This tutorial guides you through constructing robust data pipelines on the edge, ensuring flexibility and scalability. Learn to create, populate, and transform datasets seamlessly while prioritizing data privacy. Master the art of automation with MinIO's Python SDK. Read more... Replication Strategies Deep Dive AJ AJ on DevOps With all these different types of replication types floating around one has to wonder which replication strategy to use where? Today we’ll demystify these different replication strategies to see which one should be used in which scenario. Read more... Backing Up Weaviate with MinIO S3 Buckets David Cannan David Cannan on AI/ML Explore integrating MinIO with Weaviate using Docker Compose for AI-enhanced data management. Learn to back up Weaviate to MinIO S3 buckets, ensuring data integrity and scalability with practical Docker and Python examples. Streamline your AI-driven search and analysis with this robust setup. Read more... SQL Server 2022 Machine Learning Services Unlock the Value of Your Data Matt Sarrel Matt Sarrel @msarrel on Integrations Learn how to run Python stored procedures on SQL Server 2022.\n",
      "Read more... MinIO and Apache Tika: A Pattern for Text Extraction Sidharth Rajaram Sidharth Rajaram @sidharrrrrth on AI/ML Tl;dr: In this post, we will use MinIO Bucket Notifications and Apache Tika, for document text extraction, which is at the heart of critical downstream tasks like Large Language Model (LLM) training and Retrieval Augmented Generation (RAG). The Premise Let’s say that I want to construct a dataset of text that I can then use to fine-tune an Read more... Hungry GPUs Need Fast Object Storage Keith Pijanowski Keith Pijanowski on AI/ML A chain is as strong as its weakest link - and your AI/ML infrastructure is only as fast as your slowest component. If you train machine learning models with GPUs, then your weak link may be your storage solution. The result is what I call the “Starving GPU Problem.” The Starving GPU problem occurs when your network or your Read more... Why Your Enterprise AI Strategy Is Likely to Fail in 2024: Model Down vs. Data Up Jonathan Symonds Jonathan Symonds on AI/ML I suspect some folks will accuse me of clickbait titling. Others will say, that’s not really a reach - most folks will fail in their initial AI attempts but it doesn’t matter and the learnings are worth it. On some level both are right - but I think WHY enterprises will fail is worth exploration and may allow Read more...\n",
      "Innovating S3 Bucket Retrieval: Langchain Community S3 Loaders with OpenAI API David Cannan David Cannan on AI/ML Explore the synergy of MinIO, Langchain, and OpenAI in enhancing data storage and processing. This article illustrates MinIO’s integration for efficient document summarization using Langchain and OpenAI’s GPT, revolutionizing AI and ML data handling. Read more... Supercharge TileDB Engine with MinIO AJ AJ on Vector Database MinIO makes a powerful primary TileDB backend because both are built for performance and scale. Read more... Data Before Models: The Unsung Heroes Who Unlock Real AI Results Brenna Buuck Brenna Buuck on AI/ML Explore the essential role of Data Engineers in unleashing the true power of AI! Data Engineers have a critical foundation in cleaning and structuring raw data for ML success. Learn why their expertise in data infrastructure, feature engineering, and pipeline optimization is indispensable. Read more... The Strengths, Weaknesses and Dangers of LLMs Sidharth Rajaram Sidharth Rajaram @sidharrrrrth Keith Pijanowski Keith Pijanowski on AI/ML Much has been said lately about the wonders of Large Language Models (LLMs). Most of these accolades are deserved. Ask ChatGPT to describe the General Theory of Relativity and you will get a very good (and accurate) answer.\n",
      "However, at the end of the day ChatGPT is still a computer program (as are all other LLMs) that is blindly executing Read more... We Read Google’s New Egress Policy So You Don’t Have To…It Is Surprising Matt Sarrel Matt Sarrel @msarrel on GCP Google recently announced that it would eliminate data egress fees for those leaving the platform. Given our position on the cloud operating model and the lifecycle of the cloud, this appeared to be a major announcement. It is not. You could understand our initial enthusiasm. Google stated that any \"customers who wish to stop using Google Cloud and migrate Read more... Event-Driven Architecture: MinIO Event Notification Webhooks using Flask David Cannan David Cannan on Events Explore deploying MinIO and Flask with Docker-compose for event-driven architecture. Master MinIO bucket events and Flask webhooks for efficient data workflows and robust applications. Dive into the synergy of cloud technologies. Read more... Locking down MinIO Operator Permissions AJ AJ on Kubernetes In this post, we’ll show you how to configure the MinIO Operator with the most restrictive namespace permissions – all the while being able to fully utilize the power and flexibility of the MinIO Operator for day-to-day operations. Read more... Everything You Need to Know to Repatriate from AWS S3 to MinIO Matt Sarrel Matt Sarrel @msarrel on Operator's Guide Step by step instructions to plan for a migrate data off AWS S3 and on MinIO on-premise.\n",
      "Read more... Debugging MinIO Installs AJ AJ on DevOps In this blog post, we’ll show you how to debug a MinIO install running in Kubernetes and also some of the common issues you might encounter when doing bare metal installation and how to rectify them. Read more... Older Posts\n",
      "\n",
      "Source: python.langchain.com_docs_langserve.txt\n",
      "Content: LangServe 🦜️🏓 LangServe 🚩 We will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. Sign up here to get on the waitlist. Overview​ LangServe helps developers deploy LangChain runnables and chains as a REST API. This library is integrated with FastAPI and uses pydantic for data validation. In addition, it provides a client that can be used to call into runnables deployed on a server. A javascript client is available in LangChainJS. Features​ Input and Output schemas automatically inferred from your LangChain object, and enforced on every API call, with rich error messages API docs page with JSONSchema and Swagger (insert example link) Efficient /invoke/, /batch/ and /stream/ endpoints with support for many concurrent requests on a single server /stream_log/ endpoint for streaming all (or some) intermediate steps from your chain/agent new as of 0.0.40, supports astream_events to make it easier to stream without needing to parse the output of stream_log. Playground page at /playground/ with streaming output and intermediate steps Built-in (optional) tracing to LangSmith, just add your API key (see Instructions) All built with battle-tested open-source Python libraries like FastAPI, Pydantic, uvloop and asyncio.\n",
      "Use the client SDK to call a LangServe server as if it was a Runnable running locally (or call the HTTP API directly) LangServe Hub Limitations​ Client callbacks are not yet supported for events that originate on the server OpenAPI docs will not be generated when using Pydantic V2. Fast API does not support mixing pydantic v1 and v2 namespaces. See section below for more details. Hosted LangServe​ We will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. Sign up here to get on the waitlist. Security​ Vulnerability in Versions 0.0.13 - 0.0.15 -- playground endpoint allows accessing arbitrary files on server. Resolved in 0.0.16. Installation​ For both client and server: pip install \"langserve[all]\" or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code. LangChain CLI 🛠️​ Use the LangChain CLI to bootstrap a LangServe project quickly. To use the langchain CLI make sure that you have a recent version of langchain-cli installed. You can install it with pip install -U langchain-cli. langchain app new ../path/to/directory Examples​ Get your LangServe instance started quickly with LangChain Templates. For more examples, see the templates index or the examples directory. Description Links LLMs Minimal example that reserves OpenAI and Anthropic chat models. Uses async, supports batching and streaming. server , client Retriever Simple server that exposes a retriever as a runnable.\n",
      "server , client Conversational Retriever A Conversational Retriever exposed via LangServe server , client Agent without conversation history based on OpenAI tools server , client Agent with conversation history based on OpenAI tools server , client RunnableWithMessageHistory to implement chat persisted on backend, keyed off a session_id supplied by client. server , client RunnableWithMessageHistory to implement chat persisted on backend, keyed off a conversation_id supplied by client, and user_id (see Auth for implementing user_id properly). server , client Configurable Runnable to create a retriever that supports run time configuration of the index name. server , client Configurable Runnable that shows configurable fields and configurable alternatives. server , client APIHandler Shows how to use APIHandler instead of add_routes . This provides more flexibility for developers to define endpoints. Works well with all FastAPI patterns, but takes a bit more effort. server LCEL Example Example that uses LCEL to manipulate a dictionary input. server , client Auth with add_routes : Simple authentication that can be applied across all endpoints associated with app. (Not useful on its own for implementing per user logic.) server Auth with add_routes : Simple authentication mechanism based on path dependencies. (No useful on its own for implementing per user logic.) server Auth with add_routes : Implement per user logic and auth for endpoints that use per request config modifier.\n",
      "( Note : At the moment, does not integrate with OpenAPI docs.) server , client Auth with APIHandler : Implement per user logic and auth that shows how to search only within user owned documents. server , client Widgets Different widgets that can be used with playground (file upload and chat) server Widgets File upload widget used for LangServe playground. server , client Sample Application​ Server​ Here's a server that deploys an OpenAI chat model, an Anthropic chat model, and a chain that uses the Anthropic model to tell a joke about a topic. #!/usr/bin/env python from fastapi import FastAPI from langchain prompts import ChatPromptTemplate from langchain chat_models import ChatAnthropic ChatOpenAI from langserve import add_routes app FastAPI title \"LangChain Server\" version \"1.0\" description \"A simple api server using Langchain's Runnable interfaces\" add_routes app ChatOpenAI path \"/openai\" add_routes app ChatAnthropic path \"/anthropic\" model ChatAnthropic prompt ChatPromptTemplate from_template \"tell me a joke about {topic}\" add_routes app prompt model path \"/joke\" if __name__ == \"__main__\" import uvicorn uvicorn run app host \"localhost\" port 8000 Docs​ If you've deployed the server above, you can view the generated OpenAPI docs using: ⚠️ If using pydantic v2, docs will not be generated for invoke, batch, stream, stream_log. See Pydantic section below for more details. curl localhost:8000/docs make sure to add the /docs suffix.\n",
      "⚠️ Index page / is not defined by design, so curl localhost:8000 or visiting the URL will return a 404. If you want content at / define an endpoint @app.get(\"/\"). Client​ Python SDK from langchain schema import SystemMessage HumanMessage from langchain prompts import ChatPromptTemplate from langchain schema runnable import RunnableMap from langserve import RemoteRunnable openai RemoteRunnable \"http://localhost:8000/openai/\" anthropic RemoteRunnable \"http://localhost:8000/anthropic/\" joke_chain RemoteRunnable \"http://localhost:8000/joke/\" joke_chain invoke \"topic\" \"parrots\" # or async await joke_chain ainvoke \"topic\" \"parrots\" prompt SystemMessage content 'Act like either a cat or a parrot.' HumanMessage content 'Hello!'\n",
      "# Supports astream async for msg in anthropic astream prompt print msg end \"\" flush True prompt ChatPromptTemplate from_messages \"system\" \"Tell me a long story about {topic}\" # Can define custom chains chain prompt RunnableMap \"openai\" openai \"anthropic\" anthropic chain batch \"topic\" \"parrots\" \"topic\" \"cats\" In TypeScript (requires LangChain.js version 0.0.166 or later): import RemoteRunnable from \"langchain/runnables/remote\" const chain new RemoteRunnable url http://localhost:8000/joke/ const result await chain invoke topic \"cats\" Python using requests: import requests response requests post \"http://localhost:8000/joke/invoke\" json 'input' 'topic' 'cats' response json You can also use curl: curl --location --request POST 'http://localhost:8000/joke/invoke' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"input\": { \"topic\": \"cats\" }' Endpoints​ The following code: add_routes app runnable path \"/my_runnable\" adds of these endpoints to the server: POST /my_runnable/invoke - invoke the runnable on a single input POST /my_runnable/batch - invoke the runnable on a batch of inputs POST /my_runnable/stream - invoke on a single input and stream the output POST /my_runnable/stream_log - invoke on a single input and stream the output, including output of intermediate steps as it's generated POST /my_runnable/astream_events - invoke on a single input and stream events as they are generated, including from intermediate steps.\n",
      "GET /my_runnable/input_schema - json schema for input to the runnable GET /my_runnable/output_schema - json schema for output of the runnable GET /my_runnable/config_schema - json schema for config of the runnable These endpoints match the LangChain Expression Language interface -- please reference this documentation for more details. Playground​ You can find a playground page for your runnable at /my_runnable/playground/. This exposes a simple UI to configure and invoke your runnable with streaming output and intermediate steps. Widgets​ The playground supports widgets and can be used to test your runnable with different inputs. See the widgets section below for more details. Sharing​ In addition, for configurable runnables, the playground will allow you to configure the runnable and share a link with the configuration: Legacy Chains​ LangServe works with both Runnables (constructed via LangChain Expression Language) and legacy chains (inheriting from Chain). However, some of the input schemas for legacy chains may be incomplete/incorrect, leading to errors. This can be fixed by updating the input_schema property of those chains in LangChain. If you encounter any errors, please open an issue on THIS repo, and we will work to address it. Deployment​ Deploy to AWS​ You can deploy to AWS using the AWS Copilot CLI copilot init --app application-name --name service-name --type 'Load Balanced Web Service' --dockerfile './Dockerfile' --deploy Click here to learn more.\n",
      "Deploy to Azure​ You can deploy to Azure using Azure Container Apps (Serverless): az containerapp up --name [container-app-name] --source . --resource-group [resource-group-name] --environment [environment-name] --ingress external --target-port 8001 --env-vars=OPENAI_API_KEY=your_key You can find more info here Deploy to GCP​ You can deploy to GCP Cloud Run using the following command: gcloud run deploy [your-service-name] --source . --port 8001 --allow-unauthenticated --region us-central1 --set-env-vars=OPENAI_API_KEY=your_key Community Contributed​ Deploy to Railway​ Example Railway Repo Pydantic​ LangServe provides support for Pydantic 2 with some limitations. OpenAPI docs will not be generated for invoke/batch/stream/stream_log when using Pydantic V2. Fast API does not support [mixing pydantic v1 and v2 namespaces]. LangChain uses the v1 namespace in Pydantic v2. Please read the following guidelines to ensure compatibility with LangChain Except for these limitations, we expect the API endpoints, the playground and any other features to work as expected. Advanced​ Handling Authentication​ If you need to add authentication to your server, please read Fast API's documentation about dependencies and security. The below examples show how to wire up authentication logic LangServe endpoints using FastAPI primitives. You are responsible for providing the actual authentication logic, the users table etc.\n",
      "If you're not sure what you're doing, you could try using an existing solution Auth0. Using add_routes​ If you're using add_routes, see examples here. Description Links Auth with add_routes : Simple authentication that can be applied across all endpoints associated with app. (Not useful on its own for implementing per user logic.) server Auth with add_routes : Simple authentication mechanism based on path dependencies. (No useful on its own for implementing per user logic.) server Auth with add_routes : Implement per user logic and auth for endpoints that use per request config modifier. ( Note : At the moment, does not integrate with OpenAPI docs.) server , client Alternatively, you can use FastAPI's middleware. Using global dependencies and path dependencies has the advantage that auth will be properly supported in the OpenAPI docs page, but these are not sufficient for implement per user logic (e.g., making an application that can search only within user owned documents). If you need to implement per user logic, you can use the per_req_config_modifier or APIHandler (below) to implement this logic. Per User If you need authorization or logic that is user dependent, specify per_req_config_modifier when using add_routes. Use a callable receives the raw Request object and can extract relevant information from it for authentication and authorization purposes. Using APIHandler​ If you feel comfortable with FastAPI and python, you can use LangServe's APIHandler.\n",
      "Description Links Auth with APIHandler : Implement per user logic and auth that shows how to search only within user owned documents. server , client APIHandler Shows how to use APIHandler instead of add_routes . This provides more flexibility for developers to define endpoints. Works well with all FastAPI patterns, but takes a bit more effort. server , client It's a bit more work, but gives you complete control over the endpoint definitions, so you can do whatever custom logic you need for auth. Files​ LLM applications often deal with files. There are different architectures that can be made to implement file processing; at a high level: The file may be uploaded to the server via a dedicated endpoint and processed using a separate endpoint The file may be uploaded by either value (bytes of file) or reference (e.g., s3 url to file content) The processing endpoint may be blocking or non-blocking If significant processing is required, the processing may be offloaded to a dedicated process pool You should determine what is the appropriate architecture for your application. Currently, to upload files by value to a runnable, use base64 encoding for the file (multipart/form-data is not supported yet). Here's an example that shows how to use base64 encoding to send a file to a remote runnable. Remember, you can always upload files by reference (e.g., s3 url) or upload them as multipart/form-data to a dedicated endpoint.\n",
      "Custom Input and Output Types​ Input and Output types are defined on all runnables. You can access them via the input_schema and output_schema properties. LangServe uses these types for validation and documentation. If you want to override the default inferred types, you can use the with_types method. Here's a toy example to illustrate the idea: from typing import Any from fastapi import FastAPI from langchain schema runnable import RunnableLambda app FastAPI def func Any int \"\"\"Mistyped function that should accept an int but accepts anything.\"\"\" return runnable RunnableLambda func with_types input_type int add_routes app runnable Custom User Types​ Inherit from CustomUserType if you want the data to de-serialize into a pydantic model rather than the equivalent dict representation. At the moment, this type only works server side and is used to specify desired decoding behavior. If inheriting from this type the server will keep the decoded type as a pydantic model instead of converting it into a dict. from fastapi import FastAPI from langchain schema runnable import RunnableLambda from langserve import add_routes from langserve schema import CustomUserType app FastAPI class Foo CustomUserType bar int def func foo Foo int \"\"\"Sample function that expects a Foo type which is a pydantic model\"\"\" assert isinstance foo Foo return foo bar # Note that the input and output type are automatically inferred! # You do not need to specify them.\n",
      "# runnable = RunnableLambda(func).with_types( # <-- Not needed in this case # input_type=Foo, # output_type=int, add_routes app RunnableLambda func path \"/foo\" Playground Widgets​ The playground allows you to define custom widgets for your runnable from the backend. Here are a few examples: Description Links Widgets Different widgets that can be used with playground (file upload and chat) server , client Widgets File upload widget used for LangServe playground. server , client Schema​ A widget is specified at the field level and shipped as part of the JSON schema of the input type A widget must contain a key called type with the value being one of a well known list of widgets Other widget keys will be associated with values that describe paths in a JSON object type JsonPath number string number string type NameSpacedPath title string path JsonPath // Using title to mimick json schema, but can use namespace type OneOfPath oneOf JsonPath type Widget type string // Some well known type (e.g., base64file, chat etc.) key string JsonPath NameSpacedPath OneOfPath Available Widgets​ There are only two widgets that the user can specify manually right now: File Upload Widget Chat History Widget See below more information about these widgets. All other widgets on the playground UI are created and managed automatically by the UI based on the config schema of the Runnable.\n",
      "When you create Configurable Runnables, the playground should create appropriate widgets for you to control the behavior. File Upload Widget​ Allows creation of a file upload input in the UI playground for files that are uploaded as base64 encoded strings. Here's the full example. Snippet: try from pydantic v1 import Field except ImportError from pydantic import Field from langserve import CustomUserType # ATTENTION: Inherit from CustomUserType instead of BaseModel otherwise # the server will decode it into a dict instead of a pydantic model. class FileProcessingRequest CustomUserType \"\"\"Request including a base64 encoded file.\"\"\" # The extra field is used to specify a widget for the playground UI. file str Field extra \"widget\" \"type\" \"base64file\" num_chars int 100 Example widget: Chat Widget​ Look at widget example. To define a chat widget, make sure that you pass \"type\": \"chat\". \"input\" is JSONPath to the field in the Request that has the new input message. \"output\" is JSONPath to the field in the Response that has new output message(s). Don't specify these fields if the entire input or output should be used as they are ( e.g., if the output is a list of chat messages.) Here's a snippet: class ChatHistory CustomUserType chat_history List Tuple str str Field examples \"human input\" \"ai response\" extra \"widget\" \"type\" \"chat\" \"input\" \"question\" \"output\" \"answer\" question str def _format_to_messages input ChatHistory List BaseMessage \"\"\"Format the input to a list of messages.\"\"\"\n",
      "history input chat_history user_input input question messages for human ai in history messages append HumanMessage content human messages append AIMessage content ai messages append HumanMessage content user_input return messages model ChatOpenAI chat_model RunnableParallel \"answer\" RunnableLambda _format_to_messages model add_routes app chat_model with_types input_type ChatHistory config_keys \"configurable\" path \"/chat\" Example widget: Enabling / Disabling Endpoints (LangServe >=0.0.33)​ You can enable / disable which endpoints are exposed when adding routes for a given chain. Use enabled_endpoints if you want to make sure to never get a new endpoint when upgrading langserve to a newer verison. Enable: The code below will only enable invoke, batch and the corresponding config_hash endpoint variants.\n",
      "add_routes app chain enabled_endpoints \"invoke\" \"batch\" \"config_hashes\" path \"/mychain\" Disable: The code below will disable the playground for the chain add_routes app chain disabled_endpoints \"playground\" path \"/mychain\" PreviousToken counting NextLangSmith Overview Features Limitations Hosted LangServe Security Installation LangChain CLI 🛠️ Examples Sample ApplicationServerDocsClient Endpoints PlaygroundWidgetsSharing Legacy Chains DeploymentDeploy to AWSDeploy to AzureDeploy to GCPCommunity Contributed Pydantic AdvancedHandling AuthenticationFilesCustom Input and Output TypesCustom User TypesPlayground WidgetsAvailable WidgetsChat WidgetEnabling / Disabling Endpoints (LangServe >=0.0.33)\n",
      "\n",
      "Source: medium.com__aisagescribe_langchain-101-a-comprehensive-introduction-guide-7a5db81afa49.txt\n",
      "Content: Open in app Sign up Sign in Write Sign up Sign in Member-only story LangChain 101: A comprehensive introduction guide AI SageScribe·Follow 7 min read·Jan 28, 2024 -- Share In the ever-evolving landscape of machine learning and artificial intelligence, the advent of large language models (LLMs) like GPT-4 has opened a new frontier in natural language processing and generation. Enter LangChain, a cutting-edge software development framework designed to harness the power of these models in creating robust and dynamic applications. This framework is a game-changer for developers looking to integrate LLMs into their products seamlessly. Core Principles of LangChain Data-Aware Design LangChain stands out with its data-aware approach, allowing a language model to connect with various data sources. This integration enriches the model’s responses, making them more relevant and context-specific. Agentic Interaction The framework also emphasizes agentic interaction, enabling language models to actively engage with their environment. This approach paves the way for more interactive and responsive applications.\n",
      "High-level Structure of LangChain Below is a list of key modules of LangChain Models: Supported model types and integrations Prompts: Prompt template, optimization and serialization Memory: Memory refers to state that is persisted between calls of a chain/agent Indexes: This module contains interfaces and integrations for loading, querying and updating external data Chains: Chains are structured sequences of calls (to an LLM or to a different utility) Agents: An agent is a Chain in which an LLM, given a high-level directive and a set of tools, repeated decides an action, executes the action and observes the outcome untile high level directive is complete Callbacks: Callbacks let you log and stream the intermediate steps of any chain, making it easy to observe, debug and evaluate the internals of an application. LangChain’s different Module Explained Models Module -- -- Follow Written by AI SageScribe 22 Followers Thoughts on Machine Learning, Data Science, Stats Follow Help Status About Careers Blog Privacy Terms Text to speech Teams\n",
      "\n",
      "Source: sanity.cdaprod.dev_.txt\n",
      "Content: Blog. Exploring the Digital Frontier: A Next.js and Sanity CMS. Tech Journey by David Cannan. Introduction to cda.data-lake and MinIO The cda.data-lake project embodies a transformative approach to managing and processing data at scale. At its core, it leverages the robust capabilities of MinIO, an object storage solution that excels in performance and scalability. This integration empowers the project to handle an expansive array of data types and operations, ranging from simple storage to complex analytical computations and machine learning tasks. The use of MinIO ensures that the cda.data-lake can operate within a secure and compliant framework, making it a reliable foundation for data-driven innovation. As the cda.data-lake project evolves, the MinIO event notification system plays a pivotal role by automating workflows in real-time, thereby optimizing data processing and reducing manual intervention. This not only increases efficiency but also enables the system to swiftly adapt to the increasing volume and complexity of data. With MinIO's flexible and resilient infrastructure, the cda.data-lake project is set to redefine the standards of data handling and accessibility for diverse applications. David Cannan More Stories My Gartner's Peer Insights Review of MinIO - A Game Changer in Object Storage My experience with MinIO has been nothing short of fantastic. It's a testament to what a well-thought-out platform, backed by a passionate team and community, can achieve.\n",
      "David Cannan Unlocking Coding Efficiency: My Deep Dive into Langchain's StructuredTool and Pydantic Schema Discover how Langchain's StructuredTool and Pydantic Schema are revolutionizing my approach to coding. If you value code quality, data integrity, and efficiency, you won't want to miss this! David Cannan Building Custom Query Engines: A Journey of Discovery and Innovation Building custom query engines for my unique data needs has been an exhilarating journey filled with challenges, learning, and growth. From connecting various data sources to crafting intelligent search capabilities, this project has been a testament to the power of innovation and determination. David Cannan Revolutionizing Video Management with Langchain: A Comprehensive Guide Unleashing Langchain – a new era in video management! 🚀 With the muscle of ResNet50, the intelligence of SerpAPITool, and the flair of StreamlitTool, Langchain is more than just software; it's a symphony of technology and innovation. From indexing videos to classifying frames and researching keywords, it's a one-stop solution that's poised to redefine how we interact with video content. Watch out, world; Langchain is here, and it's ready to dazzle! 🎥✨ David Cannan Social Engineering in the Modern Age: Unveiling Hidden Threats & Harnessing Human Behavior The potency of social engineering lies in its ability to exploit human reactions and biases.\n",
      "Recognizing and manipulating these targeted reactions is a testament to the prowess of a skilled social engineer. While the techniques may vary, the underpinning principle remains constant: understanding human psychology to predict, influence, and achieve a desired outcome. David Cannan Unleashing the Power of Ansible and OpenAI in Infrastructure Automation In the thrilling intersection of automation, artificial intelligence, and infrastructure management, my application leverages the power of Ansible and OpenAI to innovate and automate like never before. With Ansible's robust automation capabilities, the application creates tailored roles and relevant files based on input from a CSV file. But the magic doesn't stop there - it then utilizes OpenAI's cutting-edge AI models to generate high-quality content for these files. The amalgamation of these powerful tools opens up a world of possibilities, redefining the way we perceive infrastructure automation. David Cannan Machine Learning Odyssey: Charting Unexplored Terrains in Bug Bounty Systems Embarking on a thrilling journey of machine learning integration with AWS SageMaker, this blog post explores the transformative potential of SageMaker for my bug bounty system project. Delve into my exploration of SageMaker's diverse features, my inspirations, and the anticipation of where this exciting new direction will lead.\n",
      "This is more than just a project update; it's a chronicle of my personal growth and the lessons learned on the path of technological exploration. David Cannan Building an AI-Driven Bug Bounty Hunting System In my latest endeavor to create an AI-driven bug bounty hunting system, I found myself navigating the rich world of automation and serverless architectures. Through the integration of private repositories, Docker containers, serverless functions via OpenFaaS, and a structured GraphQL approach using Apollo Server, I've managed to craft an increasingly robust backend for bug hunting. A key turning point was the introduction of OpenAI's functions feature, capable of outputting JSON, which perfectly integrated into my existing architecture using BaseTool and significantly boosted the system's capability. This venture is far from over, but the journey thus far has been nothing short of exhilarating. Join me as I continue to explore, learn, and build. David Cannan Pwnagotchi - My Journey into Network Exploration About a year ago, I ventured into creating a Pwnagotchi, an AI-powered device designed to capture crackable WPA key material from surrounding Wi-Fi environments. This project led me down an exciting path of Python programming, Wi-Fi and GPS technologies, and deep reinforcement learning.\n",
      "Today, the Pwnagotchi stands as a testament to my journey from woodworking and cabinet-making professional to an aspiring software engineer and open-source contributor David Cannan From Carpentry to Coding A year ago, I set out on what seemed to be a straightforward project - constructing a CNC machine to assist in my woodworking and cabinet-making profession. This simple beginning, however, marked the start of an extraordinary journey that would not only revolutionize my career but also reshape my perspective on life. David Cannan The Joy of Wrapping Your Code in a FaaS Container Wrapping your code in a container with OpenFaaS is a joyful experience that brings control, flexibility, and simplicity back to your workflows. Give it a try, and enjoy the new era of serverless computing! David Cannan Exploring AI with Ansible-RoleBuilder and GPT-Engineer Embarking on a journey of self-taught software development, I've explored the intersection of DevOps and AI, integrating the power of GPT-Engineer into my project, Ansible-RoleBuilder. This post chronicles my adventure, from the creation of Ansible-RoleBuilder to the lessons learned from an AI experiment, shedding light on the potential of AI in software development for novices and veterans alike. David Cannan Unleashing the Power of AI with Google Apps Script and OpenAI: A Detailed Guide Embark on a journey with us as we delve into the world of Google Apps Script and OpenAI.\n",
      "Discover how we transformed our workflow, starting from summarizing videos to creating custom functions. Learn about the ease of writing scripts, the potential use cases, and how you can start your own project today. David Cannan Balancing Hardware Design and Creativity for Bug Hunting Infrastructure as Code Exploring the exciting frontier of hardware design, Infrastructure as Code (IaC), and embedded software development, we find a unique blend of creativity and technical proficiency. These disciplines, when combined effectively, pave the way for intelligent, automated, and highly specialized systems. This blog post delves into the tangible world of hardware, the abstract realm of IaC and embedded software, and the intricate balance between creativity and technicality. The intersection of these fields holds enormous potential, making it a thrilling area for tech enthusiasts to navigate. David Cannan Bug Hunting Infrastructure: A Journey in Infrastructure as Code (IaC) Embracing Infrastructure as Code (IaC) has been a significant step in my journey as a bug bounty hunter. David Cannan Prompt Engineering: How to, and why? Unlock the power of AI with Prompt Engineering! Our latest blog post dives into the what, why, and how of Prompt Engineering, a key skill for leveraging large language models. Learn how to craft effective prompts and guide AI to generate useful and relevant outputs. Don't miss out on this comprehensive guide to one of the most exciting fields in AI!\n",
      "David Cannan Latest Trends and Developments in AI: May 2023 In this blog post, we explore five cutting-edge AI trends, including time series analysis using sARIMA models and Dash, geospatial data analysis with GeoPandas, building simple ETL pipelines with GitHub Actions, utilizing GPT-4 for poker coaching, and machine learning methods for protein design. These trends showcase the versatility and innovative applications of AI across various domains, from data processing to gaming and even biotechnology. Stay informed and up-to-date with the latest advancements in the ever-evolving field of artificial intelligence. David Cannan Mastering Modern Web Development: My Journey with Vercel, Cloudflare, and Sanity Dive into the world of a web developer adept at creating seamless experiences with Vercel, Cloudflare, and Sanity. Learn about the challenges faced and the skills gained, showcasing a unique skill set in the ever-evolving world of web development. David Cannan Building a Monorepo with CI/CD In the ever-evolving landscape of software development, practices like Continuous Integration and Continuous Deployment (CI/CD) have become crucial for efficient collaboration and rapid delivery of new features. By implementing CI/CD in a monorepo, which serves as an \"umbrella\" repository for multiple frameworks, developers can maintain a consistent and high-quality codebase while simplifying dependency management and fostering teamwork.\n",
      "This modern approach to software development ensures a seamless experience as teams work together to build, test, and deploy their projects, ultimately accelerating the release of new features and improvements. David Cannan My CNC Journey: From Rebuilding to Debugging and Beyond Embarking on a CNC machine rebuild led me to an exciting journey of learning C++ programming, Arduino PLCs, and a newfound passion for debugging devices. Dive into my story of discovery, growth, and the love for upgrading my CNC machine. David Cannan Embracing the Future of Web Development with Headless CMS Discover the power of headless CMS and its impact on the future of web development. Learn about its benefits, including flexibility, scalability, and improved performance, and how this decoupled architecture is revolutionizing the way we manage and deliver content across multiple platforms and devices. David Cannan\n",
      "\n",
      "Source: python.langchain.com_docs_langgraph.txt\n",
      "Content: LangGraph 🦜🕸️LangGraph ⚡ Building language agents as graphs ⚡ Overview​ LangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of (and intended to be used with) LangChain. It extends the LangChain Expression Language with the ability to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. It is inspired by Pregel and Apache Beam. The current interface exposed is one inspired by NetworkX. The main use is for adding cycles to your LLM application. Crucially, this is NOT a DAG framework. If you want to build a DAG, you should just use LangChain Expression Language. Cycles are important for agent-like behaviors, where you call an LLM in a loop, asking it what action to take next. Installation​ pip install langgraph Quick Start​ Here we will go over an example of creating a simple agent that uses chat models and function calling. This agent will represent all its state as a list of messages. We will need to install some LangChain packages, as well as Tavily to use as an example tool. pip install U langchain langchain_openai tavily-python We also need to export some environment variables for OpenAI and Tavily API access. export OPENAI_API_KEY sk- .. export TAVILY_API_KEY tvly- .. Optionally, we can set up LangSmith for best-in-class observability. export LANGCHAIN_TRACING_V2 \"true\" export LANGCHAIN_API_KEY ls__ .. Set up the tools​ We will first define the tools we want to use.\n",
      "For this simple example, we will use a built-in search tool via Tavily. However, it is really easy to create your own tools - see documentation here on how to do that. from langchain_community tools tavily_search import TavilySearchResults tools TavilySearchResults max_results We can now wrap these tools in a simple LangGraph ToolExecutor. This is a simple class that receives ToolInvocation objects, calls that tool, and returns the output. ToolInvocation is any class with tool and tool_input attributes. from langgraph prebuilt import ToolExecutor tool_executor ToolExecutor tools Set up the model​ Now we need to load the chat model we want to use. Importantly, this should satisfy two criteria: It should work with lists of messages. We will represent all agent state in the form of messages, so it needs to be able to work well with them. It should work with the OpenAI function calling interface. This means it should either be an OpenAI model or a model that exposes a similar interface. Note: these model requirements are not requirements for using LangGraph - they are just requirements for this one example. from langchain_openai import ChatOpenAI # We will set streaming=True so that we can stream tokens # See the streaming section for more information on this. model ChatOpenAI temperature streaming True After we've done this, we should make sure the model knows that it has these tools available to call.\n",
      "We can do this by converting the LangChain tools into the format for OpenAI function calling, and then bind them to the model class. from langchain tools render import format_tool_to_openai_function functions format_tool_to_openai_function for in tools model model bind_functions functions Define the agent state​ The main type of graph in langgraph is the StatefulGraph. This graph is parameterized by a state object that it passes around to each node. Each node then returns operations to update that state. These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute. Whether to set or add is denoted by annotating the state object you construct the graph with. For this example, the state we will track will just be a list of messages. We want each node to just add messages to that list. Therefore, we will use a TypedDict with one key (messages) and annotate it so that the messages attribute is always added to. from typing import TypedDict Annotated Sequence import operator from langchain_core messages import BaseMessage class AgentState TypedDict messages Annotated Sequence BaseMessage operator add Define the nodes​ We now need to define a few different nodes in our graph. In langgraph, a node can be either a function or a runnable. There are two main nodes we need for this: The agent: responsible for deciding what (if any) actions to take.\n",
      "A function to invoke tools: if the agent decides to take an action, this node will then execute that action. We will also need to define some edges. Some of these edges may be conditional. The reason they are conditional is that based on the output of a node, one of several paths may be taken. The path that is taken is not known until that node is run (the LLM decides). Conditional Edge: after the agent is called, we should either:a. If the agent said to take an action, then the function to invoke tools should be calledb. If the agent said that it was finished, then it should finish Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next Let's define the nodes, as well as a function to decide how what conditional edge to take.\n",
      "from langgraph prebuilt import ToolInvocation import json from langchain_core messages import FunctionMessage # Define the function that determines whether to continue or not def should_continue state messages state 'messages' last_message messages # If there is no function call, then we finish if \"function_call\" not in last_message additional_kwargs return \"end\" # Otherwise if there is, we continue else return \"continue\" # Define the function that calls the model def call_model state messages state 'messages' response model invoke messages # We return a list, because this will get added to the existing list return \"messages\" response # Define the function to execute tools def call_tool state messages state 'messages' # Based on the continue condition # we know the last message involves a function call last_message messages # We construct an ToolInvocation from the function_call action ToolInvocation tool last_message additional_kwargs \"function_call\" \"name\" tool_input json loads last_message additional_kwargs \"function_call\" \"arguments\" # We call the tool_executor and get back a response response tool_executor invoke action # We use the response to create a FunctionMessage function_message FunctionMessage content str response name action tool # We return a list, because this will get added to the existing list return \"messages\" function_message Define the graph​ We can now put it all together and define the graph!\n",
      "from langgraph graph import StateGraph END # Define a new graph workflow StateGraph AgentState # Define the two nodes we will cycle between workflow add_node \"agent\" call_model workflow add_node \"action\" call_tool # Set the entrypoint as `agent` # This means that this node is the first one called workflow set_entry_point \"agent\" # We now add a conditional edge workflow add_conditional_edges # First, we define the start node. We use `agent`. # This means these are the edges taken after the `agent` node is called. \"agent\" # Next, we pass in the function that will determine which node is called next. should_continue # Finally we pass in a mapping. # The keys are strings, and the values are other nodes. # END is a special node marking that the graph should finish. # What will happen is we will call `should_continue`, and then the output of that # will be matched against the keys in this mapping. # Based on which one it matches, that node will then be called. # If `tools`, then we call the tool node. \"continue\" \"action\" # Otherwise we finish. \"end\" END # We now add a normal edge from `tools` to `agent`. # This means that after `tools` is called, `agent` node is called next. workflow add_edge 'action' 'agent' # Finally, we compile it! # This compiles it into a LangChain Runnable, # meaning you can use it as you would any other runnable app workflow compile Use it!​ We can now use it! This now exposes the same interface as all other LangChain runnables.\n",
      "This runnable accepts a list of messages. from langchain_core messages import HumanMessage inputs \"messages\" HumanMessage content \"what is the weather in sf\" app invoke inputs This may take a little bit - it's making a few calls behind the scenes. In order to start seeing some intermediate results as they happen, we can use streaming - see below for more information on that. Streaming​ LangGraph has support for several different types of streaming. Streaming Node Output​ One of the benefits of using LangGraph is that it is easy to stream output as it's produced by each node.\n",
      "inputs \"messages\" HumanMessage content \"what is the weather in sf\" for output in app stream inputs # stream() yields dictionaries with output keyed by node name for key value in output items print f\"Output from node ' key ':\" print \"---\" print value print \"\\n---\\n\" Output from node 'agent': --- {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n \"query\": \"weather in San Francisco\"\\n}', 'name': 'tavily_search_results_json'}})]} --- Output from node 'action': --- {'messages': [FunctionMessage(content=\"[{'url': 'https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States', 'content': 'January 2024 Weather History in San Francisco California, United States Daily Precipitation in January 2024 in San Francisco Observed Weather in January 2024 in San Francisco San Francisco Temperature History January 2024 Hourly Temperature in January 2024 in San Francisco Hours of Daylight and Twilight in January 2024 in San FranciscoThis report shows the past weather for San Francisco, providing a weather history for January 2024. It features all historical weather data series we have available, including the San Francisco temperature history for January 2024. You can drill down from year to month and even day level reports by clicking on the graphs.\n",
      "'}]\", name='tavily_search_results_json')]} --- Output from node 'agent': --- {'messages': [AIMessage(content=\"I couldn't find the current weather in San Francisco. However, you can visit [WeatherSpark](https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States) to check the historical weather data for January 2024 in San Francisco.\")]} --- Output from node '__end__': --- {'messages': [HumanMessage(content='what is the weather in sf'), AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n \"query\": \"weather in San Francisco\"\\n}', 'name': 'tavily_search_results_json'}}), FunctionMessage(content=\"[{'url': 'https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States', 'content': 'January 2024 Weather History in San Francisco California, United States Daily Precipitation in January 2024 in San Francisco Observed Weather in January 2024 in San Francisco San Francisco Temperature History January 2024 Hourly Temperature in January 2024 in San Francisco Hours of Daylight and Twilight in January 2024 in San FranciscoThis report shows the past weather for San Francisco, providing a weather history for January 2024. It features all historical weather data series we have available, including the San Francisco temperature history for January 2024. You can drill down from year to month and even day level reports by clicking on the graphs.\n",
      "'}]\", name='tavily_search_results_json'), AIMessage(content=\"I couldn't find the current weather in San Francisco. However, you can visit [WeatherSpark](https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States) to check the historical weather data for January 2024 in San Francisco.\")]} --- Streaming LLM Tokens​ You can also access the LLM tokens as they are produced by each node. In this case only the \"agent\" node produces LLM tokens. In order for this to work properly, you must be using an LLM that supports streaming as well as have set it when constructing the LLM (e.g.\n",
      "ChatOpenAI(model=\"gpt-3.5-turbo-1106\", streaming=True)) inputs \"messages\" HumanMessage content \"what is the weather in sf\" async for output in app astream_log inputs include_types \"llm\" # astream_log() yields the requested logs (here LLMs) in JSONPatch format for op in output ops if op \"path\" == \"/streamed_output/-\" # this is the output from .stream() elif op \"path\" startswith \"/logs/\" and op \"path\" endswith \"/streamed_output/-\" # because we chose to only include LLMs, these are LLM tokens print op \"value\" content='' additional_kwargs={'function_call': {'arguments': '', 'name': 'tavily_search_results_json'}} content='' additional_kwargs={'function_call': {'arguments': '{\\n', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' ', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' \"', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': 'query', 'name':\n",
      "''}} content='' additional_kwargs={'function_call': {'arguments': '\":', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' \"', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': 'weather', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' in', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' San', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' Francisco', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': '\"\\n', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': '}', 'name': ''}} content='' content='' content='I' content=\"'m\" content=' sorry' content=',' content=' but' content=' I' content=' couldn' content=\"'t\" content=' find' content=' the' content=' current' content=' weather' content=' in' content=' San' content=' Francisco' content='.'\n",
      "content=' However' content=',' content=' you' content=' can' content=' check' content=' the' content=' historical' content=' weather' content=' data' content=' for' content=' January' content=' ' content='202' content='4' content=' in' content=' San' content=' Francisco' content=' [' content='here' content='](' content='https' content='://' content='we' content='athers' content='park' content='.com' content='/h' content='/m' content='/' content='557' content='/' content='202' content='4' content='/' content='1' content='/H' content='istorical' content='-' content='Weather' content='-in' content='-Jan' content='uary' content='-' content='202' content='4' content='-in' content='-S' content='an' content='-F' content='r' content='anc' content='isco' content='-Cal' content='ifornia' content='-' content='United' content='-' content='States' content=').' content='' When to Use​ When should you use this versus LangChain Expression Language? If you need cycles. Langchain Expression Language allows you to easily define chains (DAGs) but does not have a good mechanism for adding in cycles. langgraph adds that syntax. Examples​ ChatAgentExecutor: with function calling​ This agent executor takes a list of messages as input and outputs a list of messages. All agent state is represented as a list of messages. This specifically uses OpenAI function calling. This is recommended agent executor for newer chat based models that support function calling.\n",
      "Getting Started Notebook: Walks through creating this type of executor from scratch High Level Entrypoint: Walks through how to use the high level entrypoint for the chat agent executor. Modifications We also have a lot of examples highlighting how to slightly modify the base chat agent executor. These all build off the getting started notebook so it is recommended you start with that first. Human-in-the-loop: How to add a human-in-the-loop component Force calling a tool first: How to always call a specific tool first Respond in a specific format: How to force the agent to respond in a specific format Dynamically returning tool output directly: How to dynamically let the agent choose whether to return the result of a tool directly to the user Managing agent steps: How to more explicitly manage intermediate steps that an agent takes AgentExecutor​ This agent executor uses existing LangChain agents. Getting Started Notebook: Walks through creating this type of executor from scratch High Level Entrypoint: Walks through how to use the high level entrypoint for the chat agent executor. Modifications We also have a lot of examples highlighting how to slightly modify the base chat agent executor. These all build off the getting started notebook so it is recommended you start with that first.\n",
      "Human-in-the-loop: How to add a human-in-the-loop component Force calling a tool first: How to always call a specific tool first Managing agent steps: How to more explicitly manage intermediate steps that an agent takes Async​ If you are running LangGraph in async workflows, you may want to create the nodes to be async by default. For a walkthrough on how to do that, see this documentation Streaming Tokens​ Sometimes language models take a while to respond and you may want to stream tokens to end users. For a guide on how to do this, see this documentation Persistence​ LangGraph comes with built-in persistence, allowing you to save the state of the graph at point and resume from there. For a walkthrough on how to do that, see this documentation Human-in-the-loop​ LangGraph comes with built-in support for human-in-the-loop workflows. This is useful when you want to have a human review the current state before proceeding to a particular node. For a walkthrough on how to do that, see this documentation Planning Agent Examples​ The following notebooks implement agent architectures prototypical of the \"plan-and-execute\" style, where an LLM planner decomposes a user request into a program, an executor executes the program, and an LLM synthesizes a response (and/or dynamically replans) based on the program outputs.\n",
      "Plan-and-execute: a simple agent with a planner that generates a multi-step task list, an executor that invokes the tools in the plan, and a replanner that responds or generates an updated plan. Based on the Plan-and-solve paper by Wang, et. al. Reasoning without Observation: planner generates a task list whose observations are saved as variables. Variables can be used in subsequent tasks to reduce the need for further re-planning. Based on the ReWOO paper by Xu, et. al. LLMCompiler: planner generates a DAG of tasks with variable responses. Tasks are streamed and executed eagerly to minimize tool execution runtime. Based on the paper by Kim, et. al. Reflection / Self-Critique​ When output quality is a major concern, it's common to incorporate some combination of self-critique or reflection and external validation to refine your system's outputs. The following examples demonstrate research that implement this type of design. Basic Reflection: add a simple \"reflect\" step in your graph to prompt your system to revise its outputs. Reflexion: critique missing and superflous aspects of the agent's response to guide subsequent steps. Based on Reflexion, by Shinn, et. al. Language Agent Tree Search: execute multiple agents in parallel, using reflection and environmental rewards to drive a Monte Carlo Tree Search. Based on LATS, by Zhou, et. al.\n",
      "Multi-agent Examples​ Multi-agent collaboration: how to create two agents that work together to accomplish a task Multi-agent with supervisor: how to orchestrate individual agents by using an LLM as a \"supervisor\" to distribute work Hierarchical agent teams: how to orchestrate \"teams\" of agents as nested graphs that can collaborate to solve a problem Chatbot Evaluation via Simulation​ It can often be tough to evaluation chat bots in multi-turn situations. One way to do this is with simulations. Chat bot evaluation as multi-agent simulation: how to simulate a dialogue between a \"virtual user\" and your chat bot Multimodal Examples​ WebVoyager: vision-enabled web browsing agent that uses Set-of-marks prompting to navigate a web browser and execute tasks Documentation​ There are only a few new APIs to use. StateGraph​ The main entrypoint is StateGraph. from langgraph graph import StateGraph This class is responsible for constructing the graph. It exposes an interface inspired by NetworkX. This graph is parameterized by a state object that it passes around to each node. __init__​ def __init__ self schema Type Any None When constructing the graph, you need to pass in a schema for a state. Each node then returns operations to update that state. These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute. Whether to set or add is denoted by annotating the state object you construct the graph with.\n",
      "The recommended way to specify the schema is with a typed dictionary: from typing import TypedDict You can then annotate the different attributes using from typing imoport Annotated. Currently, the only supported annotation is import operator; operator.add. This annotation will make it so that any node that returns this attribute ADDS that new result to the existing value.\n",
      "Let's take a look at an example: from typing import TypedDict Annotated Union from langchain_core agents import AgentAction AgentFinish import operator class AgentState TypedDict # The input string input str # The outcome of a given call to the agent # Needs `None` as a valid type, since this is what this will start as agent_outcome Union AgentAction AgentFinish None # List of actions and corresponding observations # Here we annotate this with `operator.add` to indicate that operations to # this state should be ADDED to the existing values (not overwrite it) intermediate_steps Annotated list tuple AgentAction str operator add We can then use this like: # Initialize the StateGraph with this state graph StateGraph AgentState # Create nodes and edges # Compile the graph app graph compile # The inputs should be a dictionary, because the state is a TypedDict inputs # Let's assume this the input \"input\" \"hi\" # Let's assume agent_outcome is set by the graph as some point # It doesn't need to be provided, and it will be None by default # Let's assume `intermediate_steps` is built up over time by the graph # It doesn't need to provided, and it will be empty list by default # The reason `intermediate_steps` is an empty list and not `None` is because # it's annotated with `operator.add` .add_node​ def add_node self key str action RunnableLike None This method adds a node to the graph. It takes two arguments: key: A string representing the name of the node. This must be unique.\n",
      "action: The action to take when this node is called. This should either be a function or a runnable. .add_edge​ def add_edge self start_key str end_key str None Creates an edge from one node to the next. This means that output of the first node will be passed to the next node. It takes two arguments. start_key: A string representing the name of the start node. This key must have already been registered in the graph. end_key: A string representing the name of the end node. This key must have already been registered in the graph. .add_conditional_edges​ def add_conditional_edges self start_key str condition Callable str conditional_edge_mapping Dict str str None This method adds conditional edges. What this means is that only one of the downstream edges will be taken, and which one that is depends on the results of the start node. This takes three arguments: start_key: A string representing the name of the start node. This key must have already been registered in the graph. condition: A function to call to decide what to do next. The input will be the output of the start node. It should return a string that is present in conditional_edge_mapping and represents the edge to take. conditional_edge_mapping: A mapping of string to string. The keys should be strings that may be returned by condition. The values should be the downstream node to call if that condition is returned. .set_entry_point​ def set_entry_point self key str None The entrypoint to the graph.\n",
      "This is the node that is first called. It only takes one argument: key: The name of the node that should be called first. .set_finish_point​ def set_finish_point self key str None This is the exit point of the graph. When this node is called, the results will be the final result from the graph. It only has one argument: key: The name of the node that, when called, will return the results of calling it as the final output Note: This does not need to be called if at any point you previously created an edge (conditional or normal) to END Graph​ from langgraph graph import Graph graph Graph This has the same interface as StateGraph with the exception that it doesn't update a state object over time, and rather relies on passing around the full state from each step. This means that whatever is returned from one node is the input to the next as is. END​ from langgraph graph import END This is a special node representing the end of the graph. This means that anything passed to this node will be the final output of the graph. It can be used in two places: As the end_key in add_edge As a value in conditional_edge_mapping as passed to add_conditional_edges Prebuilt Examples​ There are also a few methods we've added to make it easy to use common, prebuilt graphs and components. ToolExecutor​ from langgraph prebuilt import ToolExecutor This is a simple helper class to help with calling tools.\n",
      "It is parameterized by a list of tools: tools tool_executor ToolExecutor tools It then exposes a runnable interface. It can be used to call tools: you can pass in an AgentAction and it will look up the relevant tool and call it with the appropriate input. chat_agent_executor.create_function_calling_executor​ from langgraph prebuilt import chat_agent_executor This is a helper function for creating a graph that works with a chat model that utilizes function calling. Can be created by passing in a model and a list of tools. The model must be one that supports OpenAI function calling. from langchain_openai import ChatOpenAI from langchain_community tools tavily_search import TavilySearchResults from langgraph prebuilt import chat_agent_executor from langchain_core messages import HumanMessage tools TavilySearchResults max_results model ChatOpenAI app chat_agent_executor create_function_calling_executor model tools inputs \"messages\" HumanMessage content \"what is the weather in sf\" for in app stream inputs print list values print \"----\" create_agent_executor​ from langgraph prebuilt import create_agent_executor This is a helper function for creating a graph that works with LangChain Agents. Can be created by passing in an agent and a list of tools.\n",
      "from langgraph prebuilt import create_agent_executor from langchain_openai import ChatOpenAI from langchain import hub from langchain agents import create_openai_functions_agent from langchain_community tools tavily_search import TavilySearchResults tools TavilySearchResults max_results # Get the prompt to use - you can modify this! prompt hub pull \"hwchase17/openai-functions-agent\" # Choose the LLM that will drive the agent llm ChatOpenAI model \"gpt-3.5-turbo-1106\" # Construct the OpenAI Functions agent agent_runnable create_openai_functions_agent llm tools prompt app create_agent_executor agent_runnable tools inputs \"input\" \"what is the weather in sf\" \"chat_history\" for in app stream inputs print list values print \"----\" PreviousLangSmith Walkthrough Overview Installation Quick StartSet up the toolsSet up the modelDefine the agent stateDefine the nodesDefine the graphUse it! StreamingStreaming Node OutputStreaming LLM Tokens When to Use ExamplesChatAgentExecutor: with function callingAgentExecutorAsyncStreaming TokensPersistenceHuman-in-the-loopPlanning Agent ExamplesReflection / Self-CritiqueMulti-agent ExamplesChatbot Evaluation via SimulationMultimodal Examples DocumentationStateGraphGraphEND Prebuilt ExamplesToolExecutorchat_agent_executor.create_function_calling_executorcreate_agent_executor\n",
      "\n",
      "Source: nanonets.com_blog_langchain_amp_.txt\n",
      "Content: A Complete LangChain Guide At its core, LangChain is an innovative framework tailored for crafting applications that leverage the capabilities of language models. It's a toolkit designed for developers to create applications that are context-aware and capable of sophisticated reasoning. This means LangChain applications can understand the context, such as prompt instructions or content grounding responses and use language models for complex reasoning tasks, like deciding how to respond or what actions to take. LangChain represents a unified approach to developing intelligent applications, simplifying the journey from concept to execution with its diverse components. While discussing the utility of LangChain for handling document data, it's crucial to mention the power of workflow automation. Nanonets' Workflow Automation platform takes efficiency to the next level by allowing you to seamlessly integrate AI and human-in-loop systems. Imagine transforming those scanned documents into actionable data without manual effort. With our intuitive platform, you can connect LangChain to a vast array of apps and services, streamlining your processes and freeing up valuable time. Learn more about how you can build robust, AI-enhanced workflows within minutes. Learn More Understanding LangChain LangChain is much more than just a framework; it's a full-fledged ecosystem comprising several integral parts. Firstly, there are the LangChain Libraries, available in both Python and JavaScript.\n",
      "These libraries are the backbone of LangChain, offering interfaces and integrations for various components. They provide a basic runtime for combining these components into cohesive chains and agents, along with ready-made implementations for immediate use. Next, we have LangChain Templates. These are a collection of deployable reference architectures tailored for a wide array of tasks. Whether you're building a chatbot or a complex analytical tool, these templates offer a solid starting point. LangServe steps in as a versatile library for deploying LangChain chains as REST APIs. This tool is essential for turning your LangChain projects into accessible and scalable web services. Lastly, LangSmith serves as a developer platform. It's designed to debug, test, evaluate, and monitor chains built on any LLM framework. The seamless integration with LangChain makes it an indispensable tool for developers aiming to refine and perfect their applications. Together, these components empower you to develop, productionize, and deploy applications with ease. With LangChain, you start by writing your applications using the libraries, referencing templates for guidance. LangSmith then helps you in inspecting, testing, and monitoring your chains, ensuring that your applications are constantly improving and ready for deployment. Finally, with LangServe, you can easily transform any chain into an API, making deployment a breeze.\n",
      "In the next sections, we will delve deeper into how to set up LangChain and begin your journey in creating intelligent, language model-powered applications. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams. Get Started Installation and Setup Are you ready to dive into the world of LangChain? Setting it up is straightforward, and this guide will walk you through the process step-by-step. The first step in your LangChain journey is to install it. You can do this easily using pip or conda. Run the following command in your terminal: For those who prefer the latest features and are comfortable with a bit more adventure, you can install LangChain directly from the source. Clone the repository and navigate to the langchain/libs/langchain directory. Then, run: For experimental features, consider installing langchain-experimental. It's a package that contains cutting-edge code and is intended for research and experimental purposes. Install it using: LangChain CLI is a handy tool for working with LangChain templates and LangServe projects. To install the LangChain CLI, use: LangServe is essential for deploying your LangChain chains as a REST API. It gets installed alongside the LangChain CLI. LangChain often requires integrations with model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.\n",
      "Install the OpenAI Python package using: To access the API, set your OpenAI API key as an environment variable: Alternatively, pass the key directly in your python environment: LangChain allows for the creation of language model applications through modules. These modules can either stand alone or be composed for complex use cases. These modules are - Model I/O: Facilitates interaction with various language models, handling their inputs and outputs efficiently. Retrieval: Enables access to and interaction with application-specific data, crucial for dynamic data utilization. Agents: Empower applications to select appropriate tools based on high-level directives, enhancing decision-making capabilities. Chains: Offers pre-defined, reusable compositions that serve as building blocks for application development. Memory: Maintains application state across multiple chain executions, essential for context-aware interactions. Each module targets specific development needs, making LangChain a comprehensive toolkit for creating advanced language model applications. Along with the above components, we also have LangChain Expression Language (LCEL), which is a declarative way to easily compose modules together, and this enables the chaining of components using a universal Runnable interface. LCEL looks something like this - Now that we have covered the basics, we will continue on to: Dig deeper into each Langchain module in detail. Learn how to use LangChain Expression Language.\n",
      "Explore common use cases and implement them. Deploy an end-to-end application with LangServe. Check out LangSmith for debugging, testing, and monitoring. Let's get started! Module I : Model I/O In LangChain, the core element of any application revolves around the language model. This module provides the essential building blocks to interface effectively with any language model, ensuring seamless integration and communication. Key Components of Model I/O LLMs and Chat Models (used interchangeably):LLMs:Definition: Pure text completion models.Input/Output: Take a text string as input and return a text string as output.Chat Models Definition: Models that use a language model as a base but differ in input and output formats. Input/Output: Accept a list of chat messages as input and return a Chat Message. Prompts: Templatize, dynamically select, and manage model inputs. Allows for the creation of flexible and context-specific prompts that guide the language model's responses. Output Parsers: Extract and format information from model outputs. Useful for converting the raw output of language models into structured data or specific formats needed by the application. LLMs LangChain's integration with Large Language Models (LLMs) like OpenAI, Cohere, and Hugging Face is a fundamental aspect of its functionality. LangChain itself does not host LLMs but offers a uniform interface to interact with various LLMs.\n",
      "This section provides an overview of using the OpenAI LLM wrapper in LangChain, applicable to other LLM types as well. We have already installed this in the \"Getting Started\" section. Let us initialize the LLM. LLMs implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls. LLMs accept strings as inputs, or objects which can be coerced to string prompts, including List[BaseMessage] and PromptValue. (more on these later) Let us look at some examples. You can alternatively call the stream method to stream the text response. Chat Models LangChain's integration with chat models, a specialized variation of language models, is essential for creating interactive chat applications. While they utilize language models internally, chat models present a distinct interface centered around chat messages as inputs and outputs. This section provides a detailed overview of using OpenAI's chat model in LangChain. Chat models primarily accept List[BaseMessage] as inputs. Strings can be converted to HumanMessage, and PromptValue is also supported. Prompts Prompts are essential in guiding language models to generate relevant and coherent outputs. They can range from simple instructions to complex few-shot examples. In LangChain, handling prompts can be a very streamlined process, thanks to several dedicated classes and functions.\n",
      "LangChain's PromptTemplate class is a versatile tool for creating string prompts. It uses Python's str.format syntax, allowing for dynamic prompt generation. You can define a template with placeholders and fill them with specific values as needed. For chat models, prompts are more structured, involving messages with specific roles. LangChain offers ChatPromptTemplate for this purpose. This approach allows for the creation of interactive, engaging chatbots with dynamic responses. Both PromptTemplate and ChatPromptTemplate integrate seamlessly with the LangChain Expression Language (LCEL), enabling them to be part of larger, complex workflows. We will discuss more on this later. Custom prompt templates are sometimes essential for tasks requiring unique formatting or specific instructions. Creating a custom prompt template involves defining input variables and a custom formatting method. This flexibility allows LangChain to cater to a wide array of application-specific requirements. Read more here. LangChain also supports few-shot prompting, enabling the model to learn from examples. This feature is vital for tasks requiring contextual understanding or specific patterns. Few-shot prompt templates can be built from a set of examples or by utilizing an Example Selector object. Read more here. Output Parsers Output parsers play a crucial role in Langchain, enabling users to structure the responses generated by language models.\n",
      "In this section, we will explore the concept of output parsers and provide code examples using Langchain's PydanticOutputParser, SimpleJsonOutputParser, CommaSeparatedListOutputParser, DatetimeOutputParser, and XMLOutputParser. PydanticOutputParser Langchain provides the PydanticOutputParser for parsing responses into Pydantic data structures. Below is a step-by-step example of how to use it: The output will be: SimpleJsonOutputParser Langchain's SimpleJsonOutputParser is used when you want to parse JSON-like outputs. Here's an example: CommaSeparatedListOutputParser The CommaSeparatedListOutputParser is handy when you want to extract comma-separated lists from model responses. Here's an example: DatetimeOutputParser Langchain's DatetimeOutputParser is designed to parse datetime information. Here's how to use it: These examples showcase how Langchain's output parsers can be used to structure various types of model responses, making them suitable for different applications and formats. Output parsers are a valuable tool for enhancing the usability and interpretability of language model outputs in Langchain. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams. Get Started Module II : Retrieval Retrieval in LangChain plays a crucial role in applications that require user-specific data, not included in the model's training set.\n",
      "This process, known as Retrieval Augmented Generation (RAG), involves fetching external data and integrating it into the language model's generation process. LangChain provides a comprehensive suite of tools and functionalities to facilitate this process, catering to both simple and complex applications. LangChain achieves retrieval through a series of components which we will discuss one by one. Document Loaders Document loaders in LangChain enable the extraction of data from various sources. With over 100 loaders available, they support a range of document types, apps and sources (private s3 buckets, public websites, databases). You can choose a document loader based on your requirements here. All these loaders ingest data into Document classes. We'll learn how to use data ingested into Document classes later. Text File Loader: Load a simple .txt file into a document. CSV Loader: Load a CSV file into a document. We can choose to customize the parsing by specifying field names - PDF Loaders: PDF Loaders in LangChain offer various methods for parsing and extracting content from PDF files. Each loader caters to different requirements and uses different underlying libraries. Below are detailed examples for each loader. PyPDFLoader is used for basic PDF parsing. MathPixLoader is ideal for extracting mathematical content and diagrams. PyMuPDFLoader is fast and includes detailed metadata extraction. PDFMiner Loader is used for more granular control over text extraction.\n",
      "AmazonTextractPDFParser utilizes AWS Textract for OCR and other advanced PDF parsing features. PDFMinerPDFasHTMLLoader generates HTML from PDF for semantic parsing. PDFPlumberLoader provides detailed metadata and supports one document per page. Integrated Loaders: LangChain offers a wide variety of custom loaders to directly load data from your apps (such as Slack, Sigma, Notion, Confluence, Google Drive and many more) and databases and use them in LLM applications. The complete list is here. Below are a couple of examples to illustrate this - Example I - Slack Slack, a widely-used instant messaging platform, can be integrated into LLM workflows and applications. Go to your Slack Workspace Management page. Navigate to {your_slack_domain}.slack.com/services/export. Select the desired date range and initiate the export. Slack notifies via email and DM once the export is ready. The export results in a .zip file located in your Downloads folder or your designated download path. Assign the path of the downloaded .zip file to LOCAL_ZIPFILE. Use the SlackDirectoryLoader from the langchain.document_loaders package. Example II - Figma Figma, a popular tool for interface design, offers a REST API for data integration. Obtain the Figma file key from the URL format: https://www.figma.com/file/{filekey}/sampleFilename. Node IDs are found in the URL parameter ?node-id={node_id}. Generate an access token following instructions at the Figma Help Center.\n",
      "The FigmaFileLoader class from langchain.document_loaders.figma is used to load Figma data. Various LangChain modules like CharacterTextSplitter, ChatOpenAI, etc., are employed for processing. The generate_code function uses the Figma data to create HTML/CSS code. It employs a templated conversation with a GPT-based model. The generate_code function, when executed, returns HTML/CSS code based on the Figma design input. Let us now use our knowledge to create a few document sets. We first load a PDF, the BCG annual sustainability report. We use the PyPDFLoader for this. We will ingest data from Airtable now. We have an Airtable containing information about various OCR and data extraction models - Let us use the AirtableLoader for this, found in the list of integrated loaders. Let us now proceed and learn how to use these document classes. Document Transformers Document transformers in LangChain are essential tools designed to manipulate documents, which we created in our previous subsection. They are used for tasks such as splitting long documents into smaller chunks, combining, and filtering, which are crucial for adapting documents to a model's context window or meeting specific application needs. One such tool is the RecursiveCharacterTextSplitter, a versatile text splitter that uses a character list for splitting. It allows parameters like chunk size, overlap, and starting index.\n",
      "Here's an example of how it's used in Python: Another tool is the CharacterTextSplitter, which splits text based on a specified character and includes controls for chunk size and overlap: The HTMLHeaderTextSplitter is designed to split HTML content based on header tags, retaining the semantic structure: A more complex manipulation can be achieved by combining HTMLHeaderTextSplitter with another splitter, like the Pipelined Splitter: LangChain also offers specific splitters for different programming languages, like the Python Code Splitter and the JavaScript Code Splitter: For splitting text based on token count, which is useful for language models with token limits, the TokenTextSplitter is used: Finally, the LongContextReorder reorders documents to prevent performance degradation in models due to long contexts: These tools demonstrate various ways to transform documents in LangChain, from simple text splitting to complex reordering and language-specific splitting. For more in-depth and specific use cases, the LangChain documentation and Integrations section should be consulted. In our examples, the loaders have already created chunked documents for us, and this part is already handled. Text Embedding Models Text embedding models in LangChain provide a standardized interface for various embedding model providers like OpenAI, Cohere, and Hugging Face.\n",
      "These models transform text into vector representations, enabling operations like semantic search through text similarity in vector space. To get started with text embedding models, you typically need to install specific packages and set up API keys. We have already done this for OpenAI In LangChain, the embed_documents method is used to embed multiple texts, providing a list of vector representations. For instance: For embedding a single text, such as a search query, the embed_query method is used. This is useful for comparing a query to a set of document embeddings. For example: Understanding these embeddings is crucial. Each piece of text is converted into a vector, the dimension of which depends on the model used. For instance, OpenAI models typically produce 1536-dimensional vectors. These embeddings are then used for retrieving relevant information. LangChain's embedding functionality is not limited to OpenAI but is designed to work with various providers. The setup and usage might slightly differ depending on the provider, but the core concept of embedding texts into vector space remains the same. For detailed usage, including advanced configurations and integrations with different embedding model providers, the LangChain documentation in the Integrations section is a valuable resource. Vector Stores Vector stores in LangChain support the efficient storage and searching of text embeddings.\n",
      "LangChain integrates with over 50 vector stores, providing a standardized interface for ease of use. Example: Storing and Searching Embeddings After embedding texts, we can store them in a vector store like Chroma and perform similarity searches: Let us alternatively use the FAISS vector store to create indexes for our documents. Retrievers Retrievers in LangChain are interfaces that return documents in response to an unstructured query. They are more general than vector stores, focusing on retrieval rather than storage. Although vector stores can be used as a retriever's backbone, there are other types of retrievers as well. To set up a Chroma retriever, you first install it using pip install chromadb. Then, you load, split, embed, and retrieve documents using a series of Python commands. Here's a code example for setting up a Chroma retriever: The MultiQueryRetriever automates prompt tuning by generating multiple queries for a user input query and combines the results. Here's an example of its simple usage: Contextual Compression in LangChain compresses retrieved documents using the context of the query, ensuring only relevant information is returned. This involves content reduction and filtering out less relevant documents. The following code example shows how to use Contextual Compression Retriever: The EnsembleRetriever combines different retrieval algorithms to achieve better performance.\n",
      "An example of combining BM25 and FAISS Retrievers is shown in the following code: MultiVector Retriever in LangChain allows querying documents with multiple vectors per document, which is useful for capturing different semantic aspects within a document. Methods for creating multiple vectors include splitting into smaller chunks, summarizing, or generating hypothetical questions. For splitting documents into smaller chunks, the following Python code can be used: Generating summaries for better retrieval due to more focused content representation is another method. Here's an example of generating summaries: Generating hypothetical questions relevant to each document using LLM is another approach. This can be done with the following code: The Parent Document Retriever is another retriever that strikes a balance between embedding accuracy and context retention by storing small chunks and retrieving their larger parent documents. Its implementation is as follows: A self-querying retriever constructs structured queries from natural language inputs and applies them to its underlying VectorStore. Its implementation is shown in the following code: The WebResearchRetriever performs web research based on a given query - For our examples, we can also use the standard retriever already implemented as part of our vector store object as follows - We can now query the retrievers. The output of our query will be document objects relevant to the query.\n",
      "These will be ultimately utilized to create relevant responses in further sections. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams. Get Started Module III : Agents LangChain introduces a powerful concept called \"Agents\" that takes the idea of chains to a whole new level. Agents leverage language models to dynamically determine sequences of actions to perform, making them incredibly versatile and adaptive. Unlike traditional chains, where actions are hardcoded in code, agents employ language models as reasoning engines to decide which actions to take and in what order. The Agent is the core component responsible for decision-making. It harnesses the power of a language model and a prompt to determine the next steps to achieve a specific objective. The inputs to an agent typically include: Tools: Descriptions of available tools (more on this later). User Input: The high-level objective or query from the user. Intermediate Steps: A history of (action, tool output) pairs executed to reach the current user input. Tools Tools are interfaces that an agent can use to interact with the world. They enable agents to perform various tasks, such as searching the web, running shell commands, or accessing external APIs. In LangChain, tools are essential for extending the capabilities of agents and enabling them to accomplish diverse tasks.\n",
      "To use tools in LangChain, you can load them using the following snippet: Some tools may require a base Language Model (LLM) to initialize. In such cases, you can pass an LLM as well: This setup allows you to access a variety of tools and integrate them into your agent's workflows. The complete list of tools with usage documentation is here. Let us look at some examples of Tools. DuckDuckGo The DuckDuckGo tool enables you to perform web searches using its search engine. Here's how to use it: DataForSeo The DataForSeo toolkit allows you to obtain search engine results using the DataForSeo API. To use this toolkit, you'll need to set up your API credentials. Here's how to configure the credentials: Once your credentials are set, you can create a DataForSeoAPIWrapper tool to access the API: The DataForSeoAPIWrapper tool retrieves search engine results from various sources. You can customize the type of results and fields returned in the JSON response. For example, you can specify the result types, fields, and set a maximum count for the number of top results to return: This example customizes the JSON response by specifying result types, fields, and limiting the number of results. You can also specify the location and language for your search results by passing additional parameters to the API wrapper: By providing location and language parameters, you can tailor your search results to specific regions and languages.\n",
      "You have the flexibility to choose the search engine you want to use. Simply specify the desired search engine: In this example, the search is customized to use Bing as the search engine. The API wrapper also allows you to specify the type of search you want to perform. For instance, you can perform a maps search: This customizes the search to retrieve maps-related information. Shell (bash) The Shell toolkit provides agents with access to the shell environment, allowing them to execute shell commands. This feature is powerful but should be used with caution, especially in sandboxed environments. Here's how you can use the Shell tool: In this example, the Shell tool runs two shell commands: echoing \"Hello World!\" and displaying the current time. You can provide the Shell tool to an agent to perform more complex tasks. Here's an example of an agent fetching links from a web page using the Shell tool: In this scenario, the agent uses the Shell tool to execute a sequence of commands to fetch, filter, and sort URLs from a web page. The examples provided demonstrate some of the tools available in LangChain. These tools ultimately extend the capabilities of agents (explored in next subsection) and empower them to perform various tasks efficiently. Depending on your requirements, you can choose the tools and toolkits that best suit your project's needs and integrate them into your agent's workflows. Back to Agents Let's move on to agents now.\n",
      "The AgentExecutor is the runtime environment for an agent. It is responsible for calling the agent, executing the actions it selects, passing the action outputs back to the agent, and repeating the process until the agent finishes. In pseudocode, the AgentExecutor might look something like this: The AgentExecutor handles various complexities, such as dealing with cases where the agent selects a non-existent tool, handling tool errors, managing agent-produced outputs, and providing logging and observability at all levels. While the AgentExecutor class is the primary agent runtime in LangChain, there are other, more experimental runtimes supported, including: Plan-and-execute Agent Baby AGI Auto GPT To gain a better understanding of the agent framework, let's build a basic agent from scratch, and then move on to explore pre-built agents. Before we dive into building the agent, it's essential to revisit some key terminology and schema: AgentAction: This is a data class representing the action an agent should take. It consists of a tool property (the name of the tool to invoke) and a tool_input property (the input for that tool). AgentFinish: This data class indicates that the agent has finished its task and should return a response to the user. It typically includes a dictionary of return values, often with a key \"output\" containing the response text. Intermediate Steps: These are the records of previous agent actions and corresponding outputs.\n",
      "They are crucial for passing context to future iterations of the agent. In our example, we will use OpenAI Function Calling to create our agent. This approach is reliable for agent creation. We'll start by creating a simple tool that calculates the length of a word. This tool is useful because language models can sometimes make mistakes due to tokenization when counting word lengths. First, let's load the language model we'll use to control the agent: Let's test the model with a word length calculation: The response should indicate the number of letters in the word \"educa.\" Next, we'll define a simple Python function to calculate the length of a word: We've created a tool named get_word_length that takes a word as input and returns its length. Now, let's create the prompt for the agent. The prompt instructs the agent on how to reason and format the output. In our case, we're using OpenAI Function Calling, which requires minimal instructions. We'll define the prompt with placeholders for user input and agent scratchpad: Now, how does the agent know which tools it can use? We're relying on OpenAI function calling language models, which require functions to be passed separately. To provide our tools to the agent, we'll format them as OpenAI function calls: Now, we can create the agent by defining input mappings and connecting the components: This is LCEL language. We will discuss this later in detail.\n",
      "We've created our agent, which understands user input, uses available tools, and formats output. Now, let's interact with it: The agent should respond with an AgentAction, indicating the next action to take. We've created the agent, but now we need to write a runtime for it. The simplest runtime is one that continuously calls the agent, executes actions, and repeats until the agent finishes. Here's an example: In this loop, we repeatedly call the agent, execute actions, and update the intermediate steps until the agent finishes. We also handle tool interactions within the loop. To simplify this process, LangChain provides the AgentExecutor class, which encapsulates agent execution and offers error handling, early stopping, tracing, and other improvements. Let's use AgentExecutor to interact with the agent: AgentExecutor simplifies the execution process and provides a convenient way to interact with the agent. Memory is also discussed in detail later. The agent we've created so far is stateless, meaning it doesn't remember previous interactions. To enable follow-up questions and conversations, we need to add memory to the agent. This involves two steps: Add a memory variable in the prompt to store chat history. Keep track of the chat history during interactions.\n",
      "Let's start by adding a memory placeholder in the prompt: Now, create a list to track the chat history: In the agent creation step, we'll include the memory as well: Now, when running the agent, make sure to update the chat history: This enables the agent to maintain a conversation history and answer follow-up questions based on previous interactions. Congratulations! You've successfully created and executed your first end-to-end agent in LangChain. To delve deeper into LangChain's capabilities, you can explore: Different agent types supported. Pre-built Agents How to work with tools and tool integrations. Agent Types LangChain offers various agent types, each suited for specific use cases. Here are some of the available agents: Zero-shot ReAct: This agent uses the ReAct framework to choose tools based solely on their descriptions. It requires descriptions for each tool and is highly versatile. Structured input ReAct: This agent handles multi-input tools and is suitable for complex tasks like navigating a web browser. It uses a tools' argument schema for structured input. OpenAI Functions: Specifically designed for models fine-tuned for function calling, this agent is compatible with models like gpt-3.5-turbo-0613 and gpt-4-0613. We used this to create our first agent above. Conversational: Designed for conversational settings, this agent uses ReAct for tool selection and utilizes memory to remember previous interactions.\n",
      "Self-ask with search: This agent relies on a single tool, \"Intermediate Answer,\" which looks up factual answers to questions. It's equivalent to the original self-ask with search paper. ReAct document store: This agent interacts with a document store using the ReAct framework. It requires \"Search\" and \"Lookup\" tools and is similar to the original ReAct paper's Wikipedia example. Explore these agent types to find the one that best suits your needs in LangChain. These agents allow you to bind set of tools within them to handle actions and generate responses. Learn more on how to build your own agent with tools here. Prebuilt Agents Let's continue our exploration of agents, focusing on prebuilt agents available in LangChain. Gmail LangChain offers a Gmail toolkit that allows you to connect your LangChain email to the Gmail API. To get started, you'll need to set up your credentials, which are explained in the Gmail API documentation. Once you have downloaded the credentials.json file, you can proceed with using the Gmail API. Additionally, you'll need to install some required libraries using the following commands: You can create the Gmail toolkit as follows: You can also customize authentication as per your needs. Behind the scenes, a googleapi resource is created using the following methods: The toolkit offers various tools that can be used within an agent, including: GmailCreateDraft: Create a draft email with specified message fields. GmailSendMessage: Send email messages.\n",
      "GmailSearch: Search for email messages or threads. GmailGetMessage: Fetch an email by message ID. GmailGetThread: Search for email messages. To use these tools within an agent, you can initialize the agent as follows: Here are a couple of examples of how these tools can be used: Create a Gmail draft for editing: Search for the latest email in your drafts: These examples demonstrate the capabilities of LangChain's Gmail toolkit within an agent, enabling you to interact with Gmail programmatically. SQL Database Agent This section provides an overview of an agent designed to interact with SQL databases, particularly the Chinook database. This agent can answer general questions about a database and recover from errors. Please note that it is still in active development, and not all answers may be correct. Be cautious when running it on sensitive data, as it may perform DML statements on your database. To use this agent, you can initialize it as follows: This agent can be initialized using the ZERO_SHOT_REACT_DESCRIPTION agent type. It is designed to answer questions and provide descriptions. Alternatively, you can initialize the agent using the OPENAI_FUNCTIONS agent type with OpenAI's GPT-3.5-turbo model, which we used in our earlier client. Disclaimer The query chain may generate insert/update/delete queries. Be cautious, and use a custom prompt or create a SQL user without write permissions if needed.\n",
      "Be aware that running certain queries, such as \"run the biggest query possible,\" could overload your SQL database, especially if it contains millions of rows. Data warehouse-oriented databases often support user-level quotas to limit resource usage. You can ask the agent to describe a table, such as the \"playlisttrack\" table. Here's an example of how to do it: The agent will provide information about the table's schema and sample rows. If you mistakenly ask about a table that doesn't exist, the agent can recover and provide information about the closest matching table. For example: The agent will find the nearest matching table and provide information about it. You can also ask the agent to run queries on the database. For instance: The agent will execute the query and provide the result, such as the country with the highest total sales. To get the total number of tracks in each playlist, you can use the following query: The agent will return the playlist names along with the corresponding total track counts. In cases where the agent encounters errors, it can recover and provide accurate responses. For instance: Even after encountering an initial error, the agent will adjust and provide the correct answer, which, in this case, is the top 3 best-selling artists. Pandas DataFrame Agent This section introduces an agent designed to interact with Pandas DataFrames for question-answering purposes.\n",
      "Please note that this agent utilizes the Python agent under the hood to execute Python code generated by a language model (LLM). Exercise caution when using this agent to prevent potential harm from malicious Python code generated by the LLM. You can initialize the Pandas DataFrame agent as follows: You can ask the agent to count the number of rows in the DataFrame: The agent will execute the code df.shape[0] and provide the answer, such as \"There are 891 rows in the dataframe.\" You can also ask the agent to filter rows based on specific criteria, such as finding the number of people with more than 3 siblings: The agent will execute the code df[df['SibSp'] > 3].shape[0] and provide the answer, such as \"30 people have more than 3 siblings.\" If you want to calculate the square root of the average age, you can ask the agent: The agent will calculate the average age using df['Age'].mean() and then calculate the square root using math.sqrt(). It will provide the answer, such as \"The square root of the average age is 5.449689683556195.\" Let's create a copy of the DataFrame, and missing age values are filled with the mean age: Then, you can initialize the agent with both DataFrames and ask it a question: The agent will compare the age columns in both DataFrames and provide the answer, such as \"177 rows in the age column are different.\" Jira Toolkit This section explains how to use the Jira toolkit, which allows agents to interact with a Jira instance.\n",
      "You can perform various actions such as searching for issues and creating issues using this toolkit. It utilizes the atlassian-python-api library. To use this toolkit, you need to set environment variables for your Jira instance, including JIRA_API_TOKEN, JIRA_USERNAME, and JIRA_INSTANCE_URL. Additionally, you may need to set your OpenAI API key as an environment variable. To get started, install the atlassian-python-api library and set the required environment variables: You can instruct the agent to create a new issue in a specific project with a summary and description: The agent will execute the necessary actions to create the issue and provide a response, such as \"A new issue has been created in project PW with the summary 'Make more fried rice' and description 'Reminder to make more fried rice'.\" This allows you to interact with your Jira instance using natural language instructions and the Jira toolkit. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams. Get Started Module IV : Chains LangChain is a tool designed for utilizing Large Language Models (LLMs) in complex applications. It provides frameworks for creating chains of components, including LLMs and other types of components. Two primary frameworks The LangChain Expression Language (LCEL) Legacy Chain interface The LangChain Expression Language (LCEL) is a syntax that allows for intuitive composition of chains.\n",
      "It supports advanced features like streaming, asynchronous calls, batching, parallelization, retries, fallbacks, and tracing. For example, you can compose a prompt, model, and output parser in LCEL as shown in the following code: Alternatively, the LLMChain is an option similar to LCEL for composing components. The LLMChain example is as follows: Chains in LangChain can also be stateful by incorporating a Memory object. This allows for data persistence across calls, as shown in this example: LangChain also supports integration with OpenAI's function-calling APIs, which is useful for obtaining structured outputs and executing functions within a chain. For getting structured outputs, you can specify them using Pydantic classes or JsonSchema, as illustrated below: For structured outputs, a legacy approach using LLMChain is also available: LangChain leverages OpenAI functions to create various specific chains for different purposes. These include chains for extraction, tagging, OpenAPI, and QA with citations. In the context of extraction, the process is similar to the structured output chain but focuses on information or entity extraction. For tagging, the idea is to label a document with classes such as sentiment, language, style, covered topics, or political tendency. An example of how tagging works in LangChain can be demonstrated with a Python code.\n",
      "The process begins with installing the necessary packages and setting up the environment: The schema for tagging is defined, specifying the properties and their expected types: Examples of running the tagging chain with different inputs show the model's ability to interpret sentiments, languages, and aggressiveness: For finer control, the schema can be defined more specifically, including possible values, descriptions, and required properties. An example of this enhanced control is shown below: Pydantic schemas can also be used for defining tagging criteria, providing a Pythonic way to specify required properties and types: Additionally, LangChain's metadata tagger document transformer can be used to extract metadata from LangChain Documents, offering similar functionality to the tagging chain but applied to a LangChain Document. Citing retrieval sources is another feature of LangChain, using OpenAI functions to extract citations from text. This is demonstrated in the following code: In LangChain, chaining in Large Language Model (LLM) applications typically involves combining a prompt template with an LLM and optionally an output parser. The recommended way to do this is through the LangChain Expression Language (LCEL), although the legacy LLMChain approach is also supported. Using LCEL, the BasePromptTemplate, BaseLanguageModel, and BaseOutputParser all implement the Runnable interface and can be easily piped into one another.\n",
      "Here's an example demonstrating this: Routing in LangChain allows for creating non-deterministic chains where the output of a previous step determines the next step. This helps in structuring and maintaining consistency in interactions with LLMs. For instance, if you have two templates optimized for different types of questions, you can choose the template based on user input. Here's how you can achieve this using LCEL with a RunnableBranch, which is initialized with a list of (condition, runnable) pairs and a default runnable: The final chain is then constructed using various components, such as a topic classifier, prompt branch, and an output parser, to determine the flow based on the topic of the input: This approach exemplifies the flexibility and power of LangChain in handling complex queries and routing them appropriately based on the input. In the realm of language models, a common practice is to follow up an initial call with a series of subsequent calls, using the output of one call as input for the next. This sequential approach is especially beneficial when you want to build on the information generated in previous interactions. While the LangChain Expression Language (LCEL) is the recommended method for creating these sequences, the SequentialChain method is still documented for its backward compatibility. To illustrate this, let's consider a scenario where we first generate a play synopsis and then a review based on that synopsis.\n",
      "Using Python's langchain.prompts, we create two PromptTemplate instances: one for the synopsis and another for the review. Here's the code to set up these templates: In the LCEL approach, we chain these prompts with ChatOpenAI and StrOutputParser to create a sequence that first generates a synopsis and then a review. The code snippet is as follows: If we need both the synopsis and the review, we can use RunnablePassthrough to create a separate chain for each and then combine them: For scenarios involving more complex sequences, the SequentialChain method comes into play. This allows for multiple inputs and outputs. Consider a case where we need a synopsis based on a play's title and era. Here's how we might set it up: In scenarios where you want to maintain context throughout a chain or for a later part of the chain, SimpleMemory can be used. This is particularly useful for managing complex input/output relationships. For instance, in a scenario where we want to generate social media posts based on a play's title, era, synopsis, and review, SimpleMemory can help manage these variables: In addition to sequential chains, there are specialized chains for working with documents. Each of these chains serves a different purpose, from combining documents to refining answers based on iterative document analysis, to mapping and reducing document content for summarization or re-ranking based on scored responses.\n",
      "These chains can be recreated with LCEL for additional flexibility and customization. StuffDocumentsChain combines a list of documents into a single prompt passed to an LLM. RefineDocumentsChain updates its answer iteratively for each document, suitable for tasks where documents exceed the model's context capacity. MapReduceDocumentsChain applies a chain to each document individually and then combines the results. MapRerankDocumentsChain scores each document-based response and selects the highest-scoring one. Here's an example of how you might set up a MapReduceDocumentsChain using LCEL: This configuration allows for a detailed and comprehensive analysis of document content, leveraging the strengths of LCEL and the underlying language model. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams. Get Started Module V : Memory In LangChain, memory is a fundamental aspect of conversational interfaces, allowing systems to reference past interactions. This is achieved through storing and querying information, with two primary actions: reading and writing. The memory system interacts with a chain twice during a run, augmenting user inputs and storing the inputs and outputs for future reference. Building Memory into a System Storing Chat Messages: The LangChain memory module integrates various methods to store chat messages, ranging from in-memory lists to databases.\n",
      "This ensures that all chat interactions are recorded for future reference. Querying Chat Messages: Beyond storing chat messages, LangChain employs data structures and algorithms to create a useful view of these messages. Simple memory systems might return recent messages, while more advanced systems could summarize past interactions or focus on entities mentioned in the current interaction. To demonstrate the use of memory in LangChain, consider the ConversationBufferMemory class, a simple memory form that stores chat messages in a buffer. Here's an example: When integrating memory into a chain, it's crucial to understand the variables returned from memory and how they're used in the chain. For instance, the load_memory_variables method helps align the variables read from memory with the chain's expectations. End-to-End Example with LangChain Consider using ConversationBufferMemory in an LLMChain. The chain, combined with an appropriate prompt template and the memory, provides a seamless conversational experience. Here's a simplified example: This example illustrates how LangChain's memory system integrates with its chains to provide a coherent and contextually aware conversational experience. Memory Types in Langchain Langchain offers various memory types that can be utilized to enhance interactions with the AI models. Each memory type has its own parameters and return types, making them suitable for different scenarios.\n",
      "Let's explore some of the memory types available in Langchain along with code examples. 1. Conversation Buffer Memory This memory type allows you to store and extract messages from conversations. You can extract the history as a string or as a list of messages. You can also use Conversation Buffer Memory in a chain for chat-like interactions. 2. Conversation Buffer Window Memory This memory type keeps a list of recent interactions and uses the last K interactions, preventing the buffer from getting too large. Like Conversation Buffer Memory, you can also use this memory type in a chain for chat-like interactions. 3. Conversation Entity Memory This memory type remembers facts about specific entities in a conversation and extracts information using an LLM. 4. Conversation Knowledge Graph Memory This memory type uses a knowledge graph to recreate memory. You can extract current entities and knowledge triplets from messages. You can also use this memory type in a chain for conversation-based knowledge retrieval. 5. Conversation Summary Memory This memory type creates a summary of the conversation over time, useful for condensing information from longer conversations. 6. Conversation Summary Buffer Memory This memory type combines the conversation summary and buffer, maintaining a balance between recent interactions and a summary. It uses token length to determine when to flush interactions. You can use these memory types to enhance your interactions with AI models in Langchain.\n",
      "Each memory type serves a specific purpose and can be selected based on your requirements. 7. Conversation Token Buffer Memory ConversationTokenBufferMemory is another memory type that keeps a buffer of recent interactions in memory. Unlike the previous memory types that focus on the number of interactions, this one uses token length to determine when to flush interactions. Using memory with LLM: In this example, the memory is set to limit interactions based on token length rather than the number of interactions. You can also get the history as a list of messages when using this memory type. Using in a chain: You can use ConversationTokenBufferMemory in a chain to enhance interactions with the AI model. In this example, ConversationTokenBufferMemory is used in a ConversationChain to manage the conversation and limit interactions based on token length. 8. VectorStoreRetrieverMemory VectorStoreRetrieverMemory stores memories in a vector store and queries the top-K most \"salient\" documents every time it is called. This memory type doesn't explicitly track the order of interactions but uses vector retrieval to fetch relevant memories. In this example, VectorStoreRetrieverMemory is used to store and retrieve relevant information from a conversation based on vector retrieval. You can also use VectorStoreRetrieverMemory in a chain for conversation-based knowledge retrieval, as shown in the previous examples.\n",
      "These different memory types in Langchain provide various ways to manage and retrieve information from conversations, enhancing the capabilities of AI models in understanding and responding to user queries and context. Each memory type can be selected based on the specific requirements of your application. Now we'll learn how to use memory with an LLMChain. Memory in an LLMChain allows the model to remember previous interactions and context to provide more coherent and context-aware responses. To set up memory in an LLMChain, you need to create a memory class, such as ConversationBufferMemory. Here's how you can set it up: In this example, the ConversationBufferMemory is used to store the conversation history. The memory_key parameter specifies the key used to store the conversation history. If you are using a chat model instead of a completion-style model, you can structure your prompts differently to better utilize the memory. Here's an example of how to set up a chat model-based LLMChain with memory: In this example, the ChatPromptTemplate is used to structure the prompt, and the ConversationBufferMemory is used to store and retrieve the conversation history. This approach is particularly useful for chat-style conversations where context and history play a crucial role. Memory can also be added to a chain with multiple inputs, such as a question/answering chain.\n",
      "Here's an example of how to set up memory in a question/answering chain: In this example, a question is answered using a document split into smaller chunks. The ConversationBufferMemory is used to store and retrieve the conversation history, allowing the model to provide context-aware answers. Adding memory to an agent allows it to remember and use previous interactions to answer questions and provide context-aware responses. Here's how you can set up memory in an agent: In this example, memory is added to an agent, allowing it to remember the previous conversation history and provide context-aware answers. This enables the agent to answer follow-up questions accurately based on the information stored in memory. LangChain Expression Language In the world of natural language processing and machine learning, composing complex chains of operations can be a daunting task. Fortunately, LangChain Expression Language (LCEL) comes to the rescue, providing a declarative and efficient way to build and deploy sophisticated language processing pipelines. LCEL is designed to simplify the process of composing chains, making it possible to go from prototyping to production with ease. In this blog, we'll explore what LCEL is and why you might want to use it, along with practical code examples to illustrate its capabilities. LCEL, or LangChain Expression Language, is a powerful tool for composing language processing chains.\n",
      "It was purpose-built to support the transition from prototyping to production seamlessly, without requiring extensive code changes. Whether you're building a simple \"prompt + LLM\" chain or a complex pipeline with hundreds of steps, LCEL has you covered. Here are some reasons to use LCEL in your language processing projects: Fast Token Streaming: LCEL delivers tokens from a Language Model to an output parser in real-time, improving responsiveness and efficiency. Versatile APIs: LCEL supports both synchronous and asynchronous APIs for prototyping and production use, handling multiple requests efficiently. Automatic Parallelization: LCEL optimizes parallel execution when possible, reducing latency in both sync and async interfaces. Reliable Configurations: Configure retries and fallbacks for enhanced chain reliability at scale, with streaming support in development. Stream Intermediate Results: Access intermediate results during processing for user updates or debugging purposes. Schema Generation: LCEL generates Pydantic and JSONSchema schemas for input and output validation. Comprehensive Tracing: LangSmith automatically traces all steps in complex chains for observability and debugging. Easy Deployment: Deploy LCEL-created chains effortlessly using LangServe. Now, let's dive into practical code examples that demonstrate the power of LCEL. We'll explore common tasks and scenarios where LCEL shines.\n",
      "Prompt + LLM The most fundamental composition involves combining a prompt and a language model to create a chain that takes user input, adds it to a prompt, passes it to a model, and returns the raw model output. Here's an example: In this example, the chain generates a joke about bears. You can attach stop sequences to your chain to control how it processes text. For example: This configuration stops text generation when a newline character is encountered. LCEL supports attaching function call information to your chain. Here's an example: This example attaches function call information to generate a joke. Prompt + LLM + OutputParser You can add an output parser to transform the raw model output into a more workable format. Here's how you can do it: The output is now in a string format, which is more convenient for downstream tasks. When specifying a function to return, you can parse it directly using LCEL. For example: This example parses the output of the \"joke\" function directly. These are just a few examples of how LCEL simplifies complex language processing tasks. Whether you're building chatbots, generating content, or performing complex text transformations, LCEL can streamline your workflow and make your code more maintainable. RAG (Retrieval-augmented Generation) LCEL can be used to create retrieval-augmented generation chains, which combine retrieval and language generation steps.\n",
      "Here's an example: In this example, the chain retrieves relevant information from the context and generates a response to the question. Conversational Retrieval Chain You can easily add conversation history to your chains. Here's an example of a conversational retrieval chain: In this example, the chain handles a follow-up question within a conversational context. With Memory and Returning Source Documents LCEL also supports memory and returning source documents. Here's how you can use memory in a chain: In this example, memory is used to store and retrieve conversation history and source documents. Multiple Chains You can string together multiple chains using Runnables. Here's an example: In this example, two chains are combined to generate information about a city and its country in a specified language. Branching and Merging LCEL allows you to split and merge chains using RunnableMaps. Here's an example of branching and merging: In this example, a branching and merging chain is used to generate an argument and evaluate its pros and cons before generating a final response. Writing Python Code with LCEL One of the powerful applications of LangChain Expression Language (LCEL) is writing Python code to solve user problems. Below is an example of how to use LCEL to write Python code: In this example, a user provides input, and LCEL generates Python code to solve the problem.\n",
      "The code is then executed using a Python REPL, and the resulting Python code is returned in Markdown format. Please note that using a Python REPL can execute arbitrary code, so use it with caution. Adding Memory to a Chain Memory is essential in many conversational AI applications. Here's how to add memory to an arbitrary chain: In this example, memory is used to store and retrieve conversation history, allowing the chatbot to maintain context and respond appropriately. Using External Tools with Runnables LCEL allows you to seamlessly integrate external tools with Runnables. Here's an example using the DuckDuckGo Search tool: In this example, LCEL integrates the DuckDuckGo Search tool into the chain, allowing it to generate a search query from user input and retrieve search results. LCEL's flexibility makes it easy to incorporate various external tools and services into your language processing pipelines, enhancing their capabilities and functionality. Adding Moderation to an LLM Application To ensure that your LLM application adheres to content policies and includes moderation safeguards, you can integrate moderation checks into your chain. Here's how to add moderation using LangChain: In this example, the OpenAIModerationChain is used to add moderation to the response generated by the LLM. The moderation chain checks the response for content that violates OpenAI's content policy. If any violations are found, it will flag the response accordingly.\n",
      "Routing by Semantic Similarity LCEL allows you to implement custom routing logic based on the semantic similarity of user input. Here's an example of how to dynamically determine the chain logic based on user input: In this example, the prompt_router function calculates the cosine similarity between user input and predefined prompt templates for physics and math questions. Based on the similarity score, the chain dynamically selects the most relevant prompt template, ensuring that the chatbot responds appropriately to the user's question. Using Agents and Runnables LangChain allows you to create agents by combining Runnables, prompts, models, and tools. Here's an example of building an agent and using it: In this example, an agent is created by combining a model, tools, a prompt, and a custom logic for intermediate steps and tool conversion. The agent is then executed, providing a response to the user's query. Querying a SQL Database You can use LangChain to query a SQL database and generate SQL queries based on user questions. Here's an example: In this example, LangChain is used to generate SQL queries based on user questions and retrieve responses from a SQL database. The prompts and responses are formatted to provide natural language interactions with the database. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams.\n",
      "Get Started Deploy with LangServe LangServe helps developers deploy LangChain runnables and chains as a REST API. This library is integrated with FastAPI and uses pydantic for data validation. Additionally, it provides a client that can be used to call into runnables deployed on a server, and a JavaScript client is available in LangChainJS. Features Input and Output schemas are automatically inferred from your LangChain object and enforced on every API call, with rich error messages. An API docs page with JSONSchema and Swagger is available. Efficient /invoke, /batch, and /stream endpoints with support for many concurrent requests on a single server. /stream_log endpoint for streaming all (or some) intermediate steps from your chain/agent. Playground page at /playground with streaming output and intermediate steps. Built-in (optional) tracing to LangSmith; just add your API key (see Instructions). All built with battle-tested open-source Python libraries like FastAPI, Pydantic, uvloop, and asyncio. Limitations Client callbacks are not yet supported for events that originate on the server. OpenAPI docs will not be generated when using Pydantic V2. FastAPI does not support mixing pydantic v1 and v2 namespaces. See the section below for more details. Use the LangChain CLI to bootstrap a LangServe project quickly. To use the langchain CLI, make sure that you have a recent version of langchain-cli installed. You can install it with pip install -U langchain-cli.\n",
      "Get your LangServe instance started quickly with LangChain Templates. For more examples, see the templates index or the examples directory. Here's a server that deploys an OpenAI chat model, an Anthropic chat model, and a chain that uses the Anthropic model to tell a joke about a topic. Once you've deployed the server above, you can view the generated OpenAPI docs using: Make sure to add the /docs suffix. In TypeScript (requires LangChain.js version 0.0.166 or later): Python using requests: You can also use curl: The following code: adds of these endpoints to the server: POST /my_runnable/invoke - invoke the runnable on a single input POST /my_runnable/batch - invoke the runnable on a batch of inputs POST /my_runnable/stream - invoke on a single input and stream the output POST /my_runnable/stream_log - invoke on a single input and stream the output, including output of intermediate steps as it's generated GET /my_runnable/input_schema - json schema for input to the runnable GET /my_runnable/output_schema - json schema for output of the runnable GET /my_runnable/config_schema - json schema for config of the runnable You can find a playground page for your runnable at /my_runnable/playground. This exposes a simple UI to configure and invoke your runnable with streaming output and intermediate steps. For both client and server: or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code.\n",
      "If you need to add authentication to your server, please reference FastAPI's security documentation and middleware documentation. You can deploy to GCP Cloud Run using the following command: LangServe provides support for Pydantic 2 with some limitations. OpenAPI docs will not be generated for invoke/batch/stream/stream_log when using Pydantic V2. Fast API does not support mixing pydantic v1 and v2 namespaces. LangChain uses the v1 namespace in Pydantic v2. Please read the following guidelines to ensure compatibility with LangChain. Except for these limitations, we expect the API endpoints, the playground, and any other features to work as expected. LLM applications often deal with files. There are different architectures that can be made to implement file processing; at a high level: The file may be uploaded to the server via a dedicated endpoint and processed using a separate endpoint. The file may be uploaded by either value (bytes of file) or reference (e.g., s3 url to file content). The processing endpoint may be blocking or non-blocking. If significant processing is required, the processing may be offloaded to a dedicated process pool. You should determine what is the appropriate architecture for your application. Currently, to upload files by value to a runnable, use base64 encoding for the file (multipart/form-data is not supported yet). Here's an example that shows how to use base64 encoding to send a file to a remote runnable.\n",
      "Remember, you can always upload files by reference (e.g., s3 url) or upload them as multipart/form-data to a dedicated endpoint. Input and Output types are defined on all runnables. You can access them via the input_schema and output_schema properties. LangServe uses these types for validation and documentation. If you want to override the default inferred types, you can use the with_types method. Here's a toy example to illustrate the idea: Inherit from CustomUserType if you want the data to deserialize into a pydantic model rather than the equivalent dict representation. At the moment, this type only works server-side and is used to specify desired decoding behavior. If inheriting from this type, the server will keep the decoded type as a pydantic model instead of converting it into a dict. The playground allows you to define custom widgets for your runnable from the backend. A widget is specified at the field level and shipped as part of the JSON schema of the input type. A widget must contain a key called type with the value being one of a well-known list of widgets. Other widget keys will be associated with values that describe paths in a JSON object. General schema: Allows the creation of a file upload input in the UI playground for files that are uploaded as base64 encoded strings. Here's the full example. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams.\n",
      "Get Started Introduction to LangSmith LangChain makes it easy to prototype LLM applications and Agents. However, delivering LLM applications to production can be deceptively difficult. You will likely have to heavily customize and iterate on your prompts, chains, and other components to create a high-quality product. To aid in this process, LangSmith was introduced, a unified platform for debugging, testing, and monitoring your LLM applications. When might this come in handy? You may find it useful when you want to quickly debug a new chain, agent, or set of tools, visualize how components (chains, llms, retrievers, etc.) relate and are used, evaluate different prompts and LLMs for a single component, run a given chain several times over a dataset to ensure it consistently meets a quality bar, or capture usage traces and use LLMs or analytics pipelines to generate insights. Prerequisites: Create a LangSmith account and create an API key (see bottom left corner). Familiarize yourself with the platform by looking through the docs. Now, let's get started! First, configure your environment variables to tell LangChain to log traces. This is done by setting the LANGCHAIN_TRACING_V2 environment variable to true. You can tell LangChain which project to log to by setting the LANGCHAIN_PROJECT environment variable (if this isn't set, runs will be logged to the default project). This will automatically create the project for you if it doesn't exist.\n",
      "You must also set the LANGCHAIN_ENDPOINT and LANGCHAIN_API_KEY environment variables. NOTE: You can also use a context manager in python to log traces using: However, in this example, we will use environment variables. Create the LangSmith client to interact with the API: Create a LangChain component and log runs to the platform. In this example, we will create a ReAct-style agent with access to a general search tool (DuckDuckGo). The agent's prompt can be viewed in the Hub here: We are running the agent concurrently on multiple inputs to reduce latency. Runs get logged to LangSmith in the background, so execution latency is unaffected: Assuming you've successfully set up your environment, your agent traces should show up in the Projects section in the app. Congrats! It looks like the agent isn't effectively using the tools though. Let's evaluate this so we have a baseline. In addition to logging runs, LangSmith also allows you to test and evaluate your LLM applications. In this section, you will leverage LangSmith to create a benchmark dataset and run AI-assisted evaluators on an agent. You will do so in a few steps: Create a LangSmith dataset: Below, we use the LangSmith client to create a dataset from the input questions from above and a list labels. You will use these later to measure performance for a new agent.\n",
      "A dataset is a collection of examples, which are nothing more than input-output pairs you can use as test cases to your application: Initialize a new agent to benchmark: LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn't shared between dataset runs, we will pass in a chain_factory ( aka a constructor) function to initialize for each call: Configure evaluation: Manually comparing the results of chains in the UI is effective, but it can be time-consuming. It can be helpful to use automated metrics and AI-assisted feedback to evaluate your component's performance: Run the agent and evaluators: Use the run_on_dataset (or asynchronous arun_on_dataset) function to evaluate your model. This will: Fetch example rows from the specified dataset. Run your agent (or any custom function) on each example. Apply evaluators to the resulting run traces and corresponding reference examples to generate automated feedback. The results will be visible in the LangSmith app: Now that we have our test run results, we can make changes to our agent and benchmark them. Let's try this again with a different prompt and see the results: LangSmith lets you export data to common formats such as CSV or JSONL directly in the web app. You can also use the client to fetch runs for further analysis, to store in your own database, or to share with others.\n",
      "Let's fetch the run traces from the evaluation run: This was a quick guide to get started, but there are many more ways to use LangSmith to speed up your developer flow and produce better results. For more information on how you can get the most out of LangSmith, check out LangSmith documentation. Level up with Nanonets While LangChain is a valuable tool for integrating language models (LLMs) with your applications and workflows, it may face limitations when it comes to enterprise use cases. Data Connectivity: Limited support for various business applications and data formats. Task Automation: Challenges in automating tasks across different applications. Data Synchronization: Inadequate real-time data update capabilities. Complex Configuration: Difficult and time-consuming setup processes. Format Adherence: No assurance that the model will follow specified formats accurately. Invalid Outputs: Risks of generating incorrect tool names or inputs. Non-Termination Issues: Problems with processes not ending appropriately. Customization Difficulty: Challenges in modifying or creating custom agents. Performance Issues: Slowness, especially when reprocessing prompts. Security Vulnerabilities: Risks of data loss or sensitive information exposure due to prompt injection attacks. Enter Nanonets Workflows!\n",
      "Harnessing the Power of Workflow Automation: A Game-Changer for Modern Businesses In today's fast-paced business environment, workflow automation stands out as a crucial innovation, offering a competitive edge to companies of all sizes. The integration of automated workflows into daily business operations is not just a trend; it's a strategic necessity. In addition to this, the advent of LLMs has opened even more opportunities for automation of manual tasks and processes. Welcome to Nanonets Workflow Automation, where AI-driven technology empowers you and your team to automate manual tasks and construct efficient workflows in minutes. Utilize natural language to effortlessly create and manage workflows that seamlessly integrate with all your documents, apps, and databases. Our platform offers not only seamless app integrations for unified workflows but also the ability to build and utilize custom Large Language Models Apps for sophisticated text writing and response posting within your apps. All the while ensuring data security remains our top priority, with strict adherence to GDPR, SOC 2, and HIPAA compliance standards​. To better understand the practical applications of Nanonets workflow automation, let's delve into some real-world examples. Automated Customer Support and Engagement Process Ticket Creation – Zendesk: The workflow is triggered when a customer submits a new support ticket in Zendesk, indicating they need assistance with a product or service.\n",
      "Ticket Update – Zendesk: After the ticket is created, an automated update is immediately logged in Zendesk to indicate that the ticket has been received and is being processed, providing the customer with a ticket number for reference. Information Retrieval – Nanonets Browsing: Concurrently, the Nanonets Browsing feature searches through all the knowledge base pages to find relevant information and possible solutions related to the customer's issue. Customer History Access – HubSpot: Simultaneously, HubSpot is queried to retrieve the customer's previous interaction records, purchase history, and any past tickets to provide context to the support team. Ticket Processing – Nanonets AI: With the relevant information and customer history at hand, Nanonets AI processes the ticket, categorizing the issue and suggesting potential solutions based on similar past cases. Notification – Slack: Finally, the responsible support team or individual is notified through Slack with a message containing the ticket details, customer history, and suggested solutions, prompting a swift and informed response. Automated Issue Resolution Process Initial Trigger – Slack Message: The workflow begins when a customer service representative receives a new message in a dedicated channel on Slack, signaling a customer issue that needs to be addressed.\n",
      "Classification – Nanonets AI: Once the message is detected, Nanonets AI steps in to classify the message based on its content and past classification data (from Airtable records). Using LLMs, it classifies it as a bug along with determining urgency. Record Creation – Airtable: After classification, the workflow automatically creates a new record in Airtable, a cloud collaboration service. This record includes all relevant details from the customer's message, such as customer ID, issue category, and urgency level. Team Assignment – Airtable: With the record created, the Airtable system then assigns a team to handle the issue. Based on the classification done by Nanonets AI, the system selects the most appropriate team – tech support, billing, customer success, etc. – to take over the issue. Notification – Slack: Finally, the assigned team is notified through Slack. An automated message is sent to the team's channel, alerting them of the new issue, providing a direct link to the Airtable record, and prompting a timely response. Automated Meeting Scheduling Process Initial Contact – LinkedIn: The workflow is initiated when a professional connection sends a new message on LinkedIn expressing interest in scheduling a meeting. An LLM parses incoming messages and triggers the workflow if it deems the message as a request for a meeting from a potential job candidate.\n",
      "Document Retrieval – Google Drive: Following the initial contact, the workflow automation system retrieves a pre-prepared document from Google Drive that contains information about the meeting agenda, company overview, or any relevant briefing materials. Scheduling – Google Calendar: Next, the system interacts with Google Calendar to get available times for the meeting. It checks the calendar for open slots that align with business hours (based on the location parsed from LinkedIn profile) and previously set preferences for meetings. Confirmation Message as Reply – LinkedIn: Once a suitable time slot is found, the workflow automation system sends a message back through LinkedIn. This message includes the proposed time for the meeting, access to the document retrieved from Google Drive, and a request for confirmation or alternative suggestions. Invoice Processing in Accounts Payable Receipt of Invoice - Gmail: An invoice is received via email or uploaded to the system. Data Extraction - Nanonets OCR: The system automatically extracts relevant data (like vendor details, amounts, due dates). Data Verification - Quickbooks: The Nanonets workflow verifies the extracted data against purchase orders and receipts. Approval Routing - Slack: The invoice is routed to the appropriate manager for approval based on predefined thresholds and rules. Payment Processing - Brex: Once approved, the system schedules the payment according to the vendor's terms and updates the finance records.\n",
      "Archiving - Quickbooks: The completed transaction is archived for future reference and audit trails. Internal Knowledge Base Assistance Initial Inquiry – Slack: A team member, Smith, inquires in the #chat-with-data Slack channel about customers experiencing issues with QuickBooks integration. Automated Data Aggregation - Nanonets Knowledge Base:Ticket Lookup - Zendesk: The Zendesk app in Slack automatically provides a summary of today's tickets, indicating that there are issues with exporting invoice data to QuickBooks for some customers.Slack Search - Slack: Simultaneously, the Slack app notifies the channel that team members Patrick and Rachel are actively discussing the resolution of the QuickBooks export bug in another channel, with a fix scheduled to go live at 4 PM.Ticket Tracking – JIRA: The JIRA app updates the channel about a ticket created by Emily titled \"QuickBooks export failing for QB Desktop integrations,\" which helps track the status and resolution progress of the issue.Reference Documentation – Google Drive: The Drive app mentions the existence of a runbook for fixing bugs related to QuickBooks integrations, which can be referenced to understand the steps for troubleshooting and resolution.Ongoing Communication and Resolution Confirmation – Slack: As the conversation progresses, the Slack channel serves as a real-time forum for discussing updates, sharing findings from the runbook, and confirming the deployment of the bug fix.\n",
      "Team members use the channel to collaborate, share insights, and ask follow-up questions to ensure a comprehensive understanding of the issue and its resolution.Resolution Documentation and Knowledge Sharing: After the fix is implemented, team members update the internal documentation in Google Drive with new findings and any additional steps taken to resolve the issue. A summary of the incident, resolution, and any lessons learned are already shared in the Slack channel. Thus, the team’s internal knowledge base is automatically enhanced for future use. The Future of Business Efficiency Nanonets Workflows is a secure, multi-purpose workflow automation platform that automates your manual tasks and workflows. It offers an easy-to-use user interface, making it accessible for both individuals and organizations. To get started, you can schedule a call with one of our AI experts, who can provide a personalized demo and trial of Nanonets Workflows tailored to your specific use case. Once set up, you can use natural language to design and execute complex applications and workflows powered by LLMs, integrating seamlessly with your apps and data. Supercharge your teams with Nanonets Workflows allowing them to focus on what truly matters. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams. Get Started\n",
      "\n",
      "Source: python.langchain.com_docs_expression_language_interface.txt\n",
      "Content: LangChain Expression Language Interface Interface To make it as easy as possible to create custom chains, we’ve implemented a “Runnable” protocol. The Runnable protocol is implemented for most components. This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way.\n",
      "The standard interface includes: stream: stream back chunks of the response invoke: call the chain on an input batch: call the chain on a list of inputs These also have corresponding async methods: astream: stream back chunks of the response async ainvoke: call the chain on an input async abatch: call the chain on a list of inputs async astream_log: stream back intermediate steps as they happen, in addition to the final response astream_events: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14) The input type and output type varies by component: Component Input Type Output Type Prompt Dictionary PromptValue ChatModel Single string, list of chat messages or a PromptValue ChatMessage LLM Single string, list of chat messages or a PromptValue String OutputParser The output of an LLM or ChatModel Depends on the parser Retriever Single string List of Documents Tool Single string or dictionary, depending on the tool Depends on the tool All runnables expose input and output schemas to inspect the inputs and outputs: - input_schema: an input Pydantic model auto-generated from the structure of the Runnable - output_schema: an output Pydantic model auto-generated from the structure of the Runnable Let’s take a look at these methods. To do so, we’ll create a super simple PromptTemplate + ChatModel chain.\n",
      "%pip install –upgrade –quiet langchain-core langchain-community langchain-openai from langchain_core prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI model ChatOpenAI prompt ChatPromptTemplate from_template \"tell me a joke about {topic}\" chain prompt model Input Schema​ A description of the inputs accepted by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation. # The input schema of the chain is the input schema of its first part, the prompt. chain input_schema schema {'title': 'PromptInput', 'type': 'object', 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}} prompt input_schema schema {'title': 'PromptInput', 'type': 'object', 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}} model input_schema schema {'title': 'ChatOpenAIInput', 'anyOf': [{'type': 'string'}, {'$ref': '#/definitions/StringPromptValue'}, {'$ref': '#/definitions/ChatPromptValueConcrete'}, {'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}, {'$ref': '#/definitions/ToolMessage'}]}}], 'definitions': {'StringPromptValue': {'title': 'StringPromptValue', 'description': 'String prompt value.\n",
      "', 'type': 'object', 'properties': {'text': {'title': 'Text', 'type': 'string'}, 'type': {'title': 'Type', 'default': 'StringPromptValue', 'enum': ['StringPromptValue'], 'type': 'string'}}, 'required': ['text']}, 'AIMessage': {'title': 'AIMessage', 'description': 'A Message from an AI. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'HumanMessage': {'title': 'HumanMessage', 'description': 'A Message from a human. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'human', 'enum': ['human'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'ChatMessage': {'title': 'ChatMessage', 'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).\n",
      "', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'chat', 'enum': ['chat'], 'type': 'string'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role']}, 'SystemMessage': {'title': 'SystemMessage', 'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'system', 'enum': ['system'], 'type': 'string'}}, 'required': ['content']}, 'FunctionMessage': {'title': 'FunctionMessage', 'description': 'A Message for passing the result of executing a function back to a model.\n",
      "', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'function', 'enum': ['function'], 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['content', 'name']}, 'ToolMessage': {'title': 'ToolMessage', 'description': 'A Message for passing the result of executing a tool back to a model. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'tool', 'enum': ['tool'], 'type': 'string'}, 'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}}, 'required': ['content', 'tool_call_id']}, 'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete', 'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.\n",
      "', 'type': 'object', 'properties': {'messages': {'title': 'Messages', 'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}, {'$ref': '#/definitions/ToolMessage'}]}}, 'type': {'title': 'Type', 'default': 'ChatPromptValueConcrete', 'enum': ['ChatPromptValueConcrete'], 'type': 'string'}}, 'required': ['messages']}}} Output Schema​ A description of the outputs produced by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation. # The output schema of the chain is the output schema of its last part, in this case a ChatModel, which outputs a ChatMessage chain output_schema schema {'title': 'ChatOpenAIOutput', 'anyOf': [{'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}, {'$ref': '#/definitions/ToolMessage'}], 'definitions': {'AIMessage': {'title': 'AIMessage', 'description': 'A Message from an AI.\n",
      "', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'HumanMessage': {'title': 'HumanMessage', 'description': 'A Message from a human. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'human', 'enum': ['human'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'ChatMessage': {'title': 'ChatMessage', 'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).\n",
      "', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'chat', 'enum': ['chat'], 'type': 'string'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role']}, 'SystemMessage': {'title': 'SystemMessage', 'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'system', 'enum': ['system'], 'type': 'string'}}, 'required': ['content']}, 'FunctionMessage': {'title': 'FunctionMessage', 'description': 'A Message for passing the result of executing a function back to a model.\n",
      "', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'function', 'enum': ['function'], 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['content', 'name']}, 'ToolMessage': {'title': 'ToolMessage', 'description': 'A Message for passing the result of executing a tool back to a model. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'tool', 'enum': ['tool'], 'type': 'string'}, 'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}}, 'required': ['content', 'tool_call_id']}}} Stream​ for in chain stream \"topic\" \"bears\" print content end \"\" flush True Sure, here's a bear-themed joke for you: Why don't bears wear shoes? Because they already have bear feet! Invoke​ chain invoke \"topic\" \"bears\" AIMessage(content=\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\") Batch​ chain batch \"topic\" \"bears\" \"topic\" \"cats\" [AIMessage(content=\"Sure, here's a bear joke for you:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they already have bear feet!\n",
      "\"), AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\")] You can set the number of concurrent requests by using the max_concurrency parameter chain batch \"topic\" \"bears\" \"topic\" \"cats\" config \"max_concurrency\" [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet! \"), AIMessage(content=\"Why don't cats play poker in the wild? Too many cheetahs!\")] Async Stream​ async for in chain astream \"topic\" \"bears\" print content end \"\" flush True Why don't bears wear shoes? Because they have bear feet! Async Invoke​ await chain ainvoke \"topic\" \"bears\" AIMessage(content=\"Why don't bears ever wear shoes?\\n\\nBecause they already have bear feet!\") Async Batch​ await chain abatch \"topic\" \"bears\" [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\")] Async Stream Events (beta)​ Event Streaming is a beta API, and may change a bit based on feedback. Note: Introduced in langchain-core 0.2.0 For now, when using the astream_events API, for everything to work properly please: Use async throughout the code (including async tools etc) Propagate callbacks if defining custom functions / runnables. Whenever using runnables without LCEL, make sure to call .astream() on LLMs rather than .ainvoke to force the LLM to stream tokens. Event Reference​ Here is a reference table that shows some events that might be emitted by the various Runnable objects. Definitions for some of the Runnable are included after the table.\n",
      "⚠️ When streaming the inputs for the runnable will not be available until the input stream has been entirely consumed This means that the inputs will be available at for the corresponding end hook rather than start event. event name chunk input output on_chat_model_start [model name] {“messages”: [[SystemMessage, HumanMessage]]} on_chat_model_stream [model name] AIMessageChunk(content=“hello”) on_chat_model_end [model name] {“messages”: [[SystemMessage, HumanMessage]]} {“generations”: […], “llm_output”: None, …} on_llm_start [model name] {‘input’: ‘hello’} on_llm_stream [model name] ‘Hello’ on_llm_end [model name] ‘Hello human!’ on_chain_start format_docs on_chain_stream format_docs “hello world!, goodbye world!” on_chain_end format_docs [Document(…)] “hello world!, goodbye world!” on_tool_start some_tool {“x”: 1, “y”: “2”} on_tool_stream some_tool {“x”: 1, “y”: “2”} on_tool_end some_tool {“x”: 1, “y”: “2”} on_retriever_start [retriever name] {“query”: “hello”} on_retriever_chunk [retriever name] {documents: […]} on_retriever_end [retriever name] {“query”: “hello”} {documents: […]} on_prompt_start [template_name] {“question”: “hello”} on_prompt_end [template_name] {“question”: “hello”} ChatPromptValue(messages: [SystemMessage, …]) Here are declarations associated with the events shown above: format_docs: def format_docs docs List Document str '''Format the docs.'''\n",
      "return \", \" join doc page_content for doc in docs format_docs RunnableLambda format_docs some_tool: @tool def some_tool int str dict '''Some_tool.''' return \"x\" \"y\" prompt: template ChatPromptTemplate from_messages \"system\" \"You are Cat Agent 007\" \"human\" \"{question}\" with_config \"run_name\" \"my_template\" \"tags\" \"my_template\" Let’s define a new chain to make it more interesting to show off the astream_events interface (and later the astream_log interface). from langchain_community vectorstores import FAISS from langchain_core output_parsers import StrOutputParser from langchain_core runnables import RunnablePassthrough from langchain_openai import OpenAIEmbeddings template \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\" prompt ChatPromptTemplate from_template template vectorstore FAISS from_texts \"harrison worked at kensho\" embedding OpenAIEmbeddings retriever vectorstore as_retriever retrieval_chain \"context\" retriever with_config run_name \"Docs\" \"question\" RunnablePassthrough prompt model with_config run_name \"my_llm\" StrOutputParser Now let’s use astream_events to get events from the retriever and the LLM. async for event in retrieval_chain astream_events \"where did harrison work?\"\n",
      "version \"v1\" include_names \"Docs\" \"my_llm\" kind event \"event\" if kind == \"on_chat_model_stream\" print event \"data\" \"chunk\" content end \"|\" elif kind in \"on_chat_model_start\" print print \"Streaming LLM:\" elif kind in \"on_chat_model_end\" print print \"Done streaming LLM.\" elif kind == \"on_retriever_end\" print \"--\" print \"Retrieved the following documents:\" print event \"data\" \"output\" \"documents\" elif kind == \"on_tool_end\" print f\"Ended tool: event 'name' else pass /home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future. warn_beta( -- Retrieved the following documents: [Document(page_content='harrison worked at kensho')] Streaming LLM: |H|arrison| worked| at| Kens|ho|.|| Done streaming LLM. Async Stream Intermediate Steps​ All runnables also have a method .astream_log() which is used to stream (as they happen) all or part of the intermediate steps of your chain/sequence. This is useful to show progress to the user, to use intermediate results, or to debug your chain. You can stream all steps (default) or include/exclude steps by name, tags or metadata. This method yields JSONPatch ops that when applied in the same order as received build up the RunState. class LogEntry TypedDict id str \"\"\"ID of the sub-run.\"\"\" name str \"\"\"Name of the object being run.\"\"\" type str \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\" tags List str \"\"\"List of tags for the run.\"\"\"\n",
      "metadata Dict str Any \"\"\"Key-value pairs of metadata for the run.\"\"\" start_time str \"\"\"ISO-8601 timestamp of when the run started.\"\"\" streamed_output_str List str \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\" final_output Optional Any \"\"\"Final output of this run. Only available after the run has finished successfully.\"\"\" end_time Optional str \"\"\"ISO-8601 timestamp of when the run ended. Only available after the run has finished.\"\"\" class RunState TypedDict id str \"\"\"ID of the run.\"\"\" streamed_output List Any \"\"\"List of output chunks streamed by Runnable.stream()\"\"\" final_output Optional Any \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output. Only available after the run has finished successfully.\"\"\" logs Dict str LogEntry \"\"\"Map of run names to sub-runs. If filters were supplied, this list will contain only the runs that matched the filters.\"\"\" Streaming JSONPatch chunks​ This is useful eg. to stream the JSONPatch in an HTTP server, and then apply the ops on the client to rebuild the run state there. See LangServe for tooling to make it easier to build a webserver from any Runnable. async for chunk in retrieval_chain astream_log \"where did harrison work?\"\n",
      "include_names \"Docs\" print \"-\" 40 print chunk ---------------------------------------- RunLogPatch({'op': 'replace', 'path': '', 'value': {'final_output': None, 'id': '82e9b4b1-3dd6-4732-8db9-90e79c4da48c', 'logs': {}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': 'chain'}}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/logs/Docs', 'value': {'end_time': None, 'final_output': None, 'id': '9206e94a-57bd-48ee-8c5e-fdd1c52a6da2', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:55.902+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/logs/Docs/final_output', 'value': {'documents': [Document(page_content='harrison worked at kensho')]}}, {'op': 'add', 'path': '/logs/Docs/end_time', 'value': '2024-01-19T22:33:56.064+00:00'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''}, {'op': 'replace', 'path': '/final_output', 'value': ''}) ---------------------------------------- RunLogPatch({'op':\n",
      "'add', 'path': '/streamed_output/-', 'value': 'H'}, {'op': 'replace', 'path': '/final_output', 'value': 'H'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'arrison'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' worked'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' at'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked at'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Kens'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked at Kens'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ho'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked at Kensho'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '. '}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked at Kensho.'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''}) Streaming the incremental RunState​ You can simply pass diff=False to get incremental values of RunState.\n",
      "You get more verbose output with more repetitive parts. async for chunk in retrieval_chain astream_log \"where did harrison work?\"\n",
      "include_names \"Docs\" diff False print \"-\" 70 print chunk ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': None, 'final_output': None, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': 'chain'})\n",
      "---------------------------------------------------------------------- RunLog({'final_output': '', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [''], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'H', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H'], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents':\n",
      "[Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison'], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison worked', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked'], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison worked at', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00',\n",
      "'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at'], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison worked at Kens', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens'], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison worked at Kensho', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}},\n",
      "'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho'], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison worked at Kensho. ', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho', '. '], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison worked at Kensho.\n",
      "', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho', '. ', ''], 'type': 'chain'}) Parallelism​ Let’s take a look at how LangChain Expression Language supports parallel requests. For example, when using a RunnableParallel (often written as a dictionary) it executes each element in parallel. from langchain_core runnables import RunnableParallel chain1 ChatPromptTemplate from_template \"tell me a joke about {topic}\" model chain2 ChatPromptTemplate from_template \"write a short (2 line) poem about {topic}\" model combined RunnableParallel joke chain1 poem chain2 time chain1 invoke \"topic\" \"bears\" CPU times: user 18 ms, sys: 1.27 ms, total: 19.3 ms Wall time: 692 ms AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\") time chain2 invoke \"topic\" \"bears\" CPU times: user 10.5 ms, sys: 166 µs, total: 10.7 ms Wall time: 579 ms AIMessage(content=\"In forest's embrace,\\nMajestic bears pace.\")\n",
      "time combined invoke \"topic\" \"bears\" CPU times: user 32 ms, sys: 2.59 ms, total: 34.6 ms Wall time: 816 ms {'joke': AIMessage(content=\"Sure, here's a bear-related joke for you:\\n\\nWhy did the bear bring a ladder to the bar?\\n\\nBecause he heard the drinks were on the house! \"), 'poem': AIMessage(content=\"In wilderness they roam,\\nMajestic strength, nature's throne.\")} Parallelism on batches​ Parallelism can be combined with other runnables. Let’s try to use parallelism with batches. time chain1 batch \"topic\" \"bears\" \"topic\" \"cats\" CPU times: user 17.3 ms, sys: 4.84 ms, total: 22.2 ms Wall time: 628 ms [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet! \"), AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\")] time chain2 batch \"topic\" \"bears\" \"topic\" \"cats\" CPU times: user 15.8 ms, sys: 3.83 ms, total: 19.7 ms Wall time: 718 ms [AIMessage(content='In the wild, bears roam,\\nMajestic guardians of ancient home. '), AIMessage(content='Whiskers grace, eyes gleam,\\nCats dance through the moonbeam.')] time combined batch \"topic\" \"bears\" \"topic\" \"cats\" CPU times: user 44.8 ms, sys: 3.17 ms, total: 48 ms Wall time: 721 ms [{'joke': AIMessage(content=\"Sure, here's a bear joke for you:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they have bear feet! \"), 'poem': AIMessage(content=\"Majestic bears roam,\\nNature's strength, beauty shown. \")}, {'joke': AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\n",
      "\"), 'poem': AIMessage(content=\"Whiskers dance, eyes aglow,\\nCats embrace the night's gentle flow.\")}] PreviousWhy use LCEL NextStreaming Input Schema Output Schema Stream Invoke Batch Async Stream Async Invoke Async Batch Async Stream Events (beta)Event Reference Async Stream Intermediate StepsStreaming JSONPatch chunksStreaming the incremental RunState ParallelismParallelism on batches\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query #1\n",
    "query_result = client.query.get(\n",
    "    \"Document\",\n",
    "    [\"source\", \"content\"]\n",
    ").do()\n",
    "\n",
    "for result in query_result['data']['Get']['Document']:\n",
    "    print(f\"Source: {result['source']}\")\n",
    "    print(f\"Content: {result['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb67d35",
   "metadata": {},
   "source": [
    "#### Query #2\n",
    "\n",
    "This approach is suitable for text fields in Weaviate and leverages its vector search capabilities to find documents that are semantically related to your query text, even if they don’t contain the exact words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f923106f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: blog.min.io_.txt\n",
      "Content: Topics All Architect's Guide Operator's Guide Best Practices AI/ML Modern Data Lakes Performance Kubernetes Integrations Benchmarks Security Multicloud Building Modern Data Architectures with Iceberg, Tabular and MinIO Brenna Buuck Brenna Buuck on Modern Data Lakes Explore modern data architecture with Iceberg, Tabular, and MinIO. Learn to seamlessly integrate structured and unstructured data, optimize AI/ML workloads, and build a high-performance, cloud-native data lake. Read more... Developing Langchain Agents with the MinIO SDK for LLM Tool-Use David Cannan David Cannan on AI/ML Explore Langchain’s LLM Tool-Use and leverage Langgraph for monitoring MinIO’s S3 Object Store. This guide walks you through developing custom conversational AI agents and creating powerful OpenAI LLM chains for efficient data management and enhanced application functionality. Read more... Prefix vs Folder AJ AJ on Object Storage How you ever wondered how object storage creates its folder structure mimicking a POSIX style hierarchy but something that is actually built for speed and efficiency? Today in this post you will find out what actually makes the internal structure you see visually in your MInIO buckets. Read more... Powering AI/ML workflows with GitOps Automation David Cannan David Cannan on AI/ML Explore the fusion of GitOps, MinIO, Weaviate, and Python in AI development for unparalleled automation and innovation.\n",
      "This combination offers a solid foundation for creating scalable, efficient, and automated AI solutions, propelling projects from concept to reality with ease. Read more... Automated Data Prep for ML with MinIO's SDK Brenna Buuck Brenna Buuck on AI/ML This tutorial guides you through constructing robust data pipelines on the edge, ensuring flexibility and scalability. Learn to create, populate, and transform datasets seamlessly while prioritizing data privacy. Master the art of automation with MinIO's Python SDK. Read more... Replication Strategies Deep Dive AJ AJ on DevOps With all these different types of replication types floating around one has to wonder which replication strategy to use where? Today we’ll demystify these different replication strategies to see which one should be used in which scenario. Read more... Backing Up Weaviate with MinIO S3 Buckets David Cannan David Cannan on AI/ML Explore integrating MinIO with Weaviate using Docker Compose for AI-enhanced data management. Learn to back up Weaviate to MinIO S3 buckets, ensuring data integrity and scalability with practical Docker and Python examples. Streamline your AI-driven search and analysis with this robust setup. Read more... SQL Server 2022 Machine Learning Services Unlock the Value of Your Data Matt Sarrel Matt Sarrel @msarrel on Integrations Learn how to run Python stored procedures on SQL Server 2022.\n",
      "Read more... MinIO and Apache Tika: A Pattern for Text Extraction Sidharth Rajaram Sidharth Rajaram @sidharrrrrth on AI/ML Tl;dr: In this post, we will use MinIO Bucket Notifications and Apache Tika, for document text extraction, which is at the heart of critical downstream tasks like Large Language Model (LLM) training and Retrieval Augmented Generation (RAG). The Premise Let’s say that I want to construct a dataset of text that I can then use to fine-tune an Read more... Hungry GPUs Need Fast Object Storage Keith Pijanowski Keith Pijanowski on AI/ML A chain is as strong as its weakest link - and your AI/ML infrastructure is only as fast as your slowest component. If you train machine learning models with GPUs, then your weak link may be your storage solution. The result is what I call the “Starving GPU Problem.” The Starving GPU problem occurs when your network or your Read more... Why Your Enterprise AI Strategy Is Likely to Fail in 2024: Model Down vs. Data Up Jonathan Symonds Jonathan Symonds on AI/ML I suspect some folks will accuse me of clickbait titling. Others will say, that’s not really a reach - most folks will fail in their initial AI attempts but it doesn’t matter and the learnings are worth it. On some level both are right - but I think WHY enterprises will fail is worth exploration and may allow Read more...\n",
      "Innovating S3 Bucket Retrieval: Langchain Community S3 Loaders with OpenAI API David Cannan David Cannan on AI/ML Explore the synergy of MinIO, Langchain, and OpenAI in enhancing data storage and processing. This article illustrates MinIO’s integration for efficient document summarization using Langchain and OpenAI’s GPT, revolutionizing AI and ML data handling. Read more... Supercharge TileDB Engine with MinIO AJ AJ on Vector Database MinIO makes a powerful primary TileDB backend because both are built for performance and scale. Read more... Data Before Models: The Unsung Heroes Who Unlock Real AI Results Brenna Buuck Brenna Buuck on AI/ML Explore the essential role of Data Engineers in unleashing the true power of AI! Data Engineers have a critical foundation in cleaning and structuring raw data for ML success. Learn why their expertise in data infrastructure, feature engineering, and pipeline optimization is indispensable. Read more... The Strengths, Weaknesses and Dangers of LLMs Sidharth Rajaram Sidharth Rajaram @sidharrrrrth Keith Pijanowski Keith Pijanowski on AI/ML Much has been said lately about the wonders of Large Language Models (LLMs). Most of these accolades are deserved. Ask ChatGPT to describe the General Theory of Relativity and you will get a very good (and accurate) answer.\n",
      "However, at the end of the day ChatGPT is still a computer program (as are all other LLMs) that is blindly executing Read more... We Read Google’s New Egress Policy So You Don’t Have To…It Is Surprising Matt Sarrel Matt Sarrel @msarrel on GCP Google recently announced that it would eliminate data egress fees for those leaving the platform. Given our position on the cloud operating model and the lifecycle of the cloud, this appeared to be a major announcement. It is not. You could understand our initial enthusiasm. Google stated that any \"customers who wish to stop using Google Cloud and migrate Read more... Event-Driven Architecture: MinIO Event Notification Webhooks using Flask David Cannan David Cannan on Events Explore deploying MinIO and Flask with Docker-compose for event-driven architecture. Master MinIO bucket events and Flask webhooks for efficient data workflows and robust applications. Dive into the synergy of cloud technologies. Read more... Locking down MinIO Operator Permissions AJ AJ on Kubernetes In this post, we’ll show you how to configure the MinIO Operator with the most restrictive namespace permissions – all the while being able to fully utilize the power and flexibility of the MinIO Operator for day-to-day operations. Read more... Everything You Need to Know to Repatriate from AWS S3 to MinIO Matt Sarrel Matt Sarrel @msarrel on Operator's Guide Step by step instructions to plan for a migrate data off AWS S3 and on MinIO on-premise.\n",
      "Read more... Debugging MinIO Installs AJ AJ on DevOps In this blog post, we’ll show you how to debug a MinIO install running in Kubernetes and also some of the common issues you might encounter when doing bare metal installation and how to rectify them. Read more... Older Posts\n",
      "\n",
      "Source: sanity.cdaprod.dev_.txt\n",
      "Content: Blog. Exploring the Digital Frontier: A Next.js and Sanity CMS. Tech Journey by David Cannan. Introduction to cda.data-lake and MinIO The cda.data-lake project embodies a transformative approach to managing and processing data at scale. At its core, it leverages the robust capabilities of MinIO, an object storage solution that excels in performance and scalability. This integration empowers the project to handle an expansive array of data types and operations, ranging from simple storage to complex analytical computations and machine learning tasks. The use of MinIO ensures that the cda.data-lake can operate within a secure and compliant framework, making it a reliable foundation for data-driven innovation. As the cda.data-lake project evolves, the MinIO event notification system plays a pivotal role by automating workflows in real-time, thereby optimizing data processing and reducing manual intervention. This not only increases efficiency but also enables the system to swiftly adapt to the increasing volume and complexity of data. With MinIO's flexible and resilient infrastructure, the cda.data-lake project is set to redefine the standards of data handling and accessibility for diverse applications. David Cannan More Stories My Gartner's Peer Insights Review of MinIO - A Game Changer in Object Storage My experience with MinIO has been nothing short of fantastic. It's a testament to what a well-thought-out platform, backed by a passionate team and community, can achieve.\n",
      "David Cannan Unlocking Coding Efficiency: My Deep Dive into Langchain's StructuredTool and Pydantic Schema Discover how Langchain's StructuredTool and Pydantic Schema are revolutionizing my approach to coding. If you value code quality, data integrity, and efficiency, you won't want to miss this! David Cannan Building Custom Query Engines: A Journey of Discovery and Innovation Building custom query engines for my unique data needs has been an exhilarating journey filled with challenges, learning, and growth. From connecting various data sources to crafting intelligent search capabilities, this project has been a testament to the power of innovation and determination. David Cannan Revolutionizing Video Management with Langchain: A Comprehensive Guide Unleashing Langchain – a new era in video management! 🚀 With the muscle of ResNet50, the intelligence of SerpAPITool, and the flair of StreamlitTool, Langchain is more than just software; it's a symphony of technology and innovation. From indexing videos to classifying frames and researching keywords, it's a one-stop solution that's poised to redefine how we interact with video content. Watch out, world; Langchain is here, and it's ready to dazzle! 🎥✨ David Cannan Social Engineering in the Modern Age: Unveiling Hidden Threats & Harnessing Human Behavior The potency of social engineering lies in its ability to exploit human reactions and biases.\n",
      "Recognizing and manipulating these targeted reactions is a testament to the prowess of a skilled social engineer. While the techniques may vary, the underpinning principle remains constant: understanding human psychology to predict, influence, and achieve a desired outcome. David Cannan Unleashing the Power of Ansible and OpenAI in Infrastructure Automation In the thrilling intersection of automation, artificial intelligence, and infrastructure management, my application leverages the power of Ansible and OpenAI to innovate and automate like never before. With Ansible's robust automation capabilities, the application creates tailored roles and relevant files based on input from a CSV file. But the magic doesn't stop there - it then utilizes OpenAI's cutting-edge AI models to generate high-quality content for these files. The amalgamation of these powerful tools opens up a world of possibilities, redefining the way we perceive infrastructure automation. David Cannan Machine Learning Odyssey: Charting Unexplored Terrains in Bug Bounty Systems Embarking on a thrilling journey of machine learning integration with AWS SageMaker, this blog post explores the transformative potential of SageMaker for my bug bounty system project. Delve into my exploration of SageMaker's diverse features, my inspirations, and the anticipation of where this exciting new direction will lead.\n",
      "This is more than just a project update; it's a chronicle of my personal growth and the lessons learned on the path of technological exploration. David Cannan Building an AI-Driven Bug Bounty Hunting System In my latest endeavor to create an AI-driven bug bounty hunting system, I found myself navigating the rich world of automation and serverless architectures. Through the integration of private repositories, Docker containers, serverless functions via OpenFaaS, and a structured GraphQL approach using Apollo Server, I've managed to craft an increasingly robust backend for bug hunting. A key turning point was the introduction of OpenAI's functions feature, capable of outputting JSON, which perfectly integrated into my existing architecture using BaseTool and significantly boosted the system's capability. This venture is far from over, but the journey thus far has been nothing short of exhilarating. Join me as I continue to explore, learn, and build. David Cannan Pwnagotchi - My Journey into Network Exploration About a year ago, I ventured into creating a Pwnagotchi, an AI-powered device designed to capture crackable WPA key material from surrounding Wi-Fi environments. This project led me down an exciting path of Python programming, Wi-Fi and GPS technologies, and deep reinforcement learning.\n",
      "Today, the Pwnagotchi stands as a testament to my journey from woodworking and cabinet-making professional to an aspiring software engineer and open-source contributor David Cannan From Carpentry to Coding A year ago, I set out on what seemed to be a straightforward project - constructing a CNC machine to assist in my woodworking and cabinet-making profession. This simple beginning, however, marked the start of an extraordinary journey that would not only revolutionize my career but also reshape my perspective on life. David Cannan The Joy of Wrapping Your Code in a FaaS Container Wrapping your code in a container with OpenFaaS is a joyful experience that brings control, flexibility, and simplicity back to your workflows. Give it a try, and enjoy the new era of serverless computing! David Cannan Exploring AI with Ansible-RoleBuilder and GPT-Engineer Embarking on a journey of self-taught software development, I've explored the intersection of DevOps and AI, integrating the power of GPT-Engineer into my project, Ansible-RoleBuilder. This post chronicles my adventure, from the creation of Ansible-RoleBuilder to the lessons learned from an AI experiment, shedding light on the potential of AI in software development for novices and veterans alike. David Cannan Unleashing the Power of AI with Google Apps Script and OpenAI: A Detailed Guide Embark on a journey with us as we delve into the world of Google Apps Script and OpenAI.\n",
      "Discover how we transformed our workflow, starting from summarizing videos to creating custom functions. Learn about the ease of writing scripts, the potential use cases, and how you can start your own project today. David Cannan Balancing Hardware Design and Creativity for Bug Hunting Infrastructure as Code Exploring the exciting frontier of hardware design, Infrastructure as Code (IaC), and embedded software development, we find a unique blend of creativity and technical proficiency. These disciplines, when combined effectively, pave the way for intelligent, automated, and highly specialized systems. This blog post delves into the tangible world of hardware, the abstract realm of IaC and embedded software, and the intricate balance between creativity and technicality. The intersection of these fields holds enormous potential, making it a thrilling area for tech enthusiasts to navigate. David Cannan Bug Hunting Infrastructure: A Journey in Infrastructure as Code (IaC) Embracing Infrastructure as Code (IaC) has been a significant step in my journey as a bug bounty hunter. David Cannan Prompt Engineering: How to, and why? Unlock the power of AI with Prompt Engineering! Our latest blog post dives into the what, why, and how of Prompt Engineering, a key skill for leveraging large language models. Learn how to craft effective prompts and guide AI to generate useful and relevant outputs. Don't miss out on this comprehensive guide to one of the most exciting fields in AI!\n",
      "David Cannan Latest Trends and Developments in AI: May 2023 In this blog post, we explore five cutting-edge AI trends, including time series analysis using sARIMA models and Dash, geospatial data analysis with GeoPandas, building simple ETL pipelines with GitHub Actions, utilizing GPT-4 for poker coaching, and machine learning methods for protein design. These trends showcase the versatility and innovative applications of AI across various domains, from data processing to gaming and even biotechnology. Stay informed and up-to-date with the latest advancements in the ever-evolving field of artificial intelligence. David Cannan Mastering Modern Web Development: My Journey with Vercel, Cloudflare, and Sanity Dive into the world of a web developer adept at creating seamless experiences with Vercel, Cloudflare, and Sanity. Learn about the challenges faced and the skills gained, showcasing a unique skill set in the ever-evolving world of web development. David Cannan Building a Monorepo with CI/CD In the ever-evolving landscape of software development, practices like Continuous Integration and Continuous Deployment (CI/CD) have become crucial for efficient collaboration and rapid delivery of new features. By implementing CI/CD in a monorepo, which serves as an \"umbrella\" repository for multiple frameworks, developers can maintain a consistent and high-quality codebase while simplifying dependency management and fostering teamwork.\n",
      "This modern approach to software development ensures a seamless experience as teams work together to build, test, and deploy their projects, ultimately accelerating the release of new features and improvements. David Cannan My CNC Journey: From Rebuilding to Debugging and Beyond Embarking on a CNC machine rebuild led me to an exciting journey of learning C++ programming, Arduino PLCs, and a newfound passion for debugging devices. Dive into my story of discovery, growth, and the love for upgrading my CNC machine. David Cannan Embracing the Future of Web Development with Headless CMS Discover the power of headless CMS and its impact on the future of web development. Learn about its benefits, including flexibility, scalability, and improved performance, and how this decoupled architecture is revolutionizing the way we manage and deliver content across multiple platforms and devices. David Cannan\n",
      "\n",
      "Source: blog.min.io_author_david-cannan_.txt\n",
      "Content: Topics All Architect's Guide Operator's Guide Best Practices AI/ML Modern Data Lakes Performance Kubernetes Integrations Benchmarks Security Multicloud Try the Erasure Code Calculator to configure your usable capacity Try Now Developing Langchain Agents with the MinIO SDK for LLM Tool-Use David Cannan David Cannan on AI/ML Explore Langchain’s LLM Tool-Use and leverage Langgraph for monitoring MinIO’s S3 Object Store. This guide walks you through developing custom conversational AI agents and creating powerful OpenAI LLM chains for efficient data management and enhanced application functionality. Read more... Powering AI/ML workflows with GitOps Automation David Cannan David Cannan on AI/ML Explore the fusion of GitOps, MinIO, Weaviate, and Python in AI development for unparalleled automation and innovation. This combination offers a solid foundation for creating scalable, efficient, and automated AI solutions, propelling projects from concept to reality with ease. Read more... Backing Up Weaviate with MinIO S3 Buckets David Cannan David Cannan on AI/ML Explore integrating MinIO with Weaviate using Docker Compose for AI-enhanced data management. Learn to back up Weaviate to MinIO S3 buckets, ensuring data integrity and scalability with practical Docker and Python examples. Streamline your AI-driven search and analysis with this robust setup. Read more...\n",
      "Innovating S3 Bucket Retrieval: Langchain Community S3 Loaders with OpenAI API David Cannan David Cannan on AI/ML Explore the synergy of MinIO, Langchain, and OpenAI in enhancing data storage and processing. This article illustrates MinIO’s integration for efficient document summarization using Langchain and OpenAI’s GPT, revolutionizing AI and ML data handling. Read more... Event-Driven Architecture: MinIO Event Notification Webhooks using Flask David Cannan David Cannan on Events Explore deploying MinIO and Flask with Docker-compose for event-driven architecture. Master MinIO bucket events and Flask webhooks for efficient data workflows and robust applications. Dive into the synergy of cloud technologies. Read more... Streamlining Data Events with MinIO and PostgreSQL David Cannan David Cannan on Events Explore 'Streamlining Data Events with MinIO and PostgreSQL,' a guide for developers using Docker, MinIO, and PostgreSQL. Learn about using Docker Compose for real-time data events, enhancing data analytics, and developing robust, event-driven applications. Read more... Smooth Sailing from Docker to Localhost David Cannan David Cannan on Docker Explore the integration of Dockerized MinIO with localhost Flask apps. This guide addresses Docker networking challenges, ensuring seamless MinIO and Flask communication for a development environment that closely mirrors production. Dive into practical solutions for robust workflows. Read more...\n",
      "\n",
      "Source: www.33rdsquare.com_langchain_.txt\n",
      "Content: LangChain: The Complete Guide for AI Developers December 22, 2023 by Jordan Brown LangChain is an open-source Python framework that connects large language models to external data for building informed AI applications. This comprehensive guide covers what LangChain provides, underlying concepts, use cases, performance analysis, current limitations and more. Introduction to LangChain Released in 2022, LangChain allows developers to connect large language models like GPT-3, BLOOM and Codex to external knowledge sources like databases, documents and proprietary data. This facilitates: Data-aware answers: Answers that combine external knowledge with language model context and semantics. Agent intelligence: Building agents that execute actions based on user input, model output, and external data state. Informed workflows: Chains that incorporate models, data processing, proxies to other systems and more to generate responses, artifacts and enact real-world changes. Initial use cases demonstrate 2-4x gains in task success rate for data heavy applications by grounding language models, reducing hallucinated output and speculation [1]. LangChain moves LLM applications beyond solely conversational to being able to query knowledge bases and take pragmatic actions. Let‘s look under the hood at how this works.\n",
      "Core Concepts A LangChain application consists of 5 key components working in conjunction: LLM Wrappers… Prompts Chains Vector Stores Agents Evolution of LangChain LangChain originated from techniques detailed in academic papers published by researchers at Anthropic – an AI safety startup that open sourced the framework [2]. In April 2022, a paper titled \"Language Model Steering through External Knowledge Retrieval\" outlined methods to connect large language models to vector databases holding document embeddings [3]. This research described: Using vector similarity search for low latency document chunk retrieval directly queriable by neural networks. Interleaving model queries with database retrievals to improve output relevance and factual accuracy over long conversations. A proposed system architecture enabling fluid incorporation of external information to \"steer\" model responses. The techniques discussed form the backbone of LangChain today. Another paper in June 2022 expanded on dynamically ordering database retrievals to maximize useful signal to models [4]. These concepts were productized into the open source Python framework LangChain in August 2022. By late 2022, LangChain saw rapid community adoption with over 5400 GitHub stars and 200+ commits from 50+ developers [5]. Multiple startups like Anthropic and Scale AI build directly on LangChain for applications, demonstrating real-world commercial value.\n",
      "Usage spansQA systems, classification models, search engines, dialog agents and more with TrailDB, Elasticsearch, FAISS, Milvus and Pinecone as popular vector store choices. This research lineage and rapid adoption underscore the transformative potential LangChain brings in connecting LLMs with external signals – let‘s see why. Architectural Deep Dive Under the surface, a LangChain deployment comprises of several key components [6]: Vectorstore Serving layer that stores vector encodings of text (embeddings) for low latency retrieval. Optimized for similarity search across millions of high dimensional vectors. Popular options include Pinecone, FAISS, Milvus. Controller Queries the vectorstore for document embeddings based on user questions and model outputs. Manages tradeoffs between precision and latency. Sampler Samples the most useful document chunks from retrieved candidates to maximize new information provided to the LLM. Prevents overloading the LLM with redundant text across retrieval steps. Optimizer Handles dynamically adjusting the number of documents retrieved per cycle to fit time constraints. Also sets the sample size and other hyperparameters like similarity threshold. Together, these components facilitate efficient incorporation of relevant external knowledge into model predictions.\n",
      "The overall workflow looks like: User provides query Query embeddings retrieved from vectorstore Embeddings ranked and filtered for novelty Corresponding document chunks sampled Text snippets concatenated to prime LLM LLM provides updated response Further query cycles initiate if needed Now let‘s see an example of building a conversational agent with LangChain. Building a Conversational Agent A key use case for LangChain is developing conversational bots that can provide knowledgeable, consistent and factual responses. This is facilitated by hooking into external data sources. Let‘s walk through a simplified architecture: Our chatbot uses: Long Short Term Memory (LSTM) Networks – Processes dialogue history to track dialog state. Extracts intent and entities. Sentiment Classifier – Detects user sentiment to guide responses. Policy Manager – Decides next action for bot based on full context. Document Vector Store – Corpus of help articles to source answers from. GPT-3 / Codex – Generates natural language responses. User queries first get processed by the NLU and state tracking modules. The policy manager selects the next action – either ask clarifying question, pull from FAQ, search documents. Relevant articles get retrieved from the vectorstore and fed to GPT-3 with dialog context to get an informed response. Generated text gets ranked and returned. By mixing reactive and knowledgeable responses, this architecture facilitates consistent and high quality conversations.\n",
      "LangChain streamlines glueing the different parts together into an integrated pipeline. Next let‘s discuss some cutting edge research projects built using LangChain. Cutting Edge Research Papers The openness and flexibility of LangChain has facilitated exciting new research into pushing LLMs to new frontiers. Here are a few sample papers and techniques unlocked: Scalable agents for dialogue modeling [7] Proposes an evaluation methodology for open ended dialogue agents Built using LangChain for incorporating common sense knowledge Demonstrates higher consistency, engagingness over baseline agents Better-Few-Shot learning for instruction following [8] Pretrains an agent on instruction following tasks using LangChain Leverages external QA systems and demonstrations to outperform baseline on new tasks Improving story generation with information filtering [9] Generates fantasy stories with narratives grounded in facts Filters retrieved information to map it appropriately into creative fiction plotlines Human evaluations show higher quality, coherency, consistency over baseline Multitasking question answering model [10] Single model able to perform multiple QA datasets with different formats LangChain provides pipeline for scalable question ingestion, document store interaction Outperforms baselines while maintaining 95% parameters of single-task models These demonstrate how LangChain facilitates innovating on top of LLMs and knowledge retrieval to push boundaries.\n",
      "Let‘s analyze quantitative performance as well. Benchmarking Performance In their paper, authors Xia et al. benchmark LangChain against traditional pipeline methods on question answering, dialogue consistency and common sense reasoning tasks [11]. Some key metrics: System Conversation Consistency QA Accuracy Common Sense Accuracy Traditional pipeline 37.2% 68.1% 83.7% LangChain 48.3% 73.2% 89.1% Across tasks, LangChain shows significant gains by avoiding compounding errors across pipeline stages. Further benchmarks by Anthropic on sales conversation success rate show 2-4x improvements over baseline. On raw compute: System Latency Throughput Model Parameters Traditional pipeline 510ms 17 q/s 125M (T5-Small) LangChain 620ms 15 q/s 125M (T5-Small) We see a moderate latency tax for improved accuracy and consistency. Parameter counts stay similar given most compute still goes to the LLM. These metrics provide confidence that LangChain can deliver meaningful accuracy and capability gains without drastic efficiency tradeoffs for many applications. But limitations remain. Limitations and Challenges While promising, LangChain has challenges to scale to enterprise grade applications: Vectorstore retrieval latency increases with size, hampering real-time use at hundreds of millions of documents. No built-in monitoring, logging or diagnostics to debug systems. Changing LLMs like GPT requires updating indexing and embeddings.\n",
      "Queries formulated poorly can fail to retrieve relevant information. Scaling to 10,000+ QPS requires specialized systems design. There are also issues intrinsic to LLMs like GPT-3: Hallucination and fact fabrication still occur without supervision. Personal beliefs and toxicity leak into responses. Limited ability to correct wrong information without retraining. Queries cost money with commercial LLMs. While areas for improvement remain, the fundamentals enable new capabilities not possible previously. Combining scalable knowledge retrieval with conversational intelligence paves the way for more useful applications. Conclusion This guide covered LangChain – its background, architecture, use cases, performance and current shortcomings. The main takeaways are: LangChain enables connecting LLMs to external data for data-aware responses. It facilitates building intelligent agents that take informed actions. Cutting edge research demonstrates capabilities not possible with standalone models. Quantitative benchmarks prove meaningful accuracy and consistency gains over baselines. There remain open challenges around scalability, monitoring and model stability. With fundamentals established here, developers should feel equipped to start building with LangChain. The opportunities to create novel applications by tying together language models, knowledge bases and external services are immense. To learn more, visit the Documentation and GitHub repo.\n",
      "Share your creations with the growing community! How useful was this post? Click on a star to rate it! Average rating 3.1 / 5. Vote count: 7 No votes so far! Be the first to rate this post. Related You May Like to Read, GPTGO: Pioneering the Future of Search Through AI Conversations Lovo AI Review: The Game-Changing Voice Solution Gamma App: A Powerful AI Presentation Tool, But Not Without Limitations Quivr AI: The Definitive Technical Guide Pushing the Boundaries of Expression with ElevenLabs Speech-to-Speech FlowGPT: The Community Platform Unlocking AI‘s Potential Monica AI: Transforming Writing and Research with Intelligent Assistance AIPRM Prompts For Sales Funnel: Tripwire Tactics Guide\n",
      "\n",
      "Source: medium.com_widle-studio_building-ai-solutions-with-langchain-and-node-js-a-comprehensive-guide-widle-studio-4812753aedff.txt\n",
      "Content: Open in app Sign up Sign in Write Sign up Sign in Building AI Solutions with LangChain and Node.js: A Comprehensive Guide Saunak Surani·Follow Published inWidle Studio LLP·14 min read·Jul 25, 2023 -- Listen Share In the rapidly evolving landscape of artificial intelligence (AI) and natural language processing (NLP), developers seek powerful tools that simplify the creation of AI-driven solutions. LangChain is an exciting and innovative library that offers a wide range of NLP capabilities to developers using Node.js. In this comprehensive guide, we will explore the world of LangChain and demonstrate how to build AI solutions effortlessly with its powerful features. Table of Contents: Understanding LangChain1.1 What is LangChain?1.2 Key Features of LangChain1.3 Why Choose LangChain for AI Solutions?\n",
      "Setting Up the Environment2.1 Installing Node.js2.2 Initializing a Node.js Project2.3 Installing LangChain with npm Working with LangChain3.1 Text Tokenization3.2 Part-of-Speech Tagging3.3 Named Entity Recognition (NER)3.4 Sentiment Analysis3.5 Language Translation3.6 Speech-to-Text and Text-to-Speech3.7 AI Chatbots with LangChain Building a Language Translator4.1 Defining the Project Structure4.2 Implementing the Translation Logic4.3 Enhancing Translation Accuracy with Language Models4.4 Creating a User-Friendly Interface Sentiment Analysis for Social Media5.1 Gathering Social Media Data5.2 Preprocessing and Analyzing Sentiments5.3 Visualizing Sentiment Analysis Results Speech-to-Text in Action6.1 Recording and Processing Audio6.2 Converting Speech to Text6.3 Transcribing\n",
      "Audio Files Integrating LangChain with Existing Apps7.1 Enhancing an E-commerce Chatbot7.2 Improving Customer Support with NLP7.3 Streamlining Language Processing in Big Data Deploying LangChain-powered Solutions8.1 Choosing the Right Hosting Environment8.2 Ensuring Scalability and Performance Future Trends and Advanced Use Cases9.1 Deep Learning with LangChain9.2 LangChain for Real-Time Translation9.3 Extending LangChain with Custom Models Best Practices for LangChain Development10.1 Code Optimization and Performance Tuning10.2 Ensuring Data Privacy and Security10.3 Continuous Integration and Testing10.4 Model Performance Monitoring10.5 Error Handling and Logging10.6 Version Control and Collaboration10.7 Documentation10.8 Deployment and Scalability Conclusion 1. Understanding LangChain 1.1 What is LangChain? LangChain is a Node.js library that empowers developers with powerful natural language processing capabilities. It leverages advanced AI algorithms and models to perform tasks like text tokenization, part-of-speech tagging, named entity recognition, language translation, and sentiment analysis. Developers can build sophisticated NLP applications effortlessly with the simplicity and efficiency of LangChain's APIs. 1.2 Key Features of LangChain Text Tokenization: Breaks down textual content into smaller tokens, facilitating language processing tasks. Part-of-Speech Tagging: Identifies the grammatical parts of each word in a sentence, aiding in language understanding.\n",
      "Named Entity Recognition (NER): Detects and categorizes entities like names, locations, and dates in a text. Sentiment Analysis: Determines the emotional tone of a text, classifying it as positive, negative, or neutral. Language Translation: Translates text from one language to another, bridging communication gaps. Speech-to-Text and Text-to-Speech: Converts audio recordings into written text and generates speech from textual content. 1.3 Why Choose LangChain for AI Solutions? LangChain offers a myriad of benefits that make it an excellent choice for AI-driven solutions: Ease of Integration: LangChain is easy to integrate into existing Node.js projects, making it accessible to developers of all levels of expertise. Versatility: With a wide range of NLP capabilities, LangChain caters to diverse AI solution requirements. High Performance: LangChain's underlying algorithms ensure speedy and efficient language processing. Community and Support: The LangChain community provides active support and regular updates, ensuring a seamless development experience. 2. Setting Up the Environment 2.1 Installing Node.js Before we start using LangChain, we need to have Node.js installed on our system. Node.js is a JavaScript runtime that enables us to run JavaScript code outside the browser. Visit the official Node.js website (https://nodejs.org) and download the latest stable version for your operating system. Once installed, verify the installation by running node -v in your terminal.\n",
      "2.2 Initializing a Node.js Project Once Node.js is installed, navigate to your project directory and initialize a new Node.js project by running npm init in the terminal. Follow the prompts to set up your project, and a package.json file will be created. 2.3 Installing LangChain with npm With the Node.js project set up, we can now install LangChain using npm, the Node.js package manager. Open the terminal and run the following command to install LangChain as a dependency in your project: npm install -S langchain The -S flag saves LangChain as a dependency in the package.json file. 3. Working with LangChain With LangChain successfully installed, we can now explore its powerful NLP capabilities. Let's dive into various tasks we can perform with LangChain. 3.1 Text Tokenization Text tokenization is the process of breaking down text into smaller units, called tokens. These tokens can be words, phrases, or sentences, depending on the level of tokenization required. LangChain provides an easy-to-use function to tokenize text: const langchain = require('langchain'); const text = \"LangChain makes NLP easy! \"; const tokens = langchain.tokenize(text); console.log(tokens); The output will be an array of tokens: [\"LangChain\", \"makes\", \"NLP\", \"easy\", \"!\"]. 3.2 Part-of-Speech Tagging Part-of-speech tagging involves assigning grammatical parts to each word in a sentence, such as nouns, verbs, adjectives, etc.\n",
      "LangChain enables us to perform part-of-speech tagging with a simple function call: const langchain = require('langchain'); const sentence = \"LangChain is an amazing tool. \"; const posTags = langchain.posTag(sentence); console.log(posTags); The output will be an array of objects, each representing a word and its part-of-speech tag: [{\"word\": \"LangChain\", \"tag\": \"NNP\"}, {\"word\": \"is\", \"tag\": \"VBZ\"}, {\"word\": \"an\", \"tag\": \"DT\"}, {\"word\": \"amazing\", \"tag\": \"JJ\"}, {\"word\": \"tool\", \"tag\": \"NN\"}, {\"word\": \". \", \"tag\": \".\"}]. 3.3 Named Entity Recognition (NER) Named Entity Recognition identifies and categorizes entities like names, locations, organizations, and dates in a text. LangChain provides a function to perform NER: const langchain = require('langchain'); const text = \"Apple Inc. was founded by Steve Jobs in Cupertino on April 1, 1976. \"; const entities = langchain.ner(text); console.log(entities); The output will be an array of objects, each representing an entity and its category: [{\"text\": \"Apple Inc.\", \"category\": \"ORG\"}, {\"text\": \"Steve Jobs\", \"category\": \"PERSON\"}, {\"text\": \"Cupertino\", \"category\": \"LOCATION\"}, {\"text\": \"April 1, 1976\", \"category\": \"DATE\"}]. 3.4 Sentiment Analysis Sentiment analysis helps determine the emotional tone of a piece of text, classifying it as positive, negative, or neutral. LangChain provides a sentiment analysis function: const langchain = require('langchain'); const review = \"The movie was fantastic!\n",
      "\"; const sentiment = langchain.sentiment(review); console.log(sentiment); The output will be a sentiment object with a label representing the sentiment and a score indicating the confidence level: {\"label\": \"positive\", \"score\": 0.9}. 3.5 Language Translation Language translation involves converting text from one language to another. LangChain offers an easy-to-use translation function: const langchain = require('langchain'); const textToTranslate = \"Hello, how are you? \"; const translatedText = langchain.translate(textToTranslate, 'fr'); // Translate to French console.log(translatedText); The output will be the translated text: \"Bonjour, comment ça va?\". 3.6 Speech-to-Text and Text-to-Speech LangChain also provides functions for speech-to-text (STT) and text-to-speech (TTS) conversion. These features enable interaction with audio data: const langchain = require('langchain'); // Speech-to-Text const audioFile = \"path/to/audiofile.wav\"; const transcript = langchain.speechToText(audioFile); console.log(transcript); // Text-to-Speech const textToSpeak = \"Welcome to LangChain. \"; langchain.textToSpeech(textToSpeak, 'en'); // Convert text to English speech 3.7 AI Chatbots with LangChain LangChain can power AI chatbots by combining its NLP capabilities with chatbot logic. Developers can create chatbots that understand user input, generate appropriate responses, and even conduct sentiment analysis on user messages. 4.\n",
      "Building a Language Translator To demonstrate LangChain's power, let's build a simple language translator that converts text from one language to another. We'll use the Google Translate API for translation, combined with LangChain for preprocessing. 4.1 Defining the Project Structure Set up the project structure with the following files: translator.js translator.js package.json 4.2 Implementing the Translation Logic In the translator.js file, we'll implement the language translation logic using LangChain and Google Translate API: const langchain = require('langchain'); const { Translate } = require('@google-cloud/translate').v2; // Google Cloud Translate API Configuration const translate = new Translate({ projectId: 'your-project-id', keyFilename: 'path/to/your/credentials.json', }); // Function to translate text async function translateText(text, targetLanguage) { const preprocessedText = langchain.preprocess(text); // Preprocess text using LangChain const [translation] = await translate.translate(preprocessedText, targetLanguage); return translation; module.exports = { translateText }; In this code, we set up the Google Translate API client and define a function translateText() that takes the text and the target language code as input. The text is preprocessed using LangChain's preprocess() function, and then the Google Translate API is used to perform the translation.\n",
      "4.3 Enhancing Translation Accuracy with Language Models To improve translation accuracy, we can leverage LangChain's language models. Let's integrate a language model to enhance our language translator: const langchain = require('langchain'); const { Translate } = require('@google-cloud/translate').v2; // Google Cloud Translate API Configuration const translate = new Translate({ projectId: 'your-project-id', keyFilename: 'path/to/your/credentials.json', }); // Function to translate text with language model async function translateWithModel(text, targetLanguage, model) { const preprocessedText = langchain.preprocess(text); const [translation] = await translate.translate(preprocessedText, { targetLanguage, model, }); return translation; module.exports = { translateWithModel }; In this enhanced version, we added a model parameter to the translateWithModel() function, allowing us to specify a specific language model for translation. The available models depend on the supported languages and may include general models, news models, or conversation models.\n",
      "4.4 Creating a User-Friendly Interface Now that our translation logic is in place, let's create a simple command-line interface for our language translator: const { translateWithModel } = require('./translator'); const readline = require('readline'); const rl = readline.createInterface({ input: process.stdin, output: process.stdout, }); rl.question('Enter text to translate: ', async (text) => { rl.question('Enter target language code (e.g., fr for French): ', async (targetLanguage) => { rl.question('Enter model (e.g., nmt for Neural Machine Translation): ', async (model) => { try { const translatedText = await translateWithModel(text, targetLanguage, model); console.log('Translated Text:', translatedText); } catch (error) { console.error('Translation Error:', error.message); } finally { rl.close(); }); }); }); In this interface, we use readline to accept user input for the text to translate, the target language code, and the desired model for translation. The translateWithModel() function is called, and the translated text is displayed to the user. 5. Sentiment Analysis for Social Media In this section, we will use LangChain's sentiment analysis feature to analyze sentiments expressed on social media platforms. We'll gather social media data, perform sentiment analysis, and visualize the results. 5.1 Gathering Social Media Data To demonstrate sentiment analysis, we'll assume we have a dataset of social media posts stored in a JSON file.\n",
      "Each post object includes a text field with the post content and a timestamp field with the post's timestamp: \"text\": \"Just watched an amazing movie! \", \"timestamp\": \"2023-07-01T12:30:00Z\" }, \"text\": \"Feeling down today...\", \"timestamp\": \"2023-07-02T08:45:00Z\" }, \"text\": \"Excited for the weekend getaway! \", \"timestamp\": \"2023-07-03T15:20:00Z\" }, // More posts... To read this data into our Node.js environment, we can use the fs module: const fs = require('fs'); const socialMediaData = JSON.parse(fs.readFileSync('social_media_data.json', 'utf8')); 5.2 Preprocessing and Analyzing Sentiments Next, we'll preprocess the social media posts and perform sentiment analysis using LangChain: const langchain = require('langchain'); // Function to preprocess text and perform sentiment analysis function analyzeSentiments(posts) { const sentiments = []; posts.forEach((post) => { const { text, timestamp } = post; const preprocessedText = langchain.preprocess(text); const sentiment = langchain.sentiment(preprocessedText); sentiments.push({ text, timestamp, sentiment }); }); return sentiments; const socialMediaSentiments = analyzeSentiments(socialMediaData); console.log(socialMediaSentiments); The analyzeSentiments() function preprocesses each post's text using Lang Chain's preprocess() function and performs sentiment analysis with sentiment(). The results are stored in an array containing the post's original text, timestamp, and sentiment object.\n",
      "5.3 Visualizing Sentiment Analysis Results To visualize the sentiment analysis results, we can use a plotting library like chart.js: npm install chart.js Now, let's create a bar chart to display the sentiments over time: const { analyzeSentiments } = require('./sentiment'); const ctx = document.getElementById('sentimentChart').getContext('2d'); const posts = socialMediaSentiments.map((post) => post.text); const sentiments = socialMediaSentiments.map((post) => post.sentiment.score); const timestamps = socialMediaSentiments.map((post) => new Date(post.timestamp)); const data = { labels: timestamps, datasets: [ label: 'Sentiment Score', data: sentiments, backgroundColor: 'rgba(75, 192, 192, 0.2)', borderColor: 'rgba(75, 192, 192, 1)', borderWidth: 1, }, ], }; const config = { type: 'bar', data: data, options: { scales: { x: { type: 'time', time: { unit: 'day', }, }, y: { beginAtZero: true, suggestedMax: 1, }, }, }, }; const myChart = new Chart(ctx, config); In this code, we use Chart.js to create a bar chart to visualize the sentiments over time. The chart displays the sentiment score on the y-axis and the timestamps on the x-axis. 6. Speech-to-Text in Action In this section, we will use LangChain's speech-to-text feature to transcribe audio recordings into text. Let's assume we have an audio file of an interview that we want to transcribe. 6.1 Recording and Processing Audio To perform speech-to-text, we need an audio recording.\n",
      "You can record a short audio clip using any recording device or use an existing audio file. Once you have the audio file, ensure it's in a compatible format (e.g., WAV or MP3). 6.2 Converting Speech to Text With the audio file ready, we can use LangChain's speech-to-text functionality: const langchain = require('langchain'); // Function to transcribe audio to text async function transcribeAudio(audioFilePath) { const transcript = await langchain.speechToText(audioFilePath); return transcript; const audioFilePath = 'path/to/audiofile.wav'; transcribeAudio(audioFilePath) .then((transcript) => console.log('Transcription:', transcript)) .catch((error) => console.error('Transcription Error:', error.message)); In this code, we define the transcribeAudio() function, which takes the path to the audio file as input and returns the transcribed text. 6.3 Transcribing Audio Files The transcribeAudio() function uses LangChain's speechToText() function to perform the transcription. Make sure you have a stable internet connection as the transcription requires API calls to LangChain's backend. 7. Integrating LangChain with Existing Apps One of LangChain's strengths is its ability to integrate seamlessly with existing applications. Here are a few examples of how to enhance applications with LangChain's NLP capabilities. 7.1 Enhancing an E-commerce Chatbot Imagine you have an e-commerce website with a chatbot to assist customers.\n",
      "By integrating LangChain, you can enable the chatbot to understand user queries better and provide more accurate responses. const langchain = require('langchain'); // Function to process user query function processUserQuery(userInput) { const preprocessedQuery = langchain.preprocess(userInput); // Perform logic to handle the user query // Return appropriate response In this code, we preprocess the user's input using LangChain's preprocess() function, which standardizes the text for better processing. 7.2 Improving Customer Support with NLP For businesses offering customer support, LangChain's sentiment analysis can be invaluable in understanding customer feedback. By analyzing the sentiments expressed in customer messages, support teams can identify potential issues and respond appropriately. const langchain = require('langchain'); // Function to analyze customer feedback function analyzeCustomerFeedback(feedback) { const sentiment = langchain.sentiment(feedback); if (sentiment.label === 'negative') { // Notify the support team about the negative feedback // Take appropriate action In this example, we use LangChain's sentiment analysis to identify negative feedback and notify the support team for a quick resolution. 7.3 Streamlining Language Processing in Big Data For large-scale applications dealing with big data, LangChain's language processing capabilities can be a game-changer.\n",
      "By integrating LangChain into your big data pipeline, you can analyze vast amounts of text data efficiently. const langchain = require('langchain'); // Function to analyze text data in big data pipeline function analyzeTextData(textData) { const preprocessedData = textData.map((text) => langchain.preprocess(text)); // Perform analysis on the preprocessed data // Return insights and results In this scenario, LangChain preprocesses the text data before analysis, ensuring consistency and accurate results. 8. Deploying LangChain-powered Solutions After developing AI solutions with LangChain, the next step is deployment. Deployment requires considerations such as hosting environment, scalability, and performance. 8.1 Choosing the Right Hosting Environment When deploying LangChain-powered applications, you can choose from various hosting options, including cloud platforms like Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP). Each platform offers different services and pricing models, so select the one that best suits your project's requirements and budget. 8.2 Ensuring Scalability and Performance To handle increasing user demands, ensure that your LangChain-powered application is scalable. Utilize cloud-based solutions that offer auto-scaling capabilities to dynamically adjust resources based on traffic. Additionally, optimize your code and minimize unnecessary computations to improve overall performance. 9.\n",
      "Future Trends and Advanced Use Cases As the field of AI continues to evolve, LangChain is likely to keep up with advancements. Here are some future trends and advanced use cases to explore: 9.1 Deep Learning with LangChain LangChain may integrate more advanced deep learning models to improve the accuracy and performance of NLP tasks. This can lead to more accurate translations, better sentiment analysis, and advanced language understanding. 9.2 LangChain for Real-Time Translation The demand for real-time language translation is on the rise. LangChain can further enhance its translation capabilities to deliver instantaneous translations for live events, meetings, and communication across languages. 9.3 Extending LangChain with Custom Models To cater to specific business needs, LangChain may offer the option to integrate custom NLP models. Businesses can train their language models and use them in combination with LangChain's existing features. 10. Best Practices for LangChain Development Now that we have explored the capabilities of LangChain and how to integrate it with Node.js, let’s dive into some best practices for LangChain development. Following these practices will not only improve the performance and efficiency of your AI solutions but also enhance the overall development process. 10.1 Optimize Data Processing When dealing with large volumes of text data, data processing can be a significant bottleneck.\n",
      "To optimize data processing, consider the following techniques: a) Batch Processing: Instead of processing individual text entries one by one, group them into batches and process them together. Batch processing can significantly reduce the overhead and improve efficiency. b) Multithreading: Utilize multithreading or worker threads to parallelize data processing tasks. This approach can take advantage of multi-core processors and speed up computations. c) Streaming: When working with real-time data streams, consider using stream processing to handle data in chunks rather than loading the entire dataset at once. This can lead to more efficient memory usage and faster processing. 10.2 Ensure Data Privacy and Security When working with sensitive data, it’s crucial to prioritize data privacy and security. Here are some practices to follow: a) Encryption: Encrypt data at rest and in transit to protect it from unauthorized access. b) Access Controls: Implement proper access controls and authentication mechanisms to restrict access to sensitive data. c) Data Anonymization: If possible, anonymize data to protect the identities of individuals in the dataset. 10.3 Continuous Integration and Testing Implement continuous integration (CI) and automated testing to catch bugs and issues early in the development process. Set up a robust testing framework to thoroughly test your AI solutions under different scenarios.\n",
      "Automated testing ensures that changes or updates to the codebase do not introduce regressions. 10.4 Model Performance Monitoring AI models may degrade in performance over time due to changes in data distribution or other factors. Implement monitoring mechanisms to track model performance regularly. If the model’s performance drops below an acceptable threshold, consider retraining it with fresh data. 10.5 Error Handling and Logging Proper error handling and logging are essential to diagnose issues and troubleshoot problems effectively. Implement comprehensive error handling in your code and use logging frameworks to record errors and important events. 10.6 Version Control and Collaboration Utilize version control systems like Git to track changes to your codebase and facilitate collaboration among team members. Version control allows you to revert to previous versions if needed and helps maintain a clean and organized codebase. 10.7 Documentation Document your code thoroughly, including the purpose and functionality of each module or function. Well-documented code is easier to understand and maintain, making it more accessible to other developers who might work on the project. 10.8 Deployment and Scalability When deploying AI solutions built with LangChain, consider the scalability requirements. Ensure that the deployment environment can handle the expected workload and traffic.\n",
      "Utilize containerization technologies like Docker to package your application and its dependencies for easy deployment. 11. Conclusion LangChain offers a powerful and user-friendly platform for natural language processing tasks, making it an excellent choice for developers looking to build AI solutions without the complexity of creating models from scratch. By integrating LangChain with Node.js, developers can harness the power of AI to process and understand vast amounts of text data, unlocking a world of possibilities in the realm of NLP. In this comprehensive guide, we explored the capabilities of LangChain, from text tokenization to language translation and sentiment analysis. We learned how to set up LangChain in a Node.js environment and built practical AI-driven applications. Additionally, we discussed best practices for LangChain development, including optimizing data processing, ensuring data privacy and security, continuous integration and testing, and responsible AI practices. With LangChain’s versatility and Node.js’s flexibility, developers can build intelligent and efficient AI solutions for various industries and use cases. So, don’t wait any longer! Start building your own AI solutions with LangChain and Node.js to revolutionize the way you process and understand natural language data. Contact Us: For more information or inquiries, feel free to contact us at info@widle.studio.\n",
      "We would love to hear your feedback and discuss how LangChain can power your AI projects. Disclaimer: The views and opinions expressed in this article are those of the author and do not necessarily reflect the official policy or position of LangChain or any other company Langchain Nodejs AI Widle Studio Machine Learning -- -- Follow Written by Saunak Surani 90 Followers Editor for Widle Studio LLP Passionate about technology, design, startups, and personal development. Bringing ideas to life at https://widle.studio Follow Help Status About Careers Blog Privacy Terms Text to speech Teams\n",
      "\n",
      "Source: medium.com__aisagescribe_langchain-101-a-comprehensive-introduction-guide-7a5db81afa49.txt\n",
      "Content: Open in app Sign up Sign in Write Sign up Sign in Member-only story LangChain 101: A comprehensive introduction guide AI SageScribe·Follow 7 min read·Jan 28, 2024 -- Share In the ever-evolving landscape of machine learning and artificial intelligence, the advent of large language models (LLMs) like GPT-4 has opened a new frontier in natural language processing and generation. Enter LangChain, a cutting-edge software development framework designed to harness the power of these models in creating robust and dynamic applications. This framework is a game-changer for developers looking to integrate LLMs into their products seamlessly. Core Principles of LangChain Data-Aware Design LangChain stands out with its data-aware approach, allowing a language model to connect with various data sources. This integration enriches the model’s responses, making them more relevant and context-specific. Agentic Interaction The framework also emphasizes agentic interaction, enabling language models to actively engage with their environment. This approach paves the way for more interactive and responsive applications.\n",
      "High-level Structure of LangChain Below is a list of key modules of LangChain Models: Supported model types and integrations Prompts: Prompt template, optimization and serialization Memory: Memory refers to state that is persisted between calls of a chain/agent Indexes: This module contains interfaces and integrations for loading, querying and updating external data Chains: Chains are structured sequences of calls (to an LLM or to a different utility) Agents: An agent is a Chain in which an LLM, given a high-level directive and a set of tools, repeated decides an action, executes the action and observes the outcome untile high level directive is complete Callbacks: Callbacks let you log and stream the intermediate steps of any chain, making it easy to observe, debug and evaluate the internals of an application. LangChain’s different Module Explained Models Module -- -- Follow Written by AI SageScribe 22 Followers Thoughts on Machine Learning, Data Science, Stats Follow Help Status About Careers Blog Privacy Terms Text to speech Teams\n",
      "\n",
      "Source: nanonets.com_blog_langchain_amp_.txt\n",
      "Content: A Complete LangChain Guide At its core, LangChain is an innovative framework tailored for crafting applications that leverage the capabilities of language models. It's a toolkit designed for developers to create applications that are context-aware and capable of sophisticated reasoning. This means LangChain applications can understand the context, such as prompt instructions or content grounding responses and use language models for complex reasoning tasks, like deciding how to respond or what actions to take. LangChain represents a unified approach to developing intelligent applications, simplifying the journey from concept to execution with its diverse components. While discussing the utility of LangChain for handling document data, it's crucial to mention the power of workflow automation. Nanonets' Workflow Automation platform takes efficiency to the next level by allowing you to seamlessly integrate AI and human-in-loop systems. Imagine transforming those scanned documents into actionable data without manual effort. With our intuitive platform, you can connect LangChain to a vast array of apps and services, streamlining your processes and freeing up valuable time. Learn more about how you can build robust, AI-enhanced workflows within minutes. Learn More Understanding LangChain LangChain is much more than just a framework; it's a full-fledged ecosystem comprising several integral parts. Firstly, there are the LangChain Libraries, available in both Python and JavaScript.\n",
      "These libraries are the backbone of LangChain, offering interfaces and integrations for various components. They provide a basic runtime for combining these components into cohesive chains and agents, along with ready-made implementations for immediate use. Next, we have LangChain Templates. These are a collection of deployable reference architectures tailored for a wide array of tasks. Whether you're building a chatbot or a complex analytical tool, these templates offer a solid starting point. LangServe steps in as a versatile library for deploying LangChain chains as REST APIs. This tool is essential for turning your LangChain projects into accessible and scalable web services. Lastly, LangSmith serves as a developer platform. It's designed to debug, test, evaluate, and monitor chains built on any LLM framework. The seamless integration with LangChain makes it an indispensable tool for developers aiming to refine and perfect their applications. Together, these components empower you to develop, productionize, and deploy applications with ease. With LangChain, you start by writing your applications using the libraries, referencing templates for guidance. LangSmith then helps you in inspecting, testing, and monitoring your chains, ensuring that your applications are constantly improving and ready for deployment. Finally, with LangServe, you can easily transform any chain into an API, making deployment a breeze.\n",
      "In the next sections, we will delve deeper into how to set up LangChain and begin your journey in creating intelligent, language model-powered applications. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams. Get Started Installation and Setup Are you ready to dive into the world of LangChain? Setting it up is straightforward, and this guide will walk you through the process step-by-step. The first step in your LangChain journey is to install it. You can do this easily using pip or conda. Run the following command in your terminal: For those who prefer the latest features and are comfortable with a bit more adventure, you can install LangChain directly from the source. Clone the repository and navigate to the langchain/libs/langchain directory. Then, run: For experimental features, consider installing langchain-experimental. It's a package that contains cutting-edge code and is intended for research and experimental purposes. Install it using: LangChain CLI is a handy tool for working with LangChain templates and LangServe projects. To install the LangChain CLI, use: LangServe is essential for deploying your LangChain chains as a REST API. It gets installed alongside the LangChain CLI. LangChain often requires integrations with model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.\n",
      "Install the OpenAI Python package using: To access the API, set your OpenAI API key as an environment variable: Alternatively, pass the key directly in your python environment: LangChain allows for the creation of language model applications through modules. These modules can either stand alone or be composed for complex use cases. These modules are - Model I/O: Facilitates interaction with various language models, handling their inputs and outputs efficiently. Retrieval: Enables access to and interaction with application-specific data, crucial for dynamic data utilization. Agents: Empower applications to select appropriate tools based on high-level directives, enhancing decision-making capabilities. Chains: Offers pre-defined, reusable compositions that serve as building blocks for application development. Memory: Maintains application state across multiple chain executions, essential for context-aware interactions. Each module targets specific development needs, making LangChain a comprehensive toolkit for creating advanced language model applications. Along with the above components, we also have LangChain Expression Language (LCEL), which is a declarative way to easily compose modules together, and this enables the chaining of components using a universal Runnable interface. LCEL looks something like this - Now that we have covered the basics, we will continue on to: Dig deeper into each Langchain module in detail. Learn how to use LangChain Expression Language.\n",
      "Explore common use cases and implement them. Deploy an end-to-end application with LangServe. Check out LangSmith for debugging, testing, and monitoring. Let's get started! Module I : Model I/O In LangChain, the core element of any application revolves around the language model. This module provides the essential building blocks to interface effectively with any language model, ensuring seamless integration and communication. Key Components of Model I/O LLMs and Chat Models (used interchangeably):LLMs:Definition: Pure text completion models.Input/Output: Take a text string as input and return a text string as output.Chat Models Definition: Models that use a language model as a base but differ in input and output formats. Input/Output: Accept a list of chat messages as input and return a Chat Message. Prompts: Templatize, dynamically select, and manage model inputs. Allows for the creation of flexible and context-specific prompts that guide the language model's responses. Output Parsers: Extract and format information from model outputs. Useful for converting the raw output of language models into structured data or specific formats needed by the application. LLMs LangChain's integration with Large Language Models (LLMs) like OpenAI, Cohere, and Hugging Face is a fundamental aspect of its functionality. LangChain itself does not host LLMs but offers a uniform interface to interact with various LLMs.\n",
      "This section provides an overview of using the OpenAI LLM wrapper in LangChain, applicable to other LLM types as well. We have already installed this in the \"Getting Started\" section. Let us initialize the LLM. LLMs implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls. LLMs accept strings as inputs, or objects which can be coerced to string prompts, including List[BaseMessage] and PromptValue. (more on these later) Let us look at some examples. You can alternatively call the stream method to stream the text response. Chat Models LangChain's integration with chat models, a specialized variation of language models, is essential for creating interactive chat applications. While they utilize language models internally, chat models present a distinct interface centered around chat messages as inputs and outputs. This section provides a detailed overview of using OpenAI's chat model in LangChain. Chat models primarily accept List[BaseMessage] as inputs. Strings can be converted to HumanMessage, and PromptValue is also supported. Prompts Prompts are essential in guiding language models to generate relevant and coherent outputs. They can range from simple instructions to complex few-shot examples. In LangChain, handling prompts can be a very streamlined process, thanks to several dedicated classes and functions.\n",
      "LangChain's PromptTemplate class is a versatile tool for creating string prompts. It uses Python's str.format syntax, allowing for dynamic prompt generation. You can define a template with placeholders and fill them with specific values as needed. For chat models, prompts are more structured, involving messages with specific roles. LangChain offers ChatPromptTemplate for this purpose. This approach allows for the creation of interactive, engaging chatbots with dynamic responses. Both PromptTemplate and ChatPromptTemplate integrate seamlessly with the LangChain Expression Language (LCEL), enabling them to be part of larger, complex workflows. We will discuss more on this later. Custom prompt templates are sometimes essential for tasks requiring unique formatting or specific instructions. Creating a custom prompt template involves defining input variables and a custom formatting method. This flexibility allows LangChain to cater to a wide array of application-specific requirements. Read more here. LangChain also supports few-shot prompting, enabling the model to learn from examples. This feature is vital for tasks requiring contextual understanding or specific patterns. Few-shot prompt templates can be built from a set of examples or by utilizing an Example Selector object. Read more here. Output Parsers Output parsers play a crucial role in Langchain, enabling users to structure the responses generated by language models.\n",
      "In this section, we will explore the concept of output parsers and provide code examples using Langchain's PydanticOutputParser, SimpleJsonOutputParser, CommaSeparatedListOutputParser, DatetimeOutputParser, and XMLOutputParser. PydanticOutputParser Langchain provides the PydanticOutputParser for parsing responses into Pydantic data structures. Below is a step-by-step example of how to use it: The output will be: SimpleJsonOutputParser Langchain's SimpleJsonOutputParser is used when you want to parse JSON-like outputs. Here's an example: CommaSeparatedListOutputParser The CommaSeparatedListOutputParser is handy when you want to extract comma-separated lists from model responses. Here's an example: DatetimeOutputParser Langchain's DatetimeOutputParser is designed to parse datetime information. Here's how to use it: These examples showcase how Langchain's output parsers can be used to structure various types of model responses, making them suitable for different applications and formats. Output parsers are a valuable tool for enhancing the usability and interpretability of language model outputs in Langchain. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams. Get Started Module II : Retrieval Retrieval in LangChain plays a crucial role in applications that require user-specific data, not included in the model's training set.\n",
      "This process, known as Retrieval Augmented Generation (RAG), involves fetching external data and integrating it into the language model's generation process. LangChain provides a comprehensive suite of tools and functionalities to facilitate this process, catering to both simple and complex applications. LangChain achieves retrieval through a series of components which we will discuss one by one. Document Loaders Document loaders in LangChain enable the extraction of data from various sources. With over 100 loaders available, they support a range of document types, apps and sources (private s3 buckets, public websites, databases). You can choose a document loader based on your requirements here. All these loaders ingest data into Document classes. We'll learn how to use data ingested into Document classes later. Text File Loader: Load a simple .txt file into a document. CSV Loader: Load a CSV file into a document. We can choose to customize the parsing by specifying field names - PDF Loaders: PDF Loaders in LangChain offer various methods for parsing and extracting content from PDF files. Each loader caters to different requirements and uses different underlying libraries. Below are detailed examples for each loader. PyPDFLoader is used for basic PDF parsing. MathPixLoader is ideal for extracting mathematical content and diagrams. PyMuPDFLoader is fast and includes detailed metadata extraction. PDFMiner Loader is used for more granular control over text extraction.\n",
      "AmazonTextractPDFParser utilizes AWS Textract for OCR and other advanced PDF parsing features. PDFMinerPDFasHTMLLoader generates HTML from PDF for semantic parsing. PDFPlumberLoader provides detailed metadata and supports one document per page. Integrated Loaders: LangChain offers a wide variety of custom loaders to directly load data from your apps (such as Slack, Sigma, Notion, Confluence, Google Drive and many more) and databases and use them in LLM applications. The complete list is here. Below are a couple of examples to illustrate this - Example I - Slack Slack, a widely-used instant messaging platform, can be integrated into LLM workflows and applications. Go to your Slack Workspace Management page. Navigate to {your_slack_domain}.slack.com/services/export. Select the desired date range and initiate the export. Slack notifies via email and DM once the export is ready. The export results in a .zip file located in your Downloads folder or your designated download path. Assign the path of the downloaded .zip file to LOCAL_ZIPFILE. Use the SlackDirectoryLoader from the langchain.document_loaders package. Example II - Figma Figma, a popular tool for interface design, offers a REST API for data integration. Obtain the Figma file key from the URL format: https://www.figma.com/file/{filekey}/sampleFilename. Node IDs are found in the URL parameter ?node-id={node_id}. Generate an access token following instructions at the Figma Help Center.\n",
      "The FigmaFileLoader class from langchain.document_loaders.figma is used to load Figma data. Various LangChain modules like CharacterTextSplitter, ChatOpenAI, etc., are employed for processing. The generate_code function uses the Figma data to create HTML/CSS code. It employs a templated conversation with a GPT-based model. The generate_code function, when executed, returns HTML/CSS code based on the Figma design input. Let us now use our knowledge to create a few document sets. We first load a PDF, the BCG annual sustainability report. We use the PyPDFLoader for this. We will ingest data from Airtable now. We have an Airtable containing information about various OCR and data extraction models - Let us use the AirtableLoader for this, found in the list of integrated loaders. Let us now proceed and learn how to use these document classes. Document Transformers Document transformers in LangChain are essential tools designed to manipulate documents, which we created in our previous subsection. They are used for tasks such as splitting long documents into smaller chunks, combining, and filtering, which are crucial for adapting documents to a model's context window or meeting specific application needs. One such tool is the RecursiveCharacterTextSplitter, a versatile text splitter that uses a character list for splitting. It allows parameters like chunk size, overlap, and starting index.\n",
      "Here's an example of how it's used in Python: Another tool is the CharacterTextSplitter, which splits text based on a specified character and includes controls for chunk size and overlap: The HTMLHeaderTextSplitter is designed to split HTML content based on header tags, retaining the semantic structure: A more complex manipulation can be achieved by combining HTMLHeaderTextSplitter with another splitter, like the Pipelined Splitter: LangChain also offers specific splitters for different programming languages, like the Python Code Splitter and the JavaScript Code Splitter: For splitting text based on token count, which is useful for language models with token limits, the TokenTextSplitter is used: Finally, the LongContextReorder reorders documents to prevent performance degradation in models due to long contexts: These tools demonstrate various ways to transform documents in LangChain, from simple text splitting to complex reordering and language-specific splitting. For more in-depth and specific use cases, the LangChain documentation and Integrations section should be consulted. In our examples, the loaders have already created chunked documents for us, and this part is already handled. Text Embedding Models Text embedding models in LangChain provide a standardized interface for various embedding model providers like OpenAI, Cohere, and Hugging Face.\n",
      "These models transform text into vector representations, enabling operations like semantic search through text similarity in vector space. To get started with text embedding models, you typically need to install specific packages and set up API keys. We have already done this for OpenAI In LangChain, the embed_documents method is used to embed multiple texts, providing a list of vector representations. For instance: For embedding a single text, such as a search query, the embed_query method is used. This is useful for comparing a query to a set of document embeddings. For example: Understanding these embeddings is crucial. Each piece of text is converted into a vector, the dimension of which depends on the model used. For instance, OpenAI models typically produce 1536-dimensional vectors. These embeddings are then used for retrieving relevant information. LangChain's embedding functionality is not limited to OpenAI but is designed to work with various providers. The setup and usage might slightly differ depending on the provider, but the core concept of embedding texts into vector space remains the same. For detailed usage, including advanced configurations and integrations with different embedding model providers, the LangChain documentation in the Integrations section is a valuable resource. Vector Stores Vector stores in LangChain support the efficient storage and searching of text embeddings.\n",
      "LangChain integrates with over 50 vector stores, providing a standardized interface for ease of use. Example: Storing and Searching Embeddings After embedding texts, we can store them in a vector store like Chroma and perform similarity searches: Let us alternatively use the FAISS vector store to create indexes for our documents. Retrievers Retrievers in LangChain are interfaces that return documents in response to an unstructured query. They are more general than vector stores, focusing on retrieval rather than storage. Although vector stores can be used as a retriever's backbone, there are other types of retrievers as well. To set up a Chroma retriever, you first install it using pip install chromadb. Then, you load, split, embed, and retrieve documents using a series of Python commands. Here's a code example for setting up a Chroma retriever: The MultiQueryRetriever automates prompt tuning by generating multiple queries for a user input query and combines the results. Here's an example of its simple usage: Contextual Compression in LangChain compresses retrieved documents using the context of the query, ensuring only relevant information is returned. This involves content reduction and filtering out less relevant documents. The following code example shows how to use Contextual Compression Retriever: The EnsembleRetriever combines different retrieval algorithms to achieve better performance.\n",
      "An example of combining BM25 and FAISS Retrievers is shown in the following code: MultiVector Retriever in LangChain allows querying documents with multiple vectors per document, which is useful for capturing different semantic aspects within a document. Methods for creating multiple vectors include splitting into smaller chunks, summarizing, or generating hypothetical questions. For splitting documents into smaller chunks, the following Python code can be used: Generating summaries for better retrieval due to more focused content representation is another method. Here's an example of generating summaries: Generating hypothetical questions relevant to each document using LLM is another approach. This can be done with the following code: The Parent Document Retriever is another retriever that strikes a balance between embedding accuracy and context retention by storing small chunks and retrieving their larger parent documents. Its implementation is as follows: A self-querying retriever constructs structured queries from natural language inputs and applies them to its underlying VectorStore. Its implementation is shown in the following code: The WebResearchRetriever performs web research based on a given query - For our examples, we can also use the standard retriever already implemented as part of our vector store object as follows - We can now query the retrievers. The output of our query will be document objects relevant to the query.\n",
      "These will be ultimately utilized to create relevant responses in further sections. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams. Get Started Module III : Agents LangChain introduces a powerful concept called \"Agents\" that takes the idea of chains to a whole new level. Agents leverage language models to dynamically determine sequences of actions to perform, making them incredibly versatile and adaptive. Unlike traditional chains, where actions are hardcoded in code, agents employ language models as reasoning engines to decide which actions to take and in what order. The Agent is the core component responsible for decision-making. It harnesses the power of a language model and a prompt to determine the next steps to achieve a specific objective. The inputs to an agent typically include: Tools: Descriptions of available tools (more on this later). User Input: The high-level objective or query from the user. Intermediate Steps: A history of (action, tool output) pairs executed to reach the current user input. Tools Tools are interfaces that an agent can use to interact with the world. They enable agents to perform various tasks, such as searching the web, running shell commands, or accessing external APIs. In LangChain, tools are essential for extending the capabilities of agents and enabling them to accomplish diverse tasks.\n",
      "To use tools in LangChain, you can load them using the following snippet: Some tools may require a base Language Model (LLM) to initialize. In such cases, you can pass an LLM as well: This setup allows you to access a variety of tools and integrate them into your agent's workflows. The complete list of tools with usage documentation is here. Let us look at some examples of Tools. DuckDuckGo The DuckDuckGo tool enables you to perform web searches using its search engine. Here's how to use it: DataForSeo The DataForSeo toolkit allows you to obtain search engine results using the DataForSeo API. To use this toolkit, you'll need to set up your API credentials. Here's how to configure the credentials: Once your credentials are set, you can create a DataForSeoAPIWrapper tool to access the API: The DataForSeoAPIWrapper tool retrieves search engine results from various sources. You can customize the type of results and fields returned in the JSON response. For example, you can specify the result types, fields, and set a maximum count for the number of top results to return: This example customizes the JSON response by specifying result types, fields, and limiting the number of results. You can also specify the location and language for your search results by passing additional parameters to the API wrapper: By providing location and language parameters, you can tailor your search results to specific regions and languages.\n",
      "You have the flexibility to choose the search engine you want to use. Simply specify the desired search engine: In this example, the search is customized to use Bing as the search engine. The API wrapper also allows you to specify the type of search you want to perform. For instance, you can perform a maps search: This customizes the search to retrieve maps-related information. Shell (bash) The Shell toolkit provides agents with access to the shell environment, allowing them to execute shell commands. This feature is powerful but should be used with caution, especially in sandboxed environments. Here's how you can use the Shell tool: In this example, the Shell tool runs two shell commands: echoing \"Hello World!\" and displaying the current time. You can provide the Shell tool to an agent to perform more complex tasks. Here's an example of an agent fetching links from a web page using the Shell tool: In this scenario, the agent uses the Shell tool to execute a sequence of commands to fetch, filter, and sort URLs from a web page. The examples provided demonstrate some of the tools available in LangChain. These tools ultimately extend the capabilities of agents (explored in next subsection) and empower them to perform various tasks efficiently. Depending on your requirements, you can choose the tools and toolkits that best suit your project's needs and integrate them into your agent's workflows. Back to Agents Let's move on to agents now.\n",
      "The AgentExecutor is the runtime environment for an agent. It is responsible for calling the agent, executing the actions it selects, passing the action outputs back to the agent, and repeating the process until the agent finishes. In pseudocode, the AgentExecutor might look something like this: The AgentExecutor handles various complexities, such as dealing with cases where the agent selects a non-existent tool, handling tool errors, managing agent-produced outputs, and providing logging and observability at all levels. While the AgentExecutor class is the primary agent runtime in LangChain, there are other, more experimental runtimes supported, including: Plan-and-execute Agent Baby AGI Auto GPT To gain a better understanding of the agent framework, let's build a basic agent from scratch, and then move on to explore pre-built agents. Before we dive into building the agent, it's essential to revisit some key terminology and schema: AgentAction: This is a data class representing the action an agent should take. It consists of a tool property (the name of the tool to invoke) and a tool_input property (the input for that tool). AgentFinish: This data class indicates that the agent has finished its task and should return a response to the user. It typically includes a dictionary of return values, often with a key \"output\" containing the response text. Intermediate Steps: These are the records of previous agent actions and corresponding outputs.\n",
      "They are crucial for passing context to future iterations of the agent. In our example, we will use OpenAI Function Calling to create our agent. This approach is reliable for agent creation. We'll start by creating a simple tool that calculates the length of a word. This tool is useful because language models can sometimes make mistakes due to tokenization when counting word lengths. First, let's load the language model we'll use to control the agent: Let's test the model with a word length calculation: The response should indicate the number of letters in the word \"educa.\" Next, we'll define a simple Python function to calculate the length of a word: We've created a tool named get_word_length that takes a word as input and returns its length. Now, let's create the prompt for the agent. The prompt instructs the agent on how to reason and format the output. In our case, we're using OpenAI Function Calling, which requires minimal instructions. We'll define the prompt with placeholders for user input and agent scratchpad: Now, how does the agent know which tools it can use? We're relying on OpenAI function calling language models, which require functions to be passed separately. To provide our tools to the agent, we'll format them as OpenAI function calls: Now, we can create the agent by defining input mappings and connecting the components: This is LCEL language. We will discuss this later in detail.\n",
      "We've created our agent, which understands user input, uses available tools, and formats output. Now, let's interact with it: The agent should respond with an AgentAction, indicating the next action to take. We've created the agent, but now we need to write a runtime for it. The simplest runtime is one that continuously calls the agent, executes actions, and repeats until the agent finishes. Here's an example: In this loop, we repeatedly call the agent, execute actions, and update the intermediate steps until the agent finishes. We also handle tool interactions within the loop. To simplify this process, LangChain provides the AgentExecutor class, which encapsulates agent execution and offers error handling, early stopping, tracing, and other improvements. Let's use AgentExecutor to interact with the agent: AgentExecutor simplifies the execution process and provides a convenient way to interact with the agent. Memory is also discussed in detail later. The agent we've created so far is stateless, meaning it doesn't remember previous interactions. To enable follow-up questions and conversations, we need to add memory to the agent. This involves two steps: Add a memory variable in the prompt to store chat history. Keep track of the chat history during interactions.\n",
      "Let's start by adding a memory placeholder in the prompt: Now, create a list to track the chat history: In the agent creation step, we'll include the memory as well: Now, when running the agent, make sure to update the chat history: This enables the agent to maintain a conversation history and answer follow-up questions based on previous interactions. Congratulations! You've successfully created and executed your first end-to-end agent in LangChain. To delve deeper into LangChain's capabilities, you can explore: Different agent types supported. Pre-built Agents How to work with tools and tool integrations. Agent Types LangChain offers various agent types, each suited for specific use cases. Here are some of the available agents: Zero-shot ReAct: This agent uses the ReAct framework to choose tools based solely on their descriptions. It requires descriptions for each tool and is highly versatile. Structured input ReAct: This agent handles multi-input tools and is suitable for complex tasks like navigating a web browser. It uses a tools' argument schema for structured input. OpenAI Functions: Specifically designed for models fine-tuned for function calling, this agent is compatible with models like gpt-3.5-turbo-0613 and gpt-4-0613. We used this to create our first agent above. Conversational: Designed for conversational settings, this agent uses ReAct for tool selection and utilizes memory to remember previous interactions.\n",
      "Self-ask with search: This agent relies on a single tool, \"Intermediate Answer,\" which looks up factual answers to questions. It's equivalent to the original self-ask with search paper. ReAct document store: This agent interacts with a document store using the ReAct framework. It requires \"Search\" and \"Lookup\" tools and is similar to the original ReAct paper's Wikipedia example. Explore these agent types to find the one that best suits your needs in LangChain. These agents allow you to bind set of tools within them to handle actions and generate responses. Learn more on how to build your own agent with tools here. Prebuilt Agents Let's continue our exploration of agents, focusing on prebuilt agents available in LangChain. Gmail LangChain offers a Gmail toolkit that allows you to connect your LangChain email to the Gmail API. To get started, you'll need to set up your credentials, which are explained in the Gmail API documentation. Once you have downloaded the credentials.json file, you can proceed with using the Gmail API. Additionally, you'll need to install some required libraries using the following commands: You can create the Gmail toolkit as follows: You can also customize authentication as per your needs. Behind the scenes, a googleapi resource is created using the following methods: The toolkit offers various tools that can be used within an agent, including: GmailCreateDraft: Create a draft email with specified message fields. GmailSendMessage: Send email messages.\n",
      "GmailSearch: Search for email messages or threads. GmailGetMessage: Fetch an email by message ID. GmailGetThread: Search for email messages. To use these tools within an agent, you can initialize the agent as follows: Here are a couple of examples of how these tools can be used: Create a Gmail draft for editing: Search for the latest email in your drafts: These examples demonstrate the capabilities of LangChain's Gmail toolkit within an agent, enabling you to interact with Gmail programmatically. SQL Database Agent This section provides an overview of an agent designed to interact with SQL databases, particularly the Chinook database. This agent can answer general questions about a database and recover from errors. Please note that it is still in active development, and not all answers may be correct. Be cautious when running it on sensitive data, as it may perform DML statements on your database. To use this agent, you can initialize it as follows: This agent can be initialized using the ZERO_SHOT_REACT_DESCRIPTION agent type. It is designed to answer questions and provide descriptions. Alternatively, you can initialize the agent using the OPENAI_FUNCTIONS agent type with OpenAI's GPT-3.5-turbo model, which we used in our earlier client. Disclaimer The query chain may generate insert/update/delete queries. Be cautious, and use a custom prompt or create a SQL user without write permissions if needed.\n",
      "Be aware that running certain queries, such as \"run the biggest query possible,\" could overload your SQL database, especially if it contains millions of rows. Data warehouse-oriented databases often support user-level quotas to limit resource usage. You can ask the agent to describe a table, such as the \"playlisttrack\" table. Here's an example of how to do it: The agent will provide information about the table's schema and sample rows. If you mistakenly ask about a table that doesn't exist, the agent can recover and provide information about the closest matching table. For example: The agent will find the nearest matching table and provide information about it. You can also ask the agent to run queries on the database. For instance: The agent will execute the query and provide the result, such as the country with the highest total sales. To get the total number of tracks in each playlist, you can use the following query: The agent will return the playlist names along with the corresponding total track counts. In cases where the agent encounters errors, it can recover and provide accurate responses. For instance: Even after encountering an initial error, the agent will adjust and provide the correct answer, which, in this case, is the top 3 best-selling artists. Pandas DataFrame Agent This section introduces an agent designed to interact with Pandas DataFrames for question-answering purposes.\n",
      "Please note that this agent utilizes the Python agent under the hood to execute Python code generated by a language model (LLM). Exercise caution when using this agent to prevent potential harm from malicious Python code generated by the LLM. You can initialize the Pandas DataFrame agent as follows: You can ask the agent to count the number of rows in the DataFrame: The agent will execute the code df.shape[0] and provide the answer, such as \"There are 891 rows in the dataframe.\" You can also ask the agent to filter rows based on specific criteria, such as finding the number of people with more than 3 siblings: The agent will execute the code df[df['SibSp'] > 3].shape[0] and provide the answer, such as \"30 people have more than 3 siblings.\" If you want to calculate the square root of the average age, you can ask the agent: The agent will calculate the average age using df['Age'].mean() and then calculate the square root using math.sqrt(). It will provide the answer, such as \"The square root of the average age is 5.449689683556195.\" Let's create a copy of the DataFrame, and missing age values are filled with the mean age: Then, you can initialize the agent with both DataFrames and ask it a question: The agent will compare the age columns in both DataFrames and provide the answer, such as \"177 rows in the age column are different.\" Jira Toolkit This section explains how to use the Jira toolkit, which allows agents to interact with a Jira instance.\n",
      "You can perform various actions such as searching for issues and creating issues using this toolkit. It utilizes the atlassian-python-api library. To use this toolkit, you need to set environment variables for your Jira instance, including JIRA_API_TOKEN, JIRA_USERNAME, and JIRA_INSTANCE_URL. Additionally, you may need to set your OpenAI API key as an environment variable. To get started, install the atlassian-python-api library and set the required environment variables: You can instruct the agent to create a new issue in a specific project with a summary and description: The agent will execute the necessary actions to create the issue and provide a response, such as \"A new issue has been created in project PW with the summary 'Make more fried rice' and description 'Reminder to make more fried rice'.\" This allows you to interact with your Jira instance using natural language instructions and the Jira toolkit. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams. Get Started Module IV : Chains LangChain is a tool designed for utilizing Large Language Models (LLMs) in complex applications. It provides frameworks for creating chains of components, including LLMs and other types of components. Two primary frameworks The LangChain Expression Language (LCEL) Legacy Chain interface The LangChain Expression Language (LCEL) is a syntax that allows for intuitive composition of chains.\n",
      "It supports advanced features like streaming, asynchronous calls, batching, parallelization, retries, fallbacks, and tracing. For example, you can compose a prompt, model, and output parser in LCEL as shown in the following code: Alternatively, the LLMChain is an option similar to LCEL for composing components. The LLMChain example is as follows: Chains in LangChain can also be stateful by incorporating a Memory object. This allows for data persistence across calls, as shown in this example: LangChain also supports integration with OpenAI's function-calling APIs, which is useful for obtaining structured outputs and executing functions within a chain. For getting structured outputs, you can specify them using Pydantic classes or JsonSchema, as illustrated below: For structured outputs, a legacy approach using LLMChain is also available: LangChain leverages OpenAI functions to create various specific chains for different purposes. These include chains for extraction, tagging, OpenAPI, and QA with citations. In the context of extraction, the process is similar to the structured output chain but focuses on information or entity extraction. For tagging, the idea is to label a document with classes such as sentiment, language, style, covered topics, or political tendency. An example of how tagging works in LangChain can be demonstrated with a Python code.\n",
      "The process begins with installing the necessary packages and setting up the environment: The schema for tagging is defined, specifying the properties and their expected types: Examples of running the tagging chain with different inputs show the model's ability to interpret sentiments, languages, and aggressiveness: For finer control, the schema can be defined more specifically, including possible values, descriptions, and required properties. An example of this enhanced control is shown below: Pydantic schemas can also be used for defining tagging criteria, providing a Pythonic way to specify required properties and types: Additionally, LangChain's metadata tagger document transformer can be used to extract metadata from LangChain Documents, offering similar functionality to the tagging chain but applied to a LangChain Document. Citing retrieval sources is another feature of LangChain, using OpenAI functions to extract citations from text. This is demonstrated in the following code: In LangChain, chaining in Large Language Model (LLM) applications typically involves combining a prompt template with an LLM and optionally an output parser. The recommended way to do this is through the LangChain Expression Language (LCEL), although the legacy LLMChain approach is also supported. Using LCEL, the BasePromptTemplate, BaseLanguageModel, and BaseOutputParser all implement the Runnable interface and can be easily piped into one another.\n",
      "Here's an example demonstrating this: Routing in LangChain allows for creating non-deterministic chains where the output of a previous step determines the next step. This helps in structuring and maintaining consistency in interactions with LLMs. For instance, if you have two templates optimized for different types of questions, you can choose the template based on user input. Here's how you can achieve this using LCEL with a RunnableBranch, which is initialized with a list of (condition, runnable) pairs and a default runnable: The final chain is then constructed using various components, such as a topic classifier, prompt branch, and an output parser, to determine the flow based on the topic of the input: This approach exemplifies the flexibility and power of LangChain in handling complex queries and routing them appropriately based on the input. In the realm of language models, a common practice is to follow up an initial call with a series of subsequent calls, using the output of one call as input for the next. This sequential approach is especially beneficial when you want to build on the information generated in previous interactions. While the LangChain Expression Language (LCEL) is the recommended method for creating these sequences, the SequentialChain method is still documented for its backward compatibility. To illustrate this, let's consider a scenario where we first generate a play synopsis and then a review based on that synopsis.\n",
      "Using Python's langchain.prompts, we create two PromptTemplate instances: one for the synopsis and another for the review. Here's the code to set up these templates: In the LCEL approach, we chain these prompts with ChatOpenAI and StrOutputParser to create a sequence that first generates a synopsis and then a review. The code snippet is as follows: If we need both the synopsis and the review, we can use RunnablePassthrough to create a separate chain for each and then combine them: For scenarios involving more complex sequences, the SequentialChain method comes into play. This allows for multiple inputs and outputs. Consider a case where we need a synopsis based on a play's title and era. Here's how we might set it up: In scenarios where you want to maintain context throughout a chain or for a later part of the chain, SimpleMemory can be used. This is particularly useful for managing complex input/output relationships. For instance, in a scenario where we want to generate social media posts based on a play's title, era, synopsis, and review, SimpleMemory can help manage these variables: In addition to sequential chains, there are specialized chains for working with documents. Each of these chains serves a different purpose, from combining documents to refining answers based on iterative document analysis, to mapping and reducing document content for summarization or re-ranking based on scored responses.\n",
      "These chains can be recreated with LCEL for additional flexibility and customization. StuffDocumentsChain combines a list of documents into a single prompt passed to an LLM. RefineDocumentsChain updates its answer iteratively for each document, suitable for tasks where documents exceed the model's context capacity. MapReduceDocumentsChain applies a chain to each document individually and then combines the results. MapRerankDocumentsChain scores each document-based response and selects the highest-scoring one. Here's an example of how you might set up a MapReduceDocumentsChain using LCEL: This configuration allows for a detailed and comprehensive analysis of document content, leveraging the strengths of LCEL and the underlying language model. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams. Get Started Module V : Memory In LangChain, memory is a fundamental aspect of conversational interfaces, allowing systems to reference past interactions. This is achieved through storing and querying information, with two primary actions: reading and writing. The memory system interacts with a chain twice during a run, augmenting user inputs and storing the inputs and outputs for future reference. Building Memory into a System Storing Chat Messages: The LangChain memory module integrates various methods to store chat messages, ranging from in-memory lists to databases.\n",
      "This ensures that all chat interactions are recorded for future reference. Querying Chat Messages: Beyond storing chat messages, LangChain employs data structures and algorithms to create a useful view of these messages. Simple memory systems might return recent messages, while more advanced systems could summarize past interactions or focus on entities mentioned in the current interaction. To demonstrate the use of memory in LangChain, consider the ConversationBufferMemory class, a simple memory form that stores chat messages in a buffer. Here's an example: When integrating memory into a chain, it's crucial to understand the variables returned from memory and how they're used in the chain. For instance, the load_memory_variables method helps align the variables read from memory with the chain's expectations. End-to-End Example with LangChain Consider using ConversationBufferMemory in an LLMChain. The chain, combined with an appropriate prompt template and the memory, provides a seamless conversational experience. Here's a simplified example: This example illustrates how LangChain's memory system integrates with its chains to provide a coherent and contextually aware conversational experience. Memory Types in Langchain Langchain offers various memory types that can be utilized to enhance interactions with the AI models. Each memory type has its own parameters and return types, making them suitable for different scenarios.\n",
      "Let's explore some of the memory types available in Langchain along with code examples. 1. Conversation Buffer Memory This memory type allows you to store and extract messages from conversations. You can extract the history as a string or as a list of messages. You can also use Conversation Buffer Memory in a chain for chat-like interactions. 2. Conversation Buffer Window Memory This memory type keeps a list of recent interactions and uses the last K interactions, preventing the buffer from getting too large. Like Conversation Buffer Memory, you can also use this memory type in a chain for chat-like interactions. 3. Conversation Entity Memory This memory type remembers facts about specific entities in a conversation and extracts information using an LLM. 4. Conversation Knowledge Graph Memory This memory type uses a knowledge graph to recreate memory. You can extract current entities and knowledge triplets from messages. You can also use this memory type in a chain for conversation-based knowledge retrieval. 5. Conversation Summary Memory This memory type creates a summary of the conversation over time, useful for condensing information from longer conversations. 6. Conversation Summary Buffer Memory This memory type combines the conversation summary and buffer, maintaining a balance between recent interactions and a summary. It uses token length to determine when to flush interactions. You can use these memory types to enhance your interactions with AI models in Langchain.\n",
      "Each memory type serves a specific purpose and can be selected based on your requirements. 7. Conversation Token Buffer Memory ConversationTokenBufferMemory is another memory type that keeps a buffer of recent interactions in memory. Unlike the previous memory types that focus on the number of interactions, this one uses token length to determine when to flush interactions. Using memory with LLM: In this example, the memory is set to limit interactions based on token length rather than the number of interactions. You can also get the history as a list of messages when using this memory type. Using in a chain: You can use ConversationTokenBufferMemory in a chain to enhance interactions with the AI model. In this example, ConversationTokenBufferMemory is used in a ConversationChain to manage the conversation and limit interactions based on token length. 8. VectorStoreRetrieverMemory VectorStoreRetrieverMemory stores memories in a vector store and queries the top-K most \"salient\" documents every time it is called. This memory type doesn't explicitly track the order of interactions but uses vector retrieval to fetch relevant memories. In this example, VectorStoreRetrieverMemory is used to store and retrieve relevant information from a conversation based on vector retrieval. You can also use VectorStoreRetrieverMemory in a chain for conversation-based knowledge retrieval, as shown in the previous examples.\n",
      "These different memory types in Langchain provide various ways to manage and retrieve information from conversations, enhancing the capabilities of AI models in understanding and responding to user queries and context. Each memory type can be selected based on the specific requirements of your application. Now we'll learn how to use memory with an LLMChain. Memory in an LLMChain allows the model to remember previous interactions and context to provide more coherent and context-aware responses. To set up memory in an LLMChain, you need to create a memory class, such as ConversationBufferMemory. Here's how you can set it up: In this example, the ConversationBufferMemory is used to store the conversation history. The memory_key parameter specifies the key used to store the conversation history. If you are using a chat model instead of a completion-style model, you can structure your prompts differently to better utilize the memory. Here's an example of how to set up a chat model-based LLMChain with memory: In this example, the ChatPromptTemplate is used to structure the prompt, and the ConversationBufferMemory is used to store and retrieve the conversation history. This approach is particularly useful for chat-style conversations where context and history play a crucial role. Memory can also be added to a chain with multiple inputs, such as a question/answering chain.\n",
      "Here's an example of how to set up memory in a question/answering chain: In this example, a question is answered using a document split into smaller chunks. The ConversationBufferMemory is used to store and retrieve the conversation history, allowing the model to provide context-aware answers. Adding memory to an agent allows it to remember and use previous interactions to answer questions and provide context-aware responses. Here's how you can set up memory in an agent: In this example, memory is added to an agent, allowing it to remember the previous conversation history and provide context-aware answers. This enables the agent to answer follow-up questions accurately based on the information stored in memory. LangChain Expression Language In the world of natural language processing and machine learning, composing complex chains of operations can be a daunting task. Fortunately, LangChain Expression Language (LCEL) comes to the rescue, providing a declarative and efficient way to build and deploy sophisticated language processing pipelines. LCEL is designed to simplify the process of composing chains, making it possible to go from prototyping to production with ease. In this blog, we'll explore what LCEL is and why you might want to use it, along with practical code examples to illustrate its capabilities. LCEL, or LangChain Expression Language, is a powerful tool for composing language processing chains.\n",
      "It was purpose-built to support the transition from prototyping to production seamlessly, without requiring extensive code changes. Whether you're building a simple \"prompt + LLM\" chain or a complex pipeline with hundreds of steps, LCEL has you covered. Here are some reasons to use LCEL in your language processing projects: Fast Token Streaming: LCEL delivers tokens from a Language Model to an output parser in real-time, improving responsiveness and efficiency. Versatile APIs: LCEL supports both synchronous and asynchronous APIs for prototyping and production use, handling multiple requests efficiently. Automatic Parallelization: LCEL optimizes parallel execution when possible, reducing latency in both sync and async interfaces. Reliable Configurations: Configure retries and fallbacks for enhanced chain reliability at scale, with streaming support in development. Stream Intermediate Results: Access intermediate results during processing for user updates or debugging purposes. Schema Generation: LCEL generates Pydantic and JSONSchema schemas for input and output validation. Comprehensive Tracing: LangSmith automatically traces all steps in complex chains for observability and debugging. Easy Deployment: Deploy LCEL-created chains effortlessly using LangServe. Now, let's dive into practical code examples that demonstrate the power of LCEL. We'll explore common tasks and scenarios where LCEL shines.\n",
      "Prompt + LLM The most fundamental composition involves combining a prompt and a language model to create a chain that takes user input, adds it to a prompt, passes it to a model, and returns the raw model output. Here's an example: In this example, the chain generates a joke about bears. You can attach stop sequences to your chain to control how it processes text. For example: This configuration stops text generation when a newline character is encountered. LCEL supports attaching function call information to your chain. Here's an example: This example attaches function call information to generate a joke. Prompt + LLM + OutputParser You can add an output parser to transform the raw model output into a more workable format. Here's how you can do it: The output is now in a string format, which is more convenient for downstream tasks. When specifying a function to return, you can parse it directly using LCEL. For example: This example parses the output of the \"joke\" function directly. These are just a few examples of how LCEL simplifies complex language processing tasks. Whether you're building chatbots, generating content, or performing complex text transformations, LCEL can streamline your workflow and make your code more maintainable. RAG (Retrieval-augmented Generation) LCEL can be used to create retrieval-augmented generation chains, which combine retrieval and language generation steps.\n",
      "Here's an example: In this example, the chain retrieves relevant information from the context and generates a response to the question. Conversational Retrieval Chain You can easily add conversation history to your chains. Here's an example of a conversational retrieval chain: In this example, the chain handles a follow-up question within a conversational context. With Memory and Returning Source Documents LCEL also supports memory and returning source documents. Here's how you can use memory in a chain: In this example, memory is used to store and retrieve conversation history and source documents. Multiple Chains You can string together multiple chains using Runnables. Here's an example: In this example, two chains are combined to generate information about a city and its country in a specified language. Branching and Merging LCEL allows you to split and merge chains using RunnableMaps. Here's an example of branching and merging: In this example, a branching and merging chain is used to generate an argument and evaluate its pros and cons before generating a final response. Writing Python Code with LCEL One of the powerful applications of LangChain Expression Language (LCEL) is writing Python code to solve user problems. Below is an example of how to use LCEL to write Python code: In this example, a user provides input, and LCEL generates Python code to solve the problem.\n",
      "The code is then executed using a Python REPL, and the resulting Python code is returned in Markdown format. Please note that using a Python REPL can execute arbitrary code, so use it with caution. Adding Memory to a Chain Memory is essential in many conversational AI applications. Here's how to add memory to an arbitrary chain: In this example, memory is used to store and retrieve conversation history, allowing the chatbot to maintain context and respond appropriately. Using External Tools with Runnables LCEL allows you to seamlessly integrate external tools with Runnables. Here's an example using the DuckDuckGo Search tool: In this example, LCEL integrates the DuckDuckGo Search tool into the chain, allowing it to generate a search query from user input and retrieve search results. LCEL's flexibility makes it easy to incorporate various external tools and services into your language processing pipelines, enhancing their capabilities and functionality. Adding Moderation to an LLM Application To ensure that your LLM application adheres to content policies and includes moderation safeguards, you can integrate moderation checks into your chain. Here's how to add moderation using LangChain: In this example, the OpenAIModerationChain is used to add moderation to the response generated by the LLM. The moderation chain checks the response for content that violates OpenAI's content policy. If any violations are found, it will flag the response accordingly.\n",
      "Routing by Semantic Similarity LCEL allows you to implement custom routing logic based on the semantic similarity of user input. Here's an example of how to dynamically determine the chain logic based on user input: In this example, the prompt_router function calculates the cosine similarity between user input and predefined prompt templates for physics and math questions. Based on the similarity score, the chain dynamically selects the most relevant prompt template, ensuring that the chatbot responds appropriately to the user's question. Using Agents and Runnables LangChain allows you to create agents by combining Runnables, prompts, models, and tools. Here's an example of building an agent and using it: In this example, an agent is created by combining a model, tools, a prompt, and a custom logic for intermediate steps and tool conversion. The agent is then executed, providing a response to the user's query. Querying a SQL Database You can use LangChain to query a SQL database and generate SQL queries based on user questions. Here's an example: In this example, LangChain is used to generate SQL queries based on user questions and retrieve responses from a SQL database. The prompts and responses are formatted to provide natural language interactions with the database. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams.\n",
      "Get Started Deploy with LangServe LangServe helps developers deploy LangChain runnables and chains as a REST API. This library is integrated with FastAPI and uses pydantic for data validation. Additionally, it provides a client that can be used to call into runnables deployed on a server, and a JavaScript client is available in LangChainJS. Features Input and Output schemas are automatically inferred from your LangChain object and enforced on every API call, with rich error messages. An API docs page with JSONSchema and Swagger is available. Efficient /invoke, /batch, and /stream endpoints with support for many concurrent requests on a single server. /stream_log endpoint for streaming all (or some) intermediate steps from your chain/agent. Playground page at /playground with streaming output and intermediate steps. Built-in (optional) tracing to LangSmith; just add your API key (see Instructions). All built with battle-tested open-source Python libraries like FastAPI, Pydantic, uvloop, and asyncio. Limitations Client callbacks are not yet supported for events that originate on the server. OpenAPI docs will not be generated when using Pydantic V2. FastAPI does not support mixing pydantic v1 and v2 namespaces. See the section below for more details. Use the LangChain CLI to bootstrap a LangServe project quickly. To use the langchain CLI, make sure that you have a recent version of langchain-cli installed. You can install it with pip install -U langchain-cli.\n",
      "Get your LangServe instance started quickly with LangChain Templates. For more examples, see the templates index or the examples directory. Here's a server that deploys an OpenAI chat model, an Anthropic chat model, and a chain that uses the Anthropic model to tell a joke about a topic. Once you've deployed the server above, you can view the generated OpenAPI docs using: Make sure to add the /docs suffix. In TypeScript (requires LangChain.js version 0.0.166 or later): Python using requests: You can also use curl: The following code: adds of these endpoints to the server: POST /my_runnable/invoke - invoke the runnable on a single input POST /my_runnable/batch - invoke the runnable on a batch of inputs POST /my_runnable/stream - invoke on a single input and stream the output POST /my_runnable/stream_log - invoke on a single input and stream the output, including output of intermediate steps as it's generated GET /my_runnable/input_schema - json schema for input to the runnable GET /my_runnable/output_schema - json schema for output of the runnable GET /my_runnable/config_schema - json schema for config of the runnable You can find a playground page for your runnable at /my_runnable/playground. This exposes a simple UI to configure and invoke your runnable with streaming output and intermediate steps. For both client and server: or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code.\n",
      "If you need to add authentication to your server, please reference FastAPI's security documentation and middleware documentation. You can deploy to GCP Cloud Run using the following command: LangServe provides support for Pydantic 2 with some limitations. OpenAPI docs will not be generated for invoke/batch/stream/stream_log when using Pydantic V2. Fast API does not support mixing pydantic v1 and v2 namespaces. LangChain uses the v1 namespace in Pydantic v2. Please read the following guidelines to ensure compatibility with LangChain. Except for these limitations, we expect the API endpoints, the playground, and any other features to work as expected. LLM applications often deal with files. There are different architectures that can be made to implement file processing; at a high level: The file may be uploaded to the server via a dedicated endpoint and processed using a separate endpoint. The file may be uploaded by either value (bytes of file) or reference (e.g., s3 url to file content). The processing endpoint may be blocking or non-blocking. If significant processing is required, the processing may be offloaded to a dedicated process pool. You should determine what is the appropriate architecture for your application. Currently, to upload files by value to a runnable, use base64 encoding for the file (multipart/form-data is not supported yet). Here's an example that shows how to use base64 encoding to send a file to a remote runnable.\n",
      "Remember, you can always upload files by reference (e.g., s3 url) or upload them as multipart/form-data to a dedicated endpoint. Input and Output types are defined on all runnables. You can access them via the input_schema and output_schema properties. LangServe uses these types for validation and documentation. If you want to override the default inferred types, you can use the with_types method. Here's a toy example to illustrate the idea: Inherit from CustomUserType if you want the data to deserialize into a pydantic model rather than the equivalent dict representation. At the moment, this type only works server-side and is used to specify desired decoding behavior. If inheriting from this type, the server will keep the decoded type as a pydantic model instead of converting it into a dict. The playground allows you to define custom widgets for your runnable from the backend. A widget is specified at the field level and shipped as part of the JSON schema of the input type. A widget must contain a key called type with the value being one of a well-known list of widgets. Other widget keys will be associated with values that describe paths in a JSON object. General schema: Allows the creation of a file upload input in the UI playground for files that are uploaded as base64 encoded strings. Here's the full example. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams.\n",
      "Get Started Introduction to LangSmith LangChain makes it easy to prototype LLM applications and Agents. However, delivering LLM applications to production can be deceptively difficult. You will likely have to heavily customize and iterate on your prompts, chains, and other components to create a high-quality product. To aid in this process, LangSmith was introduced, a unified platform for debugging, testing, and monitoring your LLM applications. When might this come in handy? You may find it useful when you want to quickly debug a new chain, agent, or set of tools, visualize how components (chains, llms, retrievers, etc.) relate and are used, evaluate different prompts and LLMs for a single component, run a given chain several times over a dataset to ensure it consistently meets a quality bar, or capture usage traces and use LLMs or analytics pipelines to generate insights. Prerequisites: Create a LangSmith account and create an API key (see bottom left corner). Familiarize yourself with the platform by looking through the docs. Now, let's get started! First, configure your environment variables to tell LangChain to log traces. This is done by setting the LANGCHAIN_TRACING_V2 environment variable to true. You can tell LangChain which project to log to by setting the LANGCHAIN_PROJECT environment variable (if this isn't set, runs will be logged to the default project). This will automatically create the project for you if it doesn't exist.\n",
      "You must also set the LANGCHAIN_ENDPOINT and LANGCHAIN_API_KEY environment variables. NOTE: You can also use a context manager in python to log traces using: However, in this example, we will use environment variables. Create the LangSmith client to interact with the API: Create a LangChain component and log runs to the platform. In this example, we will create a ReAct-style agent with access to a general search tool (DuckDuckGo). The agent's prompt can be viewed in the Hub here: We are running the agent concurrently on multiple inputs to reduce latency. Runs get logged to LangSmith in the background, so execution latency is unaffected: Assuming you've successfully set up your environment, your agent traces should show up in the Projects section in the app. Congrats! It looks like the agent isn't effectively using the tools though. Let's evaluate this so we have a baseline. In addition to logging runs, LangSmith also allows you to test and evaluate your LLM applications. In this section, you will leverage LangSmith to create a benchmark dataset and run AI-assisted evaluators on an agent. You will do so in a few steps: Create a LangSmith dataset: Below, we use the LangSmith client to create a dataset from the input questions from above and a list labels. You will use these later to measure performance for a new agent.\n",
      "A dataset is a collection of examples, which are nothing more than input-output pairs you can use as test cases to your application: Initialize a new agent to benchmark: LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn't shared between dataset runs, we will pass in a chain_factory ( aka a constructor) function to initialize for each call: Configure evaluation: Manually comparing the results of chains in the UI is effective, but it can be time-consuming. It can be helpful to use automated metrics and AI-assisted feedback to evaluate your component's performance: Run the agent and evaluators: Use the run_on_dataset (or asynchronous arun_on_dataset) function to evaluate your model. This will: Fetch example rows from the specified dataset. Run your agent (or any custom function) on each example. Apply evaluators to the resulting run traces and corresponding reference examples to generate automated feedback. The results will be visible in the LangSmith app: Now that we have our test run results, we can make changes to our agent and benchmark them. Let's try this again with a different prompt and see the results: LangSmith lets you export data to common formats such as CSV or JSONL directly in the web app. You can also use the client to fetch runs for further analysis, to store in your own database, or to share with others.\n",
      "Let's fetch the run traces from the evaluation run: This was a quick guide to get started, but there are many more ways to use LangSmith to speed up your developer flow and produce better results. For more information on how you can get the most out of LangSmith, check out LangSmith documentation. Level up with Nanonets While LangChain is a valuable tool for integrating language models (LLMs) with your applications and workflows, it may face limitations when it comes to enterprise use cases. Data Connectivity: Limited support for various business applications and data formats. Task Automation: Challenges in automating tasks across different applications. Data Synchronization: Inadequate real-time data update capabilities. Complex Configuration: Difficult and time-consuming setup processes. Format Adherence: No assurance that the model will follow specified formats accurately. Invalid Outputs: Risks of generating incorrect tool names or inputs. Non-Termination Issues: Problems with processes not ending appropriately. Customization Difficulty: Challenges in modifying or creating custom agents. Performance Issues: Slowness, especially when reprocessing prompts. Security Vulnerabilities: Risks of data loss or sensitive information exposure due to prompt injection attacks. Enter Nanonets Workflows!\n",
      "Harnessing the Power of Workflow Automation: A Game-Changer for Modern Businesses In today's fast-paced business environment, workflow automation stands out as a crucial innovation, offering a competitive edge to companies of all sizes. The integration of automated workflows into daily business operations is not just a trend; it's a strategic necessity. In addition to this, the advent of LLMs has opened even more opportunities for automation of manual tasks and processes. Welcome to Nanonets Workflow Automation, where AI-driven technology empowers you and your team to automate manual tasks and construct efficient workflows in minutes. Utilize natural language to effortlessly create and manage workflows that seamlessly integrate with all your documents, apps, and databases. Our platform offers not only seamless app integrations for unified workflows but also the ability to build and utilize custom Large Language Models Apps for sophisticated text writing and response posting within your apps. All the while ensuring data security remains our top priority, with strict adherence to GDPR, SOC 2, and HIPAA compliance standards​. To better understand the practical applications of Nanonets workflow automation, let's delve into some real-world examples. Automated Customer Support and Engagement Process Ticket Creation – Zendesk: The workflow is triggered when a customer submits a new support ticket in Zendesk, indicating they need assistance with a product or service.\n",
      "Ticket Update – Zendesk: After the ticket is created, an automated update is immediately logged in Zendesk to indicate that the ticket has been received and is being processed, providing the customer with a ticket number for reference. Information Retrieval – Nanonets Browsing: Concurrently, the Nanonets Browsing feature searches through all the knowledge base pages to find relevant information and possible solutions related to the customer's issue. Customer History Access – HubSpot: Simultaneously, HubSpot is queried to retrieve the customer's previous interaction records, purchase history, and any past tickets to provide context to the support team. Ticket Processing – Nanonets AI: With the relevant information and customer history at hand, Nanonets AI processes the ticket, categorizing the issue and suggesting potential solutions based on similar past cases. Notification – Slack: Finally, the responsible support team or individual is notified through Slack with a message containing the ticket details, customer history, and suggested solutions, prompting a swift and informed response. Automated Issue Resolution Process Initial Trigger – Slack Message: The workflow begins when a customer service representative receives a new message in a dedicated channel on Slack, signaling a customer issue that needs to be addressed.\n",
      "Classification – Nanonets AI: Once the message is detected, Nanonets AI steps in to classify the message based on its content and past classification data (from Airtable records). Using LLMs, it classifies it as a bug along with determining urgency. Record Creation – Airtable: After classification, the workflow automatically creates a new record in Airtable, a cloud collaboration service. This record includes all relevant details from the customer's message, such as customer ID, issue category, and urgency level. Team Assignment – Airtable: With the record created, the Airtable system then assigns a team to handle the issue. Based on the classification done by Nanonets AI, the system selects the most appropriate team – tech support, billing, customer success, etc. – to take over the issue. Notification – Slack: Finally, the assigned team is notified through Slack. An automated message is sent to the team's channel, alerting them of the new issue, providing a direct link to the Airtable record, and prompting a timely response. Automated Meeting Scheduling Process Initial Contact – LinkedIn: The workflow is initiated when a professional connection sends a new message on LinkedIn expressing interest in scheduling a meeting. An LLM parses incoming messages and triggers the workflow if it deems the message as a request for a meeting from a potential job candidate.\n",
      "Document Retrieval – Google Drive: Following the initial contact, the workflow automation system retrieves a pre-prepared document from Google Drive that contains information about the meeting agenda, company overview, or any relevant briefing materials. Scheduling – Google Calendar: Next, the system interacts with Google Calendar to get available times for the meeting. It checks the calendar for open slots that align with business hours (based on the location parsed from LinkedIn profile) and previously set preferences for meetings. Confirmation Message as Reply – LinkedIn: Once a suitable time slot is found, the workflow automation system sends a message back through LinkedIn. This message includes the proposed time for the meeting, access to the document retrieved from Google Drive, and a request for confirmation or alternative suggestions. Invoice Processing in Accounts Payable Receipt of Invoice - Gmail: An invoice is received via email or uploaded to the system. Data Extraction - Nanonets OCR: The system automatically extracts relevant data (like vendor details, amounts, due dates). Data Verification - Quickbooks: The Nanonets workflow verifies the extracted data against purchase orders and receipts. Approval Routing - Slack: The invoice is routed to the appropriate manager for approval based on predefined thresholds and rules. Payment Processing - Brex: Once approved, the system schedules the payment according to the vendor's terms and updates the finance records.\n",
      "Archiving - Quickbooks: The completed transaction is archived for future reference and audit trails. Internal Knowledge Base Assistance Initial Inquiry – Slack: A team member, Smith, inquires in the #chat-with-data Slack channel about customers experiencing issues with QuickBooks integration. Automated Data Aggregation - Nanonets Knowledge Base:Ticket Lookup - Zendesk: The Zendesk app in Slack automatically provides a summary of today's tickets, indicating that there are issues with exporting invoice data to QuickBooks for some customers.Slack Search - Slack: Simultaneously, the Slack app notifies the channel that team members Patrick and Rachel are actively discussing the resolution of the QuickBooks export bug in another channel, with a fix scheduled to go live at 4 PM.Ticket Tracking – JIRA: The JIRA app updates the channel about a ticket created by Emily titled \"QuickBooks export failing for QB Desktop integrations,\" which helps track the status and resolution progress of the issue.Reference Documentation – Google Drive: The Drive app mentions the existence of a runbook for fixing bugs related to QuickBooks integrations, which can be referenced to understand the steps for troubleshooting and resolution.Ongoing Communication and Resolution Confirmation – Slack: As the conversation progresses, the Slack channel serves as a real-time forum for discussing updates, sharing findings from the runbook, and confirming the deployment of the bug fix.\n",
      "Team members use the channel to collaborate, share insights, and ask follow-up questions to ensure a comprehensive understanding of the issue and its resolution.Resolution Documentation and Knowledge Sharing: After the fix is implemented, team members update the internal documentation in Google Drive with new findings and any additional steps taken to resolve the issue. A summary of the incident, resolution, and any lessons learned are already shared in the Slack channel. Thus, the team’s internal knowledge base is automatically enhanced for future use. The Future of Business Efficiency Nanonets Workflows is a secure, multi-purpose workflow automation platform that automates your manual tasks and workflows. It offers an easy-to-use user interface, making it accessible for both individuals and organizations. To get started, you can schedule a call with one of our AI experts, who can provide a personalized demo and trial of Nanonets Workflows tailored to your specific use case. Once set up, you can use natural language to design and execute complex applications and workflows powered by LLMs, integrating seamlessly with your apps and data. Supercharge your teams with Nanonets Workflows allowing them to focus on what truly matters. Automate manual tasks and workflows with our AI-driven workflow builder, designed by Nanonets for you and your teams. Get Started\n",
      "\n",
      "Source: quickaitutorial.com_langgraph-create-your-hyper-ai-agent_.txt\n",
      "Content: Home News Technology All Data ScienceMachine LearningProgramming Llmlingua + LlamaIndex + RAG = Cheaper Chatbot AutoGen + LangChian + SQLite + Function Schema = Super AI Chabot Microsoft PHI-2 + Huggine Face + Langchain = Super Tiny Chatbot TaskWeaver + Planner + Plugin = Super AI Agent OpenHermes 2.5 Vs GPT-4 Vs LLama2 = The Winner AutoGen + LangChian + RAG + Function Call = Super AI Chabot Why OpenChat Model is So Much Better Than ChatGPT? How To Build a LLava chatbot How Powerful Step-Back Prompting Transforms LLM Performance Money Ai Automation Why OpenChat Model is So Much Better Than ChatGPT? No Result Home News Technology All Data ScienceMachine LearningProgramming Llmlingua + LlamaIndex + RAG = Cheaper Chatbot AutoGen + LangChian + SQLite + Function Schema = Super AI Chabot Microsoft PHI-2 + Huggine Face + Langchain = Super Tiny Chatbot TaskWeaver + Planner + Plugin = Super AI Agent OpenHermes 2.5 Vs GPT-4 Vs LLama2 = The Winner AutoGen + LangChian + RAG + Function Call = Super AI Chabot Why OpenChat Model is So Much Better Than ChatGPT? How To Build a LLava chatbot How Powerful Step-Back Prompting Transforms LLM Performance Money Ai Automation Why OpenChat Model is So Much Better Than ChatGPT? No Result No Result ADVERTISEMENT Home Blog LangGraph : Create Your Hyper AI Agent by Gao Dalie (高達烈) February 2, 2024 in Blog SHARES 1.6k VIEWS Share on Facebook Share on Twitter ADVERTISEMENT ADVERTISEMENT LangChain has been around for a year.\n",
      "As an open-source framework, providing the modules and tools needed to build AI applications based on large models just a few days LangChain officially announced the new library called LangGraph LangGraph builds upon LangChain and simplifies the process of creating and managing agents and their runtimes. in this Post, we will introduce a comprehensive of langGraph, what are agents and agent runtimes? what is the Feature of Langgraph, and how to build an agent executor in LangGraph, we going to explore the Chat Agent Executor in LangGraph and How to modify the Chat Agent Executor in LangGraph in humans in a loop and chat Table of Contents Toggle Before we start! 🦸🏻‍♀️ So, what are agents and agent runtimes? A Key Feature how to build Agent Executor Exploring Chat Agent Executor How to modify humans in a loop Modify Managing Agent Steps Force-calling a Tool Conclusion :References : Before we start! 🦸🏻‍♀️ If you like this topic and you want to support me: Follow me on Medium and subscribe to get my latest article🫶 If you prefer video tutorials, please subscribe to my YouTube channel where I started to convert most of my articles to visual demonstrations. So, what are agents and agent runtimes? In LangChain, an agent is a system driven by a language model that makes decisions about actions to take. An agent runtime is what keeps this system running, continually deciding on actions, recording observations, and maintaining this cycle until the agent’s task is completed.\n",
      "LangChain has made agent customization easy with its expression language. LangGraph takes this further by allowing more flexible and dynamic customization of the agent runtime. The traditional agent runtime was the Agent EX class, but now with LangGraph, there’s more variety and adaptability. A Key Feature A key feature of LangGraph is the addition of cycles to the agent runtime. Unlike non-cyclical frameworks, LangGraph enables these repetitive loops essential for agent operation. We’re starting with two main agent runtimes in LangGraph: The Agent Executor is similar to LangChain’s but rebuilt in LangGraph. The Chat Agent Executor handles agent states as a list of messages — perfect for chat-based models that use messages for function calls and responses. how to build Agent Executor building an agent executor in LangGraph, similar to the one in LangChain. This process is surprisingly straightforward, so let’s dive in! First things first, we’ll need to set up our environment by installing a few packages: LangChain, LangChain OpenAI, and Tavily Python. These will help us utilize existing LangChain agent classes, power our agent with OpenAI’s language models, and use the Tavily Python package for search functionality. !pip install --quiet -U langchain langchain_openai tavily-python Next, we’ll set up our API keys for OpenAI, Tavily, and LingSmith. LingSmith is particularly important for logging and observability, but it’s currently in private beta.\n",
      "If you need access, feel free to reach out to Them. os import getpass os.environ[ \"OPENAI_API_KEY\"] = getpass.getpass( \"OpenAI API Key:\") os.environ[ \"TAVILY_API_KEY\"] = getpass.getpass( \"Tavily API Key:\") os.environ[ \"LANGCHAIN_TRACING_V2\"] = \"true\" os.environ[ \"LANGCHAIN_API_KEY\"] = getpass.getpass( \"LangSmith API Key:\") Our first step in the notebook is to create a LangChain agent. This involves selecting a language model, creating a search tool, and establishing our agent. For detailed information on this, you can refer to the LangChain documentation. from langchain import hub from langchain.agents import create_openai_functions_agent from langchain_openai.chat_models import ChatOpenAI from langchain_community.tools.tavily_search import TavilySearchResults tools = [TavilySearchResults(max_results= 1)] # Get the prompt to use - you can modify this! prompt = hub.pull( \"hwchase17/openai-functions-agent\") # Choose the LLM that will drive the agent llm = ChatOpenAI(model= \"gpt-3.5-turbo-1106\", streaming= True) # Construct the OpenAI Functions agent agent_runnable = create_openai_functions_agent(llm, tools, prompt) We then define the state of our graph, which tracks changes over time. This state allows each node in our graph to update the overall state, saving us the hassle of passing it around constantly. We’ll also decide how these updates will be applied, whether by overriding existing data or adding to it.\n",
      "from typing import TypedDict, Annotated, List, Union from langchain_core.agents import AgentAction, AgentFinish from langchain_core.messages import BaseMessage import operator class AgentState( TypedDict): # The input string input: str # The list of previous messages in the conversation chat_history: list[BaseMessage] # The outcome of a given call to the agent # Needs `None` as a valid type, since this is what this will start as agent_outcome: Union[AgentAction, AgentFinish, None] # List of actions and corresponding observations # Here we annotate this with `operator.add` to indicate that operations to # this state should be ADDED to the existing values (not overwrite it) intermediate_steps: Annotated[ list[ tuple[AgentAction, str]], operator.add] After setting up our state, we focus on defining nodes and edges in our graph. We need two primary nodes: one to run the agent and another to execute tools based on the agent’s decisions. Edges in our graph are of two types: conditional and normal. Conditional edges allow for branching paths based on previous results, while normal edges represent a fixed sequence of actions. We’ll look into specifics like the ‘run agent’ node, which invokes the agent, and the ‘execute tools’ function, which executes the tool chosen by the agent. We’ll also add a ‘should continue’ function to determine the next course of action.\n",
      "from langchain_core.agents import AgentFinish from langgraph.prebuilt.tool_executor import ToolExecutor # This a helper class we have that is useful for running tools # It takes in an agent action and calls that tool and returns the result tool_executor = ToolExecutor(tools) # Define the agent def run_agent( data): agent_outcome = agent_runnable.invoke(data) return { \"agent_outcome\": agent_outcome} # Define the function to execute tools def execute_tools( data): # Get the most recent agent_outcome - this is the key added in the `agent` above agent_action = data[ 'agent_outcome'] output = tool_executor.invoke(agent_action) return { \"intermediate_steps\": [(agent_action, str(output))]} # Define logic that will be used to determine which conditional edge to go down def should_continue( data): # If the agent outcome is an AgentFinish, then we return `exit` string # This will be used when setting up the graph to define the flow if isinstance(data[ 'agent_outcome'], AgentFinish): return \"end\" # Otherwise, an AgentAction is returned # Here we return `continue` string # This will be used when setting up the graph to define the flow else: return \"continue\" Finally, we construct our graph. We define it, add our nodes, set an entry point, and establish our edges — both conditional and normal. After compiling the graph, it’s ready to be used just like any LangChain runnable.\n",
      "from langgraph.graph import END, StateGraph # Define a new graph workflow = StateGraph(AgentState) # Define the two nodes we will cycle between workflow.add_node( \"agent\", run_agent) workflow.add_node( \"action\", execute_tools) # Set the entrypoint as `agent` # This means that this node is the first one called workflow.set_entry_point( \"agent\") # We now add a conditional edge workflow.add_conditional_edges( # First, we define the start node. We use `agent`. # This means these are the edges taken after the `agent` node is called. \"agent\", # Next, we pass in the function that will determine which node is called next. should_continue, # Finally we pass in a mapping. # The keys are strings, and the values are other nodes. # END is a special node marking that the graph should finish. # What will happen is we will call `should_continue`, and then the output of that # will be matched against the keys in this mapping. # Based on which one it matches, that node will then be called. { # If `tools`, then we call the tool node. \"continue\": \"action\", # Otherwise we finish. \"end\": END } ) # We now add a normal edge from `tools` to `agent`. # This means that after `tools` is called, `agent` node is called next. workflow.add_edge( 'action', 'agent') # Finally, we compile it! # This compiles it into a LangChain Runnable, # meaning you can use it as you would any other runnable app = workflow. compile() We’ll run our executor with some input data to see our executor in action.\n",
      "This process involves streaming the results of each node, allowing us to observe the agent’s decisions, the tools executed, and the overall state at each step. \"input\": \"what is the weather in sf\", \"chat_history\": []} for s in app.stream(inputs): print(list(s.values())[0]) print( \"----\") For a more visual understanding, we can explore these processes in LingSmith, which provides a detailed view of each step, including the prompts and responses involved in the execution. 'agent_outcome': AgentActionMessageLog(tool= 'tavily_search_results_json', tool_input={ 'query': 'weather in San Francisco'}, log= \"\\nInvoking: `tavily_search_results_json` with `{'query': 'weather in San Francisco'}`\\n\\n\\n\", message_log=[AIMessage(content= '', additional_kwargs={ 'function_call': { 'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}})])} ---- { 'intermediate_steps': [(AgentActionMessageLog(tool= 'tavily_search_results_json', tool_input={ 'query': 'weather in San Francisco'}, log= \"\\nInvoking: `tavily_search_results_json` with `{'query': 'weather in San Francisco'}`\\n\\n\\n\", message_log=[AIMessage(content= '', additional_kwargs={ 'function_call': { 'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}})]), \"[{'url': 'https://www.whereandwhen.net/when/north-america/california/san-francisco-ca/january/', 'content': 'Best time to go to San Francisco?\n",
      "Weather in San Francisco in january 2024 How was the weather last january? Here is the day by day recorded weather in San Francisco in january 2023: Seasonal average climate and temperature of San Francisco in january 8% 46% 29% 12% 8% Evolution of daily average temperature and precipitation in San Francisco in januaryWeather in San Francisco in january 2024. The weather in San Francisco in january comes from statistical datas on the past years. You can view the weather statistics the entire month, but also by using the tabs for the beginning, the middle and the end of the month. ... 16-01-2023 45°F to 52°F. 17-01-2023 45°F to 54°F. 18-01-2023 47°F to ...'}]\")]} That’s how you create an agent executor in LangGraph, mirroring the functionality of LangChain’s executor. we’ll explore more about the interface of the state graph and different streaming methods to return results. Else See : AutoGen + LangChian + SQLite + Function Schema = Super AI Chabot Exploring Chat Agent Executor we’re going to explore the Chat Agent Executor in LangGraph, a tool designed to work with chat-based models. This executor is unique because it operates entirely on a list of input messages, updating the agent’s state over time by adding new messages to this list. Let’s dive into the setup process: Installing Packages: We need the LangChain package, LangChain OpenAI for the model, and the Tavily package for the search tool. Setting API keys for these services is also necessary.\n",
      "!pip install --quiet -U langchain langchain_openai tavily-python os import getpass os.environ[ \"OPENAI_API_KEY\"] = getpass.getpass( \"OpenAI API Key:\") os.environ[ \"TAVILY_API_KEY\"] = getpass.getpass( \"Tavily API Key:\") os.environ[ \"LANGCHAIN_TRACING_V2\"] = \"true\" os.environ[ \"LANGCHAIN_API_KEY\"] = getpass.getpass( \"LangSmith API Key:\") Setting Up Tools and the Model: We’ll use Tavily Search as our tool, and set up a tool executor to invoke these tools. For the model, we’ll use the Chat OpenAI model from the LangChain integration, ensuring it’s initialized with streaming enabled. This enables us to stream back tokens and attach the functions we want the model to call. from langchain_community.tools.tavily_search import TavilySearchResults from langchain_openai import ChatOpenAI from langgraph.prebuilt import ToolExecutor from langchain.tools.render import format_tool_to_openai_function tools = [TavilySearchResults(max_results= 1)] tool_executor = ToolExecutor(tools) # We will set streaming=True so that we can stream tokens # See the streaming section for more information on this. model = ChatOpenAI(temperature= 0, streaming= True) functions = [format_tool_to_openai_function(t) for t in tools] model = model.bind_functions(functions) Defining Agent State: The agent state is a simple dictionary with a key for a list of messages. We’ll use an ‘add to’ annotation so that any updates from nodes to this messages list will accumulate over time.\n",
      "from typing import TypedDict, Annotated, Sequence import operator from langchain_core.messages import BaseMessage class AgentState( TypedDict): messages: Annotated[ Sequence[BaseMessage], operator.add] Creating Nodes and Edges: Nodes do the work, and edges connect them. We need an agent node to call the language model and get a response, an action node to see if there are any tools to be called, and a function to determine if we should proceed to tool calling or finish.\n",
      "from langgraph.prebuilt import ToolInvocation import json from langchain_core.messages import FunctionMessage # Define the function that determines whether to continue or not def should_continue( state): messages = state[ 'messages'] last_message = messages[- 1] # If there is no function call, then we finish if \"function_call\" not in last_message.additional_kwargs: return \"end\" # Otherwise if there is, we continue else: return \"continue\" # Define the function that calls the model def call_model( state): messages = state[ 'messages'] response = model.invoke(messages) # We return a list, because this will get added to the existing list return { \"messages\": [response]} # Define the function to execute tools def call_tool( state): messages = state[ 'messages'] # Based on the continue\n",
      "condition # we know the last message involves a function call last_message = messages[- 1] # We construct an ToolInvocation from the function_call action = ToolInvocation( tool=last_message.additional_kwargs[ \"function_call\"][ \"name\"], tool_input=json.loads(last_message.additional_kwargs[ \"function_call\"][ \"arguments\"]), ) # We call the tool_executor and get back a response response = tool_executor.invoke(action) # We use the response to create a FunctionMessage function_message = FunctionMessage(content= str(response), name=action.tool) # We return a list, because this will get added to the existing list return { \"messages\": [function_message]} Building the Graph: We create a graph with the agent state, add nodes for the agent and action, and set the entry point to the agent node. Conditional edges are added based on whether the agent should continue or end, and a normal edge always leads back to the agent after an action. from langgraph.graph import StateGraph, END # Define a new graph workflow = StateGraph(AgentState) # Define the two nodes we will cycle between workflow.add_node( \"agent\", call_model) workflow.add_node( \"action\", call_tool) # Set the entrypoint as `agent` # This means that this node is the first one called workflow.set_entry_point( \"agent\") # We now add a conditional edge workflow.add_conditional_edges( # First, we define the start node. We use `agent`. # This means these are the edges taken after the `agent` node is called.\n",
      "\"agent\", # Next, we pass in the function that will determine which node is called next. should_continue, # Finally we pass in a mapping. # The keys are strings, and the values are other nodes. # END is a special node marking that the graph should finish. # What will happen is we will call `should_continue`, and then the output of that # will be matched against the keys in this mapping. # Based on which one it matches, that node will then be called. { # If `tools`, then we call the tool node. \"continue\": \"action\", # Otherwise we finish. \"end\": END } ) # We now add a normal edge from `tools` to `agent`. # This means that after `tools` is called, `agent` node is called next. workflow.add_edge( 'action', 'agent') # Finally, we compile it! # This compiles it into a LangChain Runnable, # meaning you can use it as you would any other runnable app = workflow. compile() Compiling and Using the Graph: After compiling the graph, we create an input dictionary with a messages key. Running the graph will process these messages, adding AI responses, function results, and final outputs to the list of messages. from langchain_core.messages import HumanMessage inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]} app.invoke(inputs) Observing Under the Hood: Using LangSmith, we can see the detailed steps taken by our agent, including the calls made to OpenAI and the resulting outputs.\n",
      "Streaming Capabilities: LangGraph also offers streaming capabilities, which we’ll explore in more detail in video. How to modify humans in a loop let’s modify the Chat Agent Executor in LangGraph to include a ‘human in the loop’ component. This addition allows for human validation of tool actions before they are executed. We’ll build on the base notebook we’ve previously worked on. If you haven’t gone through that notebook, I recommend reviewing it first, as this video will mainly focus on the modifications we make to it. Setting Up: The initial setup remains the same. There are no additional installations needed. We’ll create our tool, set up the tool executor, prepare our model, bind tools to the model, and define the agent state — all as we did in the previous session. Key Modification — Call Tool Function: The major change comes in the call tool function. We’ve added a step where the system prompts the user (that’s you!) in the interactive IDE, asking whether to proceed with a particular action. If the user responds ‘no’, an error is thrown, and the process stops. This is our human validation step.\n",
      "# Define the function to execute tools def call_tool( state): messages = state[ 'messages'] # Based on the continue condition # we know the last message involves a function call last_message = messages[- 1] # We construct an ToolInvocation from the function_call action = ToolInvocation( tool=last_message.additional_kwargs[ \"function_call\"][ \"name\"], tool_input=json.loads(last_message.additional_kwargs[ \"function_call\"][ \"arguments\"]), ) response = input(prompt= f\"[y/n] continue with: {action}?\") if response == \"n\": raise ValueError # We call the tool_executor and get back a response response = tool_executor.invoke(action) # We use the response to create a FunctionMessage function_message = FunctionMessage(content= str(response), name=action.tool) # We return a list, because this will get added to the existing list return { \"messages\": [function_message]} Using the Modified Executor: When we run this modified executor, it will ask for approval before executing any tool action. If we approve by saying ‘yes’, it proceeds as normal. However, if we say ‘no’, it raises an error and halts the process.\n",
      "utput from node 'agent': --- { 'messages': [ AIMessage(content='', additional_kwargs={ 'function_call': { 'arguments': '{\\n \"query\": \"weather in San Francisco\"\\n}', 'name': 'tavily_search_results_json'}} )]} --- --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[10], line from langchain_core.messages import HumanMessage inputs = { \"messages\": [ HumanMessage(content=\"what is the weather in sf\")]} ----> for output in app.stream(inputs): # stream() yields dictionaries with output keyed by node name for key, value in output.items(): print(f\"Output from node '{key}' :\") This is a basic implementation. In a real-world scenario, you might want to replace the error with a more sophisticated response and use a more user-friendly interface instead of a Jupyter Notebook. But this gives a clear idea of how you can add a simple yet effective human-in-the-loop component to your LangGraph agents Modify Managing Agent Steps let’s take a look at modifying the Chat Agent Executor in LangGraph to manipulate the internal state of the agent as it processes messages. This tutorial builds on the basic Chat Agent Executor setup, so if you haven’t gone through the initial setup in the base notebook, please do that first. We’ll focus here only on the new modifications. Key Modification — Filtering Messages: The primary change we’re introducing is a way to filter the messages passed to the model.\n",
      "You can now customize which messages the agent considers. For instance: 'messages'][- 5:] response = model.invoke(messages) # We return a list, because this will get added to the existing list return { \"messages\": [response]} Selecting only the five most recent messages. Including the system message plus the five latest messages. Summarizing messages that are older than the five most recent ones. This modification is a minor but powerful addition, allowing you to control how the agent interacts with its message history and improves its decision-making process. Using the Modified Executor: The implementation is straightforward. You won’t see a difference with just one input message, but the essential part is that any logic you wish to apply to the agent’s steps can be inserted into this new modification section. This method is ideal for modifying the Chat Agent Executor, but the same principle applies if you’re working with a standard agent executor. Force-calling a Tool we’ll be making a simple but effective modification to the Chat Agent Executor in LangGraph, ensuring that a tool is always called first. This builds on the base Chat Agent Executor notebook, so make sure you’ve checked that out for background information. Key Modification — Forcing a Tool Call First: Our focus here is on setting up the chat agent to call a specific tool as its first action. To do this, we’ll add a new node, which we’ll name ‘first model node’.\n",
      "This node will be programmed to return a message instructing the agent to call a particular tool, such as the ‘Tavil search results Json’ tool, with the most recent message content as the query. # This is the new first - the first call of the model we want to explicitly hard-code some action from langchain_core.messages import AIMessage import json def first_model( state): human_input = state[ 'messages'][- 1].content return { \"messages\": [ AIMessage( content= \"\", additional_kwargs={ \"function_call\": { \"name\": \"tavily_search_results_json\", \"arguments\": json.dumps({ \"query\": human_input}) } } ) ] } Updating the Graph: We’ll modify our existing graph to include this new ‘first agent’ node as the entry point. This ensures that the first agent node is always called first, followed by the action node. We set up a conditional node from the agent to the action or end, and a direct node from the action back to the agent. The crucial addition is a new node from the first agent to action, guaranteeing that the tool call happens right at the start.\n",
      "from langgraph.graph import StateGraph, END # Define a new graph workflow = StateGraph(AgentState) # Define the new entrypoint workflow.add_node( \"first_agent\", first_model) # Define the two nodes we will cycle between workflow.add_node( \"agent\", call_model) workflow.add_node( \"action\", call_tool) # Set the entrypoint as `agent` # This means that this node is the first one called workflow.set_entry_point( \"first_agent\") # We now add a conditional edge workflow.add_conditional_edges( # First, we define the start node. We use `agent`. # This means these are the edges taken after the `agent` node is called. \"agent\", # Next, we pass in the function that will determine which node is called next. should_continue, # Finally we pass in a mapping. # The keys are strings, and the values are other nodes. # END is a special node marking that the graph should finish. # What will happen is we will call `should_continue`, and then the output of that # will be matched against the keys in this mapping. # Based on which one it matches, that node will then be called. { # If `tools`, then we call the tool node. \"continue\": \"action\", # Otherwise we finish. \"end\": END } ) # We now add a normal edge from `tools` to `agent`. # This means that after `tools` is called, `agent` node is called next. workflow.add_edge( 'action', 'agent') # After we call the first agent, we know we want to go to action workflow.add_edge( 'first_agent', 'action') # Finally, we compile it!\n",
      "# This compiles it into a LangChain Runnable, # meaning you can use it as you would any other runnable app = workflow. compile() Using the Modified Executor: When we run this updated executor, the first result comes back quickly because we bypass the initial language model call and go straight to invoking the tool. This is confirmed by observing the process in LangSmith, where we can see that the tool is the first thing invoked, followed by a language model call at the end. This modification is a simple yet powerful way to ensure that specific tools are utilized immediately in your chat agent’s workflow. Else See: CrewAi + Solor/Hermes + Langchain + Ollama = Super Ai Agent Conclusion : And that’s a wrap! you know how to build a hyper AI agent. I hope you have gained a cursory understanding of langGraph capabilities. As a next step, try exploring LangGraph to build more interesting applications.\n",
      "References : https://github.com/langchain-ai/langgraph/blob/main/examples/agent_executor/base.ipynb https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/base.ipynb https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/human-in-the-loop.ipynb https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/managing-agent-steps.ipynb https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/force-calling-a-tool-first.ipynb Previous Post CrewAi + Solar/Hermes + Langchain + Ollama = Super Ai Agent Next Post What Is Langchain 0.1.0 ? Explained Simply Gao Dalie (高達烈) Gao Dalie (高達烈) he is a data driven enthusiast deeply passioned about Data Science, Automation, and Artificial Intelligence. AS a top writer on Medium in the Artificial Intelligence category ,He is also the founder of QuickAITutorial. For more of his insights, follow him on Medium @mr.tarik098. Next Post What Is Langchain 0.1.0 ? Explained Simply Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment * Name * Email * Website Save my name, email, and website in this browser for the next time I comment.\n",
      "You might also like LangGraph + Corrective RAG + Local LLM = Powerful Rag Chatbot 2 weeks ago Five Technique : VLLM + Torch + Flash_Attention =Super Local LLM 3 weeks ago [ollama libraries 🦙] Run Any Chatbot Free Locally on Your Computer 3 weeks ago LangGraph + Gemini Pro + Custom Tool + Streamlit = Multi-Agent Application Development 4 weeks ago What Is Langchain 0.1.0 ? Explained Simply 1 month ago LangGraph : Create Your Hyper AI Agent 1 month ago Quick AI Tutorial is your gateway to the world of Data Science, Machine Learning, and Artificial Intelligence. Dive into our expertly crafted articles to gain insights, master cutting-edge techniques, and discover lucrative opportunities in the digital realm. Stay Connected More… About Us Contact Us Policy Privacy Cookies Policy Home News Technology Money Ai Automation © 2023 quickaitutorial - Premium WordPress news & magazine theme by quickaitutorial. No Result Home News Technology Money Ai Automation © 2023 quickaitutorial - Premium WordPress news & magazine theme by quickaitutorial.\n",
      "\n",
      "Source: blog.min.io_minio-langchain-tool.txt\n",
      "Content: Topics All Architect's Guide Operator's Guide Best Practices AI/ML Modern Data Lakes Performance Kubernetes Integrations Benchmarks Security Multicloud Try the Erasure Code Calculator to configure your usable capacity Try Now Developing Langchain Agents with the MinIO SDK for LLM Tool-Use David Cannan David Cannan on AI/ML Share: Linkedin X (Twitter) Reddit Copy Article Link Email Article Follow: LinkedIn Reddit In my previous article on Langchain, I explored the use of the “community S3 loaders”, while useful, offer limited functionality. Here we delve into the development of customized tools (focusing on MinIO object upload for this demonstration) and their integration with Large Language Models (LLMs) through Langchain Agents and Executors. This demonstration showcases the process of uploading objects to a bucket, leveraging the MinIO Python SDK and Langchain as the foundational tools. This exploration opens up new possibilities, moving beyond basic loaders to a more versatile and powerful implementation that enhances the capabilities of language model-driven applications. This strategic move towards combining sophisticated language models with robust data storage capabilities, is an evolution that enhances the functionality of language-driven applications, providing a pathway to advanced tool utilization with Large Language Models (LLMs).\n",
      "Utilizing MinIO's secure and scalable object storage in conjunction with Langchain's ability to leverage the full capabilities of LLMs like OpenAI's GPT, developers can create systems that not only mimic human text generation but also carry out complex tasks, elevating the efficiency of data management to new heights. Langchain serves as the critical interface that translates human instructions into the operational language of machine intelligence. Envision it as a sophisticated intermediary, capable of interpreting user commands and orchestrating a range of activities from data organization within MinIO's structures to the detailed analysis of data sets. This capability effectively converts the theoretical prowess of LLMs into tangible, functional assets within the developer's toolkit, allowing for the crafting of advanced solutions that were once considered futuristic. In this guide, we prioritize practicality, steering towards actionable development practices. We invite developers to embrace the potential that lies in the union of Langchain and MinIO SDK, to not only innovate but also to redefine the boundaries of what can be achieved with the intelligent automation of today's digital tools. The source code and detailed documentation to accompany this exploration can be found here.\n",
      "This notebook provides the minimal and necessary resources to get started on your journey with Langchain and MinIO, offering a hands-on experience to deepen your understanding and skills in creating intelligent, data-driven applications. Enhancing Conversational Agents with Contextual Memory and Advanced Data Handling Integrating memory management into Langchain applications significantly elevates their ability to deliver responses that are not only relevant but also deeply context-aware. This advancement permits an agent to draw upon past dialogues, providing a richer, more layered understanding of each interaction. The real power of this feature lies in its ability to tailor responses based on the accumulated history of the user's session, transforming standard interactions into personalized experiences that resonate more deeply with users. The inclusion of memory capabilities, especially when combined with the functionality to expose object stores as agent tools, revolutionizes the landscape of AI-driven conversational agents. Developers are bestowed with the tools to create agents that not only execute tasks with unparalleled accuracy but also evolve and adapt to users' needs through ongoing interactions. This adaptability marks a leap forward in developing interactive applications, where the agent not only responds to but anticipates user requirements, crafting a truly interactive and intuitive user experience.\n",
      "Moreover, this approach lays down a comprehensive blueprint for seamlessly merging MinIO's robust data management capabilities with Langchain's advanced processing power, offering a meticulously detailed guide for enhancing conversational agents. The result is a harmonious integration that leverages the strengths of MinIO and Langchain, offering developers a rich palette for creating applications that are as technically profound as they are user-centric. Setting Up the Environment It's crucial to begin by setting up the development environment with all necessary packages. This ensures that you have all the required libraries and dependencies installed. First install two key packages: the MinIO Python SDK and Langchain. The MinIO SDK is a powerful tool that allows us to interact with MinIO buckets, enabling operations such as file uploads, downloads, and bucket management directly from our Python scripts. On the other hand, Langchain is an innovative framework that enables the creation of applications combining large language models with specific tasks, such as file management in this case. Together, these packages form the backbone of our tool, allowing us to leverage the strengths of both MinIO's robust storage solutions and the advanced natural language processing capabilities of large language models.\n",
      "To install these packages, run the following command in your terminal: Install package dependencies This command installs the latest versions of the MinIO client and Langchain, along with all optional dependencies required for Langchain. Integrating Langsmith for Process Monitoring and Tracing (Optional) A key aspect of developing with Langchain is the ability to monitor and trace the execution of tasks, especially when integrating complex functionalities like object storage operations with MinIO. Langsmith offers an intuitive platform to visualize these processes, providing real-time insights into the performance and efficiency of your Langchain applications. Below, we’ve included screenshots from Langsmith that highlight the seamless execution of tasks, from invoking LLMs to performing specific actions such as file uploads and data processing. These visual aids not only serve to demystify the underlying processes but also showcase the practical application of Langchain and MinIO SDK in creating sophisticated, AI-driven tools. Through Langsmith, developers can gain a deeper understanding of their application’s behavior, making it easier to optimize and refine their solutions for better performance and user experience. Incorporating Langsmith into your development workflow not only enhances transparency but also empowers you to build more reliable and efficient Langchain applications.\n",
      "By leveraging these insights, you can fine-tune your applications, ensuring they meet the high standards required for production environments. To get started with Langsmith, follow these steps: 1. Create a Langsmith Project: Visit smith.langchain.com and sign up or log in to your account. Once logged in, create a new project by selecting the option to create a new project and name it “Langchain MinIO Tool”. This project will be the central hub for monitoring the interactions and performance of your Langchain integrations. 2. Generate an API Key: After creating your project, navigate to the project settings to generate a new API key. This key will authenticate your application's requests to Langsmith, ensuring secure communication between your tool and the Langsmith service. 3. Configure Environment Variables: Langsmith requires several environment variables to be set up in your development environment. These variables enable your application to communicate with Langsmith's API and send tracing data. An example of these variables includes: Exporting environment variables for Langsmith Replace <your-api-key> with the actual API key generated in the previous step. These environment variables enable the Langchain SDK in your application to send tracing and monitoring data to your Langsmith project, providing you with real-time insights into the operation and performance of your Langchain integrations.\n",
      "Initializing OpenAI and MinIO Clients for File Management The foundation of building a Langchain tool that integrates with MinIO for file uploads involves setting up the clients for both OpenAI and MinIO. This setup allows your application to communicate with OpenAI's powerful language models and MinIO's efficient file storage system. Here's how you can initialize these crucial components in Python: Setting Up the Language Model with OpenAI First, we need to initialize the language model using OpenAI's API. This step involves using the langchain_openai package to create an instance of ChatOpenAI, which will serve as our interface to OpenAI's language models. This requires an API key from OpenAI, which you can obtain from your OpenAI account. Setup llm using ChatOpenAI Replace the empty string in (api_key=\"\") with your actual OpenAI API key. This key enables authenticated requests to OpenAI, allowing you to leverage the language model for processing and generating text. Importing Necessary Libraries Before proceeding, ensure you import the necessary libraries. These imports include io for handling byte streams and tool from langchain.agents, which is a decorator used to register functions as tools that can be utilized by Langchain agents. Importing io and langchain.agents tool Initializing the MinIO Client Next, we initialize the MinIO client, which allows our application to interact with MinIO buckets for operations like uploading, downloading, and listing files.\n",
      "The MinIO client is initialized with the server endpoint, access key, secret key, and a flag to indicate whether to use a secure connection (HTTPS). Setting the minio_client In this example, we're using MinIO's play server (play.min.io:443) with the default credentials (minioadmin for both access and secret keys). In a production environment, you should replace these with your MinIO server details and credentials. By initializing the OpenAI and MinIO clients, you set the stage for developing advanced tools that can interact with natural language processing models and manage files in MinIO buckets, opening a wide range of possibilities for automating and enhancing file management tasks. Managing Bucket Availability in MinIO An essential part of working with MinIO involves managing buckets, which are the basic containers that hold your data. Before uploading files, it's important to ensure that the target bucket exists. This process involves checking the existence of a bucket and creating it if it doesn't exist. This approach not only prepares your environment for file operations but also avoids errors related to non-existent buckets. Here's a simple yet effective helper function and code snippet for managing the availability of a bucket in MinIO: Bucket helper function This code performs the following operations: 1. Define the Bucket Name: It starts by specifying the name of the bucket you want to check or create, in this case, \"test\". 2.\n",
      "Check for Bucket Existence: It uses the bucket_exists method of the MinIO client to check if the bucket already exists. 3. Create the Bucket if Necessary: If the bucket does not exist, the make_bucket method is called to create a new bucket with the specified name. 4. Handle Errors: The operation is wrapped in a “try-except block” to catch and handle any S3Error exceptions that may occur during the process, such as permission issues or network errors. By ensuring the bucket's existence before performing file operations, you can make your applications more robust and user-friendly, providing clear feedback and avoiding common pitfalls associated with bucket management in MinIO. Implementing File Upload Functionality Once the environment is configured and the necessary checks are in place to ensure that the target bucket exists, the next step is to implement the core functionality of uploading files to MinIO. This involves creating a function that takes the bucket name, the object name (file name within the bucket), and the file's binary data as inputs, and then uses the MinIO client to upload the file to the specified bucket. Here's how you can define this upload function: Python “upload” function with Langchain’s @tool decorator. Key Components of the Upload Function: Function Decorator (@tool): This decorator is used to register the function as a tool within the Langchain framework, making it callable as part of a Langchain workflow or process.\n",
      "It enables the function to be integrated seamlessly with Langchain agents and executors. Parameters: The function takes three parameters: bucket_name: The name of the MinIO bucket where the file will be uploaded. object_name: The name you wish to assign to the file within the bucket. data_bytes: The binary data of the file to be uploaded. Creating a Byte Stream: The binary data (data_bytes) is wrapped in a BytesIO stream. This is necessary because the MinIO client's put_object method expects a stream of data rather than raw bytes. Uploading the File: The put_object method of the MinIO client is called with the bucket name, object name, data stream, and the length of the data. This method handles the upload process, storing the file in the specified bucket under the given object name. Return Statement: Upon successful upload, the function returns a confirmation message indicating the success of the operation. This upload function is a fundamental building block for creating applications that interact with MinIO storage. It encapsulates the upload logic in a reusable and easily callable format, allowing developers to integrate file upload capabilities into their Langchain applications and workflows efficiently. Enhancing Functionality with RunnableLambda and Secondary Tools After establishing the fundamental upload functionality, enhancing the system for broader integration and additional utility can further refine the tool's capabilities.\n",
      "This involves creating a RunnableLambda for the upload function and defining secondary tools that can be utilized within the same ecosystem. These steps not only extend the functionality but also ensure seamless integration with Langchain workflows. Creating a RunnableLambda for File Upload Langchain's architecture supports the encapsulation of functions into runnables, which can be seamlessly executed within its framework. To facilitate the execution of the upload function within Langchain workflows, we wrap it in a RunnableLambda. This allows the function to be easily integrated with Langchain's agents and executors, enabling automated and complex workflows that can interact with MinIO. Wrap upload_file_runnable with Langchain’s RunnableLambda The RunnableLambda takes our upload_file_to_minio function and makes it readily usable within Langchain's system, enhancing the tool's interoperability and ease of use within different parts of the application. Incorporating Secondary Functions for Extended Functionality In our exploration of integrating MinIO with Langchain, we've primarily focused on the core functionality of uploading files. However, the versatility of Langchain allows for the incorporation of a wide range of functionalities beyond just file management. To illustrate this flexibility, we've included an example of a secondary function, get_word_length, directly inspired by the examples found in the Langchain documentation.\n",
      "This serves to demonstrate how easily additional functions can be integrated into your Langchain projects. The inclusion of the get_word_length function is intended to showcase the process of adding more functionalities to your Langchain tool. Here's a closer look at how this secondary tool is defined: Python “secondary” function with Langchain’s @tool decorator This function, marked with the @tool decorator, is a simple yet effective demonstration of extending your tool's capabilities. By registering this function as another tool within the Langchain framework, it becomes callable in a similar manner to the file upload functionality, showcasing the ease with which developers can enrich their applications with diverse capabilities. The process of adding this function, taken from Langchain's documentation, is not only a testament to the ease of extending your application's functionality but also highlights the importance of leveraging existing resources and documentation to enhance your projects. This approach encourages developers to think beyond the immediate requirements of their projects and consider how additional features can be integrated to create more versatile and powerful tools. These enhancements demonstrate the system's capability to not only perform core tasks like file uploads but also execute additional functionalities, all integrated within the Langchain framework.\n",
      "The inclusion of a RunnableLambda and the introduction of secondary tools exemplify how developers can build a rich ecosystem of functions that work together to automate and streamline processes, leveraging both Langchain and MinIO's robust features. Crafting Interactive Chat Prompts with Langchain As we delve deeper into the integration of MinIO with Langchain, a crucial aspect is designing an interactive experience that leverages the capabilities of both the MinIO upload tool and any additional tools we've integrated. This is where the creation of a ChatPromptTemplate becomes essential. It serves as the blueprint for the interactions between the user and the system, guiding the conversation flow and ensuring that the user's commands are interpreted correctly and efficiently. Creating a ChatPromptTemplate The ChatPromptTemplate is a powerful feature of Langchain that allows developers to pre-define the structure of chat interactions. By specifying the roles of the system and the user within the chat, along with placeholders for dynamic content, we can create a flexible yet controlled dialogue framework.\n",
      "Here's how you can create a chat prompt template that incorporates the functionality of our tools: Importing and defining prompt using ChatPromptTemplate and MessagePlaceholder In this template: System Message: The first message from the \"system\" sets the context for the interaction, informing the user (or the language model assuming the user's role) that they are interacting with an assistant that possesses file management capabilities. This helps to frame the user's expectations and guide their queries or commands. User Input Placeholder: The \"user\" key is followed by \"{input}\", which acts as a placeholder for the user's actual input. This dynamic insertion point allows the system to adapt to various user commands, facilitating a wide range of file management tasks. MessagesPlaceholder for Agent Scratchpad: The MessagesPlaceholder with the variable name \"agent_scratchpad\" is a dynamic area within the chat where additional information, such as the results of tool executions or intermediate steps, can be displayed. This feature enhances the interactivity and responsiveness of the chat, providing users with feedback and results in real-time. This step in setting up the chat prompt template is pivotal for creating an engaging and functional user interface for our Langchain application.\n",
      "It not only structures the interaction but also seamlessly integrates the functionalities of our MinIO upload tool and any additional tools, making them accessible through natural language commands within the chat environment. Binding Tools to the Language Model for Enhanced Functionality A pivotal step in harnessing the full potential of our Langchain application is the integration of our custom tools with the language model. This use of “custom tools”, and similar “tool-use” integrations, allows the language model to not only understand and generate natural language but also to execute specific functionalities, such as uploading files to MinIO or calculating the length of a word. By binding these tools directly to the language model, we create a powerful, multifunctional system capable of processing user inputs and performing complex tasks based on those inputs. How to Bind Tools to the Language Model Binding tools to the language model (LLM) involves creating a list of the functions we've defined as tools and using the bind_tools method provided by Langchain's ChatOpenAI class. This method associates our tools with a particular instance of the language model, making them callable as part of the language processing workflows. Here's how this can be achieved: Binding the list of functions to the LLM In this code snippet: Tools List: We start by defining a list named tools that contains the functions we've decorated with @tool.\n",
      "This includes our upload_file_to_minio function for uploading files to MinIO and the get_word_length function as an example of a secondary tool. You can add more tools to this list as needed. Binding Tools: The bind_tools method takes the list of tools and binds them to the llm instance. This creates a new language model instance, llm_with_tools, which has our custom tools integrated into it. This enhanced language model can now interpret commands related to the functionalities provided by the bound tools, enabling a seamless interaction between natural language processing and task execution. This process of binding tools to the language model is crucial for creating interactive and functional Langchain applications. It bridges the gap between natural language understanding and practical task execution, allowing users to interact with the system using conversational language to perform specific actions, such as file management in MinIO. This integration significantly expands the capabilities of Langchain applications, paving the way for innovative uses of language models in various domains. Implementing Memory Management for Enhanced User Interaction Creating an engaging and intuitive Langchain application requires more than just processing commands—it demands a system that can remember and learn from past interactions.\n",
      "This is where memory management becomes essential, enabling the application to maintain context across conversations, which is particularly useful for handling complex queries and follow-up questions. Establishing the Agent and Executor Framework At the heart of our application lies the AgentExecutor, a sophisticated mechanism designed to interpret user inputs, manage tasks, and facilitate communication between the user and the system. To set this up, we need to incorporate several key components from Langchain: Importing AgentExecutor openai_tools , and OpenAIToolsAgentOutputParser This foundational setup ensures our application has the necessary tools to execute tasks, interpret the language model's output, and apply our custom functionalities effectively. Memory Management Integration To enrich the user experience with context-aware responses, we update our chat prompt template to include memory management features. This involves adding placeholders for storing and referencing the chat history within our conversations: Refactoring prompt and agent_scratchpad with empty chat_history list This adjustment enables our application to dynamically incorporate previous interactions into the ongoing conversation, ensuring a cohesive and context-rich dialogue with the user. Refining the Agent with Contextual Awareness To fully leverage the benefits of memory management, we refine our agent's definition to incorporate chat history actively.\n",
      "This involves defining specific behaviors for handling inputs, managing the agent's scratchpad, and incorporating the chat history into the agent's decision-making process: Define agent chain and agent_executor In this enhanced agent setup: Custom Lambda Functions: The agent utilizes lambda functions to handle the user's input, manage intermediate steps stored in the agent's scratchpad, and seamlessly integrate the chat history into the conversation flow. Agent Pipeline Assembly: By chaining the components with the pipeline operator (|), we create a streamlined process that takes the user's input, enriches it with contextual history, executes the necessary tools, and interprets the output for actionable responses. Execution with AgentExecutor: The AgentExecutor is initialized with our context-aware agent and the predefined tools, equipped with verbose logging for detailed operational insights. Executing File Uploads with Contextualized Agent Interactions Integrating MinIO file uploads into a Langchain-based application offers a practical example of how conversational AI can streamline complex operations. This capability becomes even more powerful when combined with memory management, allowing the agent to maintain context and manage tasks like file uploads dynamically. Here’s how to set up and execute a file upload process, demonstrating the application's ability to interpret and act on user commands.\n",
      "Defining the User Prompt and File Information First, we establish a user prompt that instructs the system on the desired action—in this case, uploading a file with specific parameters. Alongside this, we define the structure for the file information, including the bucket name, object name, and the data bytes of the file to be uploaded: input1 is where we define our user prompt as a string. This setup not only specifies what the user wants to do but also encapsulates the necessary details for the MinIO upload operation in a structured format. The Agent Execution With the prompt and file information defined, we proceed to simulate the execution of the file upload command through our agent. This simulation involves invoking the agent_executor with the specified input, including the chat history to maintain conversational context: Invoking our agent_executor with input1 and chat_history In this process: Invocation of Agent Executor: The agent_execution is called with a dictionary containing the user's input, the current chat history, and the structured file information. This approach allows the agent to understand the command within the context of the conversation and access the necessary details for the file upload operation. Updating Chat History: After executing the command, we update the chat history with the new interaction. This includes recording the user's input as a HumanMessage and the system's response as an AIMessage.\n",
      "This step is crucial for maintaining an accurate and comprehensive record of the conversation, ensuring that context is preserved for future interactions. This example illustrates the seamless integration of conversational AI with cloud storage operations, showcasing how a Langchain application can interpret natural language commands to perform specific tasks like file uploads. By maintaining a conversational context and dynamically managing file information, the system offers a user-friendly and efficient way to interact with cloud storage services, demonstrating the practical benefits of combining AI with cloud infrastructure. Streamlining File Uploads with Conversational Prompts One of the remarkable features of integrating Langchain with MinIO for file uploads is the flexibility it offers in handling user requests. While the detailed approach of specifying file_info directly provides clarity and control, the system is also designed to understand and extract necessary information from conversational prompts. This means users can initiate file uploads without explicitly filling out the file_info structure, simply by conveying all required information within the input1 prompt. Simplifying File Upload through Natural Language By crafting a detailed prompt like input1, users can communicate their intent and provide all necessary details for the file upload in a single message.\n",
      "The system's underlying intelligence, powered by Langchain's processing capabilities, parses this input to extract actionable data such as the bucket name, object name, and content. This approach mimics natural human communication, making the interaction with the application more intuitive and user-friendly. Example of a Conversational Prompt This prompt succinctly communicates the user's intent, specifying the object name, content, and target bucket, all within a natural language sentence. The application then processes this prompt, dynamically generating the equivalent file_info needed for the upload operation. Leveraging the file_input Method for Programmatic Execution For scenarios where the application is being used programmatically or when automating tasks as part of a larger workflow, expanding upon the file_input method becomes invaluable. This method allows for a more structured approach to specifying file details, making it ideal for situations where prompts are generated or modified by other parts of an application. The flexibility to switch between conversational prompts and programmatically specified file_info showcases the adaptability of the Langchain and MinIO integration. It enables developers to tailor the file upload process to suit the needs of their application, whether they are aiming for the simplicity of natural language interaction or the precision of programmatically defined tasks.\n",
      "The ability to run file uploads through Langchain without manually filling out the file_info strings, relying instead on the richness of natural language prompts, significantly enhances the usability and accessibility of the application. This feature, combined with the option to use the file_input method for more structured command chains, exemplifies the system's versatility in catering to diverse user needs and operational contexts. Bringing It All Together: The Proof in the Picture. As we reach the culmination of our journey through Langchain's capabilities and the MinIO SDK, it's time to reflect on the tangible outcomes of our work. The power of Langchain is not just in its ability to facilitate complex tasks but also in its tangible, visible results, which we can observe in the provided screenshots. The first screenshot offers a clear view of the MinIO bucket, showcasing the objects that have been successfully created as a result of our LLM tool-use and agent interactions. The objects listed, including \"funny_object\", \"minio-action\", and \"proof-of-concept\", serve as a testament to the practical application of our guide. This visual evidence underscores the effectiveness of using Langchain to manage and organize data within a robust object storage system like MinIO. In the second screenshot, we witness the Langchain agent in action. The trace of the AgentExecutor chain details the invocation steps, clearly marking the success of each task.\n",
      "Here, we see the sequence of the agent's operation, from the initiation of the chain to the successful execution of the file upload. This output provides users with a transparent view into the process, offering assurance and understanding of each phase in the operation. Jupyter Notebook Together, these visuals not only serve as proof of concept but also illustrate the seamless integration and interaction between the Langchain agents, LLMs, and MinIO storage. As developers and innovators, these insights are invaluable, providing a clear path to refine, optimize, and scale our applications. This is the true beauty of combining these technologies: creating a world where conversational AI meets practical execution, leading to efficient and intelligent tool-use that pushes the boundaries of what we can achieve with modern technology. Embracing the Future: Intelligent Automation with Langchain and MinIO The interplay between Langchain’s sophisticated language understanding and MinIO’s robust object storage has been vividly demonstrated through practical examples and visual evidence. The journey from conceptual explanations to the execution of real-world tasks has illustrated the transformative potential of LLMs when they are finely tuned and integrated with cloud-native technologies. The resulting synergy not only streamlines complex data operations but also enriches the user experience, providing intuitive and intelligent interactions with digital environments.\n",
      "As we reflect on the capabilities of Langchain agents and MinIO SDK, the future of tool development with LLMs looks promising, brimming with opportunities for innovation and efficiency. Whether you are a seasoned developer or a curious newcomer, the path forward is clear: the integration of AI with cloud storage is not just a leap towards more advanced applications but a stride into a new era of intelligent automation. With the right tools and understanding, there is no limit to the advancements we can make in this exciting domain. We are excited to accompany you on your journey with Langchain and MinIO. As you navigate through the intricacies of these technologies, know that our community is here to support you. Connect with us on Slack for any queries or to share your progress. We’re always ready to help or simply celebrate your successes. For those who’ve made it this far and are eager to dive into the practical implementation, don’t forget to check out the accompanying notebook for a hands-on experience; access the notebook here. Here’s to crafting the future, one innovative solution at a time!\n",
      "Previous Post Next Post S3 Select Security Modern Data Lakes Apache Presto SQL Performance S3 Brand/Design Golang Programming Cloud Computing Microservices Docker AWS Kubernetes Apache Spark Open Source Benchmarks Integrations SUBNET Edge Computing Sidekick Secure-by-Design Splunk Veeam Intel Apache Nifi Immutability Software Defined Storage VMware Apache Arrow Hybrid Cloud Red Hat OpenShift Multicloud Scalability Cloud Field Day Cloud Native Apache Kafka Architect's Guide Awards Operator's Guide Security Advisory AI/ML AGPLv3 Apache Hadoop SFD Azure GCP Observability Analytics H20 DevOps Apache Iceberg Apache Hudi YouTube Summaries EKS Elastic Load Balancers CI/CD Object Storage Compliance opentelemetry BC/DR Storage Newsletter Predictions Best Practices Dremio New MinIO Features partners Small Files Databases DuckDB PostgreSQL Delta Lake Cloud Repatriation Python Object Lambdas Data Pipelines Cloud Operating Model Webhook ClickHouse Vector Database Events Value Engineering Change Data Capture\n",
      "\n",
      "Source: www.sitepoint.com_langchain-python-complete-guide_.txt\n",
      "Content: Free Tech Books AI JavaScript Computing Design & UX HTML & CSS Entrepreneur Web PHP WordPress Mobile Programming Python AI A Complete Guide to LangChain in Python Matt Nikonorov Share LangChain is a versatile Python library that empowers developers and researchers to create, experiment with, and analyze language models and agents. It offers a rich set of features for natural language processing (NLP) enthusiasts, from building custom models to manipulating text data efficiently. In this comprehensive guide, we’ll dive deep into the essential components of LangChain and demonstrate how to harness its power in Python. Getting Set Up Agents Creating a LangChain agent Agent test example 1 Agent test example 2 Models Language model Chat model Embeddings A use case of embedding models Chunks Splitting chunks by character Recursively splitting chunks Chunk size and overlap Chains Going Beyond OpenAI Conclusion Getting Set Up To follow along with this article, create a new folder and install LangChain and OpenAI using pip: install langchain openai Agents In LangChain, an Agent is an entity that can understand and generate text. These agents can be configured with specific behaviors and data sources and trained to perform various language-related tasks, making them versatile tools for a wide range of applications. Creating a LangChain agent Agents can be configured to use “tools” to gather the data they need and formulate a good response. Take a look at the example below.\n",
      "It uses Serp API (an internet search API) to search the Internet for information relevant to the question or input, and uses that to make a response. It also uses the llm-math tool to perform mathematical operations — for example, to convert units or find the percentage change between two values: from langchain .agents import load_tools from langchain .agents import initialize_agent from langchain .agents import AgentType from langchain .llms import OpenAI import os os .environ \"OPENAI_API_KEY\" \"YOUR_OPENAI_API_KEY\" os .environ \"SERPAPI_API_KEY\" \"YOUR_SERP_API_KEY\" # get your Serp API key here: https://serpapi.com/ OpenAI .api_key \"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\" llm = OpenAI (model \"gpt-3.5-turbo\" , temperature ) tools = load_tools \"serpapi\" \"llm-math\" , llm =llm ) agent = initialize_agent (tools , llm , agent =AgentType .ZERO_SHOT_REACT_DESCRIPTION , verbose True ) agent .run \"How much energy did wind turbines produce worldwide in 2022?\" As you can see, after doing all the basic importing and initializing our LLM (llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)), the code loads the tools necessary for our agent to work using tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm). It then creates the agent using the initialize_agent function, giving it the specified tools, and it gives it the ZERO_SHOT_REACT_DESCRIPTION description, which means that it will have no memory of previous questions.\n",
      "Agent test example 1 Let’s test this agent with the following input: As you can see, it uses the following logic: search for “wind turbine energy production worldwide 2022” using the Serp internet search API analyze the best result get any relevant numbers convert 906 gigawatts to joules using the llm-math tool, since we asked for energy, not power Agent test example 2 LangChain agents aren’t limited to searching the Internet. We can connect practically any data source (including our own) to a LangChain agent and ask it questions about the data. Let’s try making an agent trained on a CSV dataset. Download this Netflix movies and TV shows dataset from SHIVAM BANSAL on Kaggle and move it into your directory. Now add this code into a new Python file: from langchain .llms import OpenAI from langchain .chat_models import ChatOpenAI from langchain .agents .agent_types import AgentType from langchain .agents import create_csv_agent import os os .environ \"OPENAI_API_KEY\" \"YOUR_OPENAI_API_KEY\" agent = create_csv_agent ( OpenAI (temperature \"netflix_titles.csv\" , verbose True , agent_type =AgentType .ZERO_SHOT_REACT_DESCRIPTION ) agent .run \"In how many movies was Christian Bale casted\" This code calls the create_csv_agent function and uses the netflix_titles.csv dataset. The image below shows our test. As shown above, its logic is to look in the cast column for all occurrences of “Christian Bale”.\n",
      "We can also make a Pandas Dataframe agent like this: from langchain .agents import create_pandas_dataframe_agent from langchain .chat_models import ChatOpenAI from langchain .agents .agent_types import AgentType from langchain .llms import OpenAI import pandas as pd import os os .environ \"OPENAI_API_KEY\" \"YOUR_OPENAI_KEY\" df = pd .read_csv \"netflix_titles.csv\" ) agent = create_pandas_dataframe_agent (OpenAI (temperature , df , verbose True ) agent .run \"In what year were the most comedy movies released?\" If we run it, we’ll see something like the results shown below. These are just a few examples. We can use practically any API or dataset with LangChain. Models There are three types of models in LangChain: LLMs, chat models, and text embedding models. Let’s explore every type of model with some examples. Language model LangChain provides a way to use language models in Python to produce text output based on text input. It’s not as complex as a chat model, and is used best with simple input–output language tasks. Here’s an example using OpenAI: from langchain .llms import OpenAI import os os .environ \"OPENAI_API_KEY\" \"YOUR_OPENAI_API_KEY\" llm = OpenAI (model \"gpt-3.5-turbo\" , temperature 0.9 print (llm \"Come up with a rap name for Matt Nikonorov\" As seen above, it uses the gpt-3.5-turbo model to generate an output for the provided input (“Come up with a rap name for Matt Nikonorov”). In this example, I’ve set the temperature to 0.9 to make the LLM really creative.\n",
      "It came up with “MC MegaMatt”. I’d give that one a solid 9/10. Chat model Making LLM models come up with rap names is fun, but if we want more sophisticated answers and conversations, we need to step up our game by using a chat model. How are chat models technically different from language models? Well, in the words of the LangChain documentation: Chat models are a variation on language models. While chat models use language models under the hood, the interface they use is a bit different. Rather than using a “text in, text out” API, they use an interface where “chat messages” are the inputs and outputs. Here’s a simple Python chat model script: from langchain .llms import OpenAI from langchain .chat_models import ChatOpenAI from langchain .schema import ( AIMessage , HumanMessage , SystemMessage import os os .environ \"OPENAI_API_KEY\" \"YOUR_OPENAI_API_KEY\" chat = ChatOpenAI ) messages [ SystemMessage (content \"You are a friendly, informal assistant\" , HumanMessage (content \"Convince me that Djokovic is better than Federer\" print (chat (messages As shown above, the code first sends a SystemMessage and tells the chatbot to be friendly and informal, and afterwards it sends a HumanMessage telling the chatbot to convince us that Djokovich is better than Federer. If you run this chatbot model, you’ll see something like the result shown below.\n",
      "Embeddings Embeddings provide a way to turn words and numbers in a block of text into vectors that can then be associated with other words or numbers. This may sound abstract, so let’s look at an example: from langchain .embeddings import OpenAIEmbeddings embeddings_model = OpenAIEmbeddings ) embedded_query = embeddings_model .embed_query \"Who created the world wide web?\" ) embedded_query This will return a list of floats: [0.022762885317206383, -0.01276398915797472, 0.004815981723368168, -0.009435392916202545, 0.010824492201209068]. This is what an embedding looks like. A use case of embedding models If we want to train a chatbot or LLM to answer questions related to our data or to a specific text sample, we need to use embeddings.\n",
      "Let’s make a simple CSV file (embs.csv) that has a “text” column containing three pieces of information: Now here’s a script that will take the question “Who was the tallest human ever?” and find the right answer in the CSV file by using embeddings: from langchain .embeddings import OpenAIEmbeddings from openai .embeddings_utils import cosine_similarity import os import pandas os .environ \"OPENAI_API_KEY\" \"YOUR_OPENAI_KEY\" embeddings_model = OpenAIEmbeddings ) df = pandas .read_csv \"embs.csv\" # Make embeddings for each piece of information emb1 = embeddings_model .embed_query (df \"text\" ) emb2 = embeddings_model .embed_query (df \"text\" ) emb3 = embeddings_model .embed_query (df \"text\" ) emb_list [emb1 , emb2 , emb3 ] df \"embedding\" = emb_list embedded_question = embeddings_model .embed_query \"Who was the tallest human ever?\" # Make an embedding for the question df \"similarity\" = df .embedding apply lambda x : cosine_similarity (x , embedded_question # Finds the relevance of each piece of data in context to the question df .to_csv \"embs.csv\" ) df2 = df .sort_values \"similarity\" , ascending False # Sorts the pieces of information by their relatedness to the question print (df2 \"text\" If we run this code, we’ll see that it outputs “Robert Wadlow was the tallest human ever”.\n",
      "The code finds the right answer by getting the embedding of each piece of information and finding the one most related to the embedding of the question “Who was the tallest human ever?” The power of embeddings! Chunks LangChain models can’t handle large texts at the same time and use them to make responses. This is where chunks and text splitting come in. Le’s look at two simple ways to split our text data into chunks before feeding it into LangChain. Splitting chunks by character To avoid abrupt breaks in chunks, we can split our texts by paragraphs by splitting them at every occurrence of a newline or double-newline: from langchain .text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter (separators \"\\n\\n\" \"\\n\" , chunk_size 2000 , chunk_overlap 250 ) texts = text_splitter .split_text (your_text Recursively splitting chunks If we want to strictly split our text by a certain length of characters, we can do so using RecursiveCharacterTextSplitter: from langchain .text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter ( chunk_size 2000 , chunk_overlap 250 , length_function len , add_start_index True ) texts = text_splitter .create_documents [your_text Chunk size and overlap While looking at the examples above, you may have wondered exactly what the chunk size and overlap parameters mean, and what implications they have on performance.\n",
      "That can be explained with two points: Chunk size decides the amount of characters that will be in each chunk. The bigger the chunk size, the more data is in the chunk, and the more time it will take LangChain to process it and to produce an output, and vice versa. Chunk overlap is what shares information between chunks so that they share some context. The higher the chunk overlap, the more redundant our chunks will be, the lower the chunk overlap, and the less context will be shared between the chunks. Generally, a good chunk overlap is between 10% and 20% of the chunk size, although the ideal chunk overlap varies across different text types and use cases. Chains Chains are basically multiple LLM functionalities linked together to perform more complex tasks that couldn’t otherwise be done with simple LLM input --> output fashion. Let’s look at a cool example: from langchain .llms import OpenAI from langchain .prompts import PromptTemplate from langchain .chains import LLMChain import os os .environ \"OPENAI_API_KEY\" \"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\" llm = OpenAI (temperature 0.9 ) prompt = PromptTemplate ( input_variables \"media\" \"topic\" , template \"What is a good title for a {media} about {topic}\" ) chain = LLMChain (llm =llm , prompt =prompt print (chain .run 'media' \"horror movie\" 'topic' \"math\" This code takes two variables into its prompt and formulates a creative answer (temperature=0.9).\n",
      "In this example, we’ve asked it to come up with a good title for a horror movie about math. The output after running this code was “The Calculating Curse”, but this doesn’t really show the full power of chains. Let’s take a look at a more practical example: from langchain .chat_models import ChatOpenAI from langchain .prompts import PromptTemplate from typing import Optional from langchain .chains .openai_functions import ( create_openai_fn_chain , create_structured_output_chain import os os .environ \"OPENAI_API_KEY\" \"YOUR_KEY\" llm = ChatOpenAI (model \"gpt-3.5-turbo\" , temperature 0.1 ) template \"\"\"Use the given format to extract information from the following input: {input}. Make sure to answer in the correct format\"\"\" prompt = PromptTemplate (template =template , input_variables \"input\" ) json_schema \"type\" \"object\" \"properties\" \"name\" \"title\" \"Name\" \"description\" \"The artist's name\" \"type\" \"string\" \"genre\" \"title\" \"Genre\" \"description\" \"The artist's music genre\" \"type\" \"string\" \"debut\" \"title\" \"Debut\" \"description\" \"The artist's debut album\" \"type\" \"string\" \"debut_year\" \"title\" \"Debut_year\" \"description\" \"Year of artist's debut album\" \"type\" \"integer\" \"required\" \"name\" \"genre\" \"debut\" \"debut_year\" } chain = create_structured_output_chain (json_schema , llm , prompt , verbose False ) f open \"Nas.txt\" \"r\" ) artist_info str (f .read print (chain .run (artist_info This code may look confusing, so let’s walk through it.\n",
      "This code reads a short biography of Nas (Hip-Hop Artist) and extracts the following values from the text and formats them into a JSON object: the artist’s name the artist’s music genre the artist’s debut album the year of artist’s debut album In the prompt we also specify “Make sure to answer in the correct format”, so that we always get the output in JSON format. Here’s the output of this code: 'name' 'Nas' 'genre' 'Hip Hop' 'debut' 'Illmatic' 'debut_year' 1994 By providing a JSON schema to the create_structured_output_chain function, we’ve made the chain put its output into JSON format. Going Beyond OpenAI Even though I keep using OpenAI models as examples of the different functionalities of LangChain, it isn’t limited to OpenAI models. We can use LangChain with a multitude of other LLMs and AI services. (Here’s a full list of LangChain integratable LLMs.) For example, we can use Cohere with LangChain. Here’s the documentation for the LangChain Cohere integration, but just to give a practical example, after installing Cohere using pip3 install cohere we can make a simple question --> answer code using LangChain and Cohere like this: from langchain .llms import Cohere from langchain .prompts import PromptTemplate from langchain .chains import LLMChain template \"\"\"Question: {question} Answer: Let's think step by step.\"\"\"\n",
      "prompt = PromptTemplate (template =template , input_variables \"question\" ) llm = Cohere (cohere_api_key \"YOUR_COHERE_KEY\" ) llm_chain = LLMChain (prompt =prompt , llm =llm ) question \"When was Novak Djokovic born?\" print (llm_chain .run (question The code above produces the following output: Conclusion In this guide, you’ve seen the different aspects and functionalities of LangChain. Armed with this knowledge, you’re now equipped to leverage LangChain’s capabilities for your NLP endeavors, whether you’re a researcher, developer, or hobbyist. You can find a repo with all the images and the Nas.txt file from this article on GitHub. Happy coding and experimenting with LangChain in Python! Share This Article Matt Nikonorov I'm a full-stack developer with 3 years of experience with PHP, Python, Javascript and CSS. I love blogging about web development, application development and machine learning. LangChain Up Next How to Analyze Large Text Datasets with LangChain and Python Matt Nikonorov LlamaIndex vs LangChain: Tools for Building LLM-powered Apps Dianne Pena A Complete Guide to LangChain in JavaScript Matt Nikonorov An Introduction to LangChain: AI-Powered Language Modeling Dianne Pena A Guide to Python Exception Handling Ini Arthur A Beginner’s Guide to HTTP Python Requests Lorenzo Bonannella\n",
      "\n",
      "Source: python.langchain.com_docs_langgraph.txt\n",
      "Content: LangGraph 🦜🕸️LangGraph ⚡ Building language agents as graphs ⚡ Overview​ LangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of (and intended to be used with) LangChain. It extends the LangChain Expression Language with the ability to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. It is inspired by Pregel and Apache Beam. The current interface exposed is one inspired by NetworkX. The main use is for adding cycles to your LLM application. Crucially, this is NOT a DAG framework. If you want to build a DAG, you should just use LangChain Expression Language. Cycles are important for agent-like behaviors, where you call an LLM in a loop, asking it what action to take next. Installation​ pip install langgraph Quick Start​ Here we will go over an example of creating a simple agent that uses chat models and function calling. This agent will represent all its state as a list of messages. We will need to install some LangChain packages, as well as Tavily to use as an example tool. pip install U langchain langchain_openai tavily-python We also need to export some environment variables for OpenAI and Tavily API access. export OPENAI_API_KEY sk- .. export TAVILY_API_KEY tvly- .. Optionally, we can set up LangSmith for best-in-class observability. export LANGCHAIN_TRACING_V2 \"true\" export LANGCHAIN_API_KEY ls__ .. Set up the tools​ We will first define the tools we want to use.\n",
      "For this simple example, we will use a built-in search tool via Tavily. However, it is really easy to create your own tools - see documentation here on how to do that. from langchain_community tools tavily_search import TavilySearchResults tools TavilySearchResults max_results We can now wrap these tools in a simple LangGraph ToolExecutor. This is a simple class that receives ToolInvocation objects, calls that tool, and returns the output. ToolInvocation is any class with tool and tool_input attributes. from langgraph prebuilt import ToolExecutor tool_executor ToolExecutor tools Set up the model​ Now we need to load the chat model we want to use. Importantly, this should satisfy two criteria: It should work with lists of messages. We will represent all agent state in the form of messages, so it needs to be able to work well with them. It should work with the OpenAI function calling interface. This means it should either be an OpenAI model or a model that exposes a similar interface. Note: these model requirements are not requirements for using LangGraph - they are just requirements for this one example. from langchain_openai import ChatOpenAI # We will set streaming=True so that we can stream tokens # See the streaming section for more information on this. model ChatOpenAI temperature streaming True After we've done this, we should make sure the model knows that it has these tools available to call.\n",
      "We can do this by converting the LangChain tools into the format for OpenAI function calling, and then bind them to the model class. from langchain tools render import format_tool_to_openai_function functions format_tool_to_openai_function for in tools model model bind_functions functions Define the agent state​ The main type of graph in langgraph is the StatefulGraph. This graph is parameterized by a state object that it passes around to each node. Each node then returns operations to update that state. These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute. Whether to set or add is denoted by annotating the state object you construct the graph with. For this example, the state we will track will just be a list of messages. We want each node to just add messages to that list. Therefore, we will use a TypedDict with one key (messages) and annotate it so that the messages attribute is always added to. from typing import TypedDict Annotated Sequence import operator from langchain_core messages import BaseMessage class AgentState TypedDict messages Annotated Sequence BaseMessage operator add Define the nodes​ We now need to define a few different nodes in our graph. In langgraph, a node can be either a function or a runnable. There are two main nodes we need for this: The agent: responsible for deciding what (if any) actions to take.\n",
      "A function to invoke tools: if the agent decides to take an action, this node will then execute that action. We will also need to define some edges. Some of these edges may be conditional. The reason they are conditional is that based on the output of a node, one of several paths may be taken. The path that is taken is not known until that node is run (the LLM decides). Conditional Edge: after the agent is called, we should either:a. If the agent said to take an action, then the function to invoke tools should be calledb. If the agent said that it was finished, then it should finish Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next Let's define the nodes, as well as a function to decide how what conditional edge to take.\n",
      "from langgraph prebuilt import ToolInvocation import json from langchain_core messages import FunctionMessage # Define the function that determines whether to continue or not def should_continue state messages state 'messages' last_message messages # If there is no function call, then we finish if \"function_call\" not in last_message additional_kwargs return \"end\" # Otherwise if there is, we continue else return \"continue\" # Define the function that calls the model def call_model state messages state 'messages' response model invoke messages # We return a list, because this will get added to the existing list return \"messages\" response # Define the function to execute tools def call_tool state messages state 'messages' # Based on the continue condition # we know the last message involves a function call last_message messages # We construct an ToolInvocation from the function_call action ToolInvocation tool last_message additional_kwargs \"function_call\" \"name\" tool_input json loads last_message additional_kwargs \"function_call\" \"arguments\" # We call the tool_executor and get back a response response tool_executor invoke action # We use the response to create a FunctionMessage function_message FunctionMessage content str response name action tool # We return a list, because this will get added to the existing list return \"messages\" function_message Define the graph​ We can now put it all together and define the graph!\n",
      "from langgraph graph import StateGraph END # Define a new graph workflow StateGraph AgentState # Define the two nodes we will cycle between workflow add_node \"agent\" call_model workflow add_node \"action\" call_tool # Set the entrypoint as `agent` # This means that this node is the first one called workflow set_entry_point \"agent\" # We now add a conditional edge workflow add_conditional_edges # First, we define the start node. We use `agent`. # This means these are the edges taken after the `agent` node is called. \"agent\" # Next, we pass in the function that will determine which node is called next. should_continue # Finally we pass in a mapping. # The keys are strings, and the values are other nodes. # END is a special node marking that the graph should finish. # What will happen is we will call `should_continue`, and then the output of that # will be matched against the keys in this mapping. # Based on which one it matches, that node will then be called. # If `tools`, then we call the tool node. \"continue\" \"action\" # Otherwise we finish. \"end\" END # We now add a normal edge from `tools` to `agent`. # This means that after `tools` is called, `agent` node is called next. workflow add_edge 'action' 'agent' # Finally, we compile it! # This compiles it into a LangChain Runnable, # meaning you can use it as you would any other runnable app workflow compile Use it!​ We can now use it! This now exposes the same interface as all other LangChain runnables.\n",
      "This runnable accepts a list of messages. from langchain_core messages import HumanMessage inputs \"messages\" HumanMessage content \"what is the weather in sf\" app invoke inputs This may take a little bit - it's making a few calls behind the scenes. In order to start seeing some intermediate results as they happen, we can use streaming - see below for more information on that. Streaming​ LangGraph has support for several different types of streaming. Streaming Node Output​ One of the benefits of using LangGraph is that it is easy to stream output as it's produced by each node.\n",
      "inputs \"messages\" HumanMessage content \"what is the weather in sf\" for output in app stream inputs # stream() yields dictionaries with output keyed by node name for key value in output items print f\"Output from node ' key ':\" print \"---\" print value print \"\\n---\\n\" Output from node 'agent': --- {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n \"query\": \"weather in San Francisco\"\\n}', 'name': 'tavily_search_results_json'}})]} --- Output from node 'action': --- {'messages': [FunctionMessage(content=\"[{'url': 'https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States', 'content': 'January 2024 Weather History in San Francisco California, United States Daily Precipitation in January 2024 in San Francisco Observed Weather in January 2024 in San Francisco San Francisco Temperature History January 2024 Hourly Temperature in January 2024 in San Francisco Hours of Daylight and Twilight in January 2024 in San FranciscoThis report shows the past weather for San Francisco, providing a weather history for January 2024. It features all historical weather data series we have available, including the San Francisco temperature history for January 2024. You can drill down from year to month and even day level reports by clicking on the graphs.\n",
      "'}]\", name='tavily_search_results_json')]} --- Output from node 'agent': --- {'messages': [AIMessage(content=\"I couldn't find the current weather in San Francisco. However, you can visit [WeatherSpark](https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States) to check the historical weather data for January 2024 in San Francisco.\")]} --- Output from node '__end__': --- {'messages': [HumanMessage(content='what is the weather in sf'), AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n \"query\": \"weather in San Francisco\"\\n}', 'name': 'tavily_search_results_json'}}), FunctionMessage(content=\"[{'url': 'https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States', 'content': 'January 2024 Weather History in San Francisco California, United States Daily Precipitation in January 2024 in San Francisco Observed Weather in January 2024 in San Francisco San Francisco Temperature History January 2024 Hourly Temperature in January 2024 in San Francisco Hours of Daylight and Twilight in January 2024 in San FranciscoThis report shows the past weather for San Francisco, providing a weather history for January 2024. It features all historical weather data series we have available, including the San Francisco temperature history for January 2024. You can drill down from year to month and even day level reports by clicking on the graphs.\n",
      "'}]\", name='tavily_search_results_json'), AIMessage(content=\"I couldn't find the current weather in San Francisco. However, you can visit [WeatherSpark](https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States) to check the historical weather data for January 2024 in San Francisco.\")]} --- Streaming LLM Tokens​ You can also access the LLM tokens as they are produced by each node. In this case only the \"agent\" node produces LLM tokens. In order for this to work properly, you must be using an LLM that supports streaming as well as have set it when constructing the LLM (e.g.\n",
      "ChatOpenAI(model=\"gpt-3.5-turbo-1106\", streaming=True)) inputs \"messages\" HumanMessage content \"what is the weather in sf\" async for output in app astream_log inputs include_types \"llm\" # astream_log() yields the requested logs (here LLMs) in JSONPatch format for op in output ops if op \"path\" == \"/streamed_output/-\" # this is the output from .stream() elif op \"path\" startswith \"/logs/\" and op \"path\" endswith \"/streamed_output/-\" # because we chose to only include LLMs, these are LLM tokens print op \"value\" content='' additional_kwargs={'function_call': {'arguments': '', 'name': 'tavily_search_results_json'}} content='' additional_kwargs={'function_call': {'arguments': '{\\n', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' ', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' \"', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': 'query', 'name':\n",
      "''}} content='' additional_kwargs={'function_call': {'arguments': '\":', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' \"', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': 'weather', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' in', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' San', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' Francisco', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': '\"\\n', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': '}', 'name': ''}} content='' content='' content='I' content=\"'m\" content=' sorry' content=',' content=' but' content=' I' content=' couldn' content=\"'t\" content=' find' content=' the' content=' current' content=' weather' content=' in' content=' San' content=' Francisco' content='.'\n",
      "content=' However' content=',' content=' you' content=' can' content=' check' content=' the' content=' historical' content=' weather' content=' data' content=' for' content=' January' content=' ' content='202' content='4' content=' in' content=' San' content=' Francisco' content=' [' content='here' content='](' content='https' content='://' content='we' content='athers' content='park' content='.com' content='/h' content='/m' content='/' content='557' content='/' content='202' content='4' content='/' content='1' content='/H' content='istorical' content='-' content='Weather' content='-in' content='-Jan' content='uary' content='-' content='202' content='4' content='-in' content='-S' content='an' content='-F' content='r' content='anc' content='isco' content='-Cal' content='ifornia' content='-' content='United' content='-' content='States' content=').' content='' When to Use​ When should you use this versus LangChain Expression Language? If you need cycles. Langchain Expression Language allows you to easily define chains (DAGs) but does not have a good mechanism for adding in cycles. langgraph adds that syntax. Examples​ ChatAgentExecutor: with function calling​ This agent executor takes a list of messages as input and outputs a list of messages. All agent state is represented as a list of messages. This specifically uses OpenAI function calling. This is recommended agent executor for newer chat based models that support function calling.\n",
      "Getting Started Notebook: Walks through creating this type of executor from scratch High Level Entrypoint: Walks through how to use the high level entrypoint for the chat agent executor. Modifications We also have a lot of examples highlighting how to slightly modify the base chat agent executor. These all build off the getting started notebook so it is recommended you start with that first. Human-in-the-loop: How to add a human-in-the-loop component Force calling a tool first: How to always call a specific tool first Respond in a specific format: How to force the agent to respond in a specific format Dynamically returning tool output directly: How to dynamically let the agent choose whether to return the result of a tool directly to the user Managing agent steps: How to more explicitly manage intermediate steps that an agent takes AgentExecutor​ This agent executor uses existing LangChain agents. Getting Started Notebook: Walks through creating this type of executor from scratch High Level Entrypoint: Walks through how to use the high level entrypoint for the chat agent executor. Modifications We also have a lot of examples highlighting how to slightly modify the base chat agent executor. These all build off the getting started notebook so it is recommended you start with that first.\n",
      "Human-in-the-loop: How to add a human-in-the-loop component Force calling a tool first: How to always call a specific tool first Managing agent steps: How to more explicitly manage intermediate steps that an agent takes Async​ If you are running LangGraph in async workflows, you may want to create the nodes to be async by default. For a walkthrough on how to do that, see this documentation Streaming Tokens​ Sometimes language models take a while to respond and you may want to stream tokens to end users. For a guide on how to do this, see this documentation Persistence​ LangGraph comes with built-in persistence, allowing you to save the state of the graph at point and resume from there. For a walkthrough on how to do that, see this documentation Human-in-the-loop​ LangGraph comes with built-in support for human-in-the-loop workflows. This is useful when you want to have a human review the current state before proceeding to a particular node. For a walkthrough on how to do that, see this documentation Planning Agent Examples​ The following notebooks implement agent architectures prototypical of the \"plan-and-execute\" style, where an LLM planner decomposes a user request into a program, an executor executes the program, and an LLM synthesizes a response (and/or dynamically replans) based on the program outputs.\n",
      "Plan-and-execute: a simple agent with a planner that generates a multi-step task list, an executor that invokes the tools in the plan, and a replanner that responds or generates an updated plan. Based on the Plan-and-solve paper by Wang, et. al. Reasoning without Observation: planner generates a task list whose observations are saved as variables. Variables can be used in subsequent tasks to reduce the need for further re-planning. Based on the ReWOO paper by Xu, et. al. LLMCompiler: planner generates a DAG of tasks with variable responses. Tasks are streamed and executed eagerly to minimize tool execution runtime. Based on the paper by Kim, et. al. Reflection / Self-Critique​ When output quality is a major concern, it's common to incorporate some combination of self-critique or reflection and external validation to refine your system's outputs. The following examples demonstrate research that implement this type of design. Basic Reflection: add a simple \"reflect\" step in your graph to prompt your system to revise its outputs. Reflexion: critique missing and superflous aspects of the agent's response to guide subsequent steps. Based on Reflexion, by Shinn, et. al. Language Agent Tree Search: execute multiple agents in parallel, using reflection and environmental rewards to drive a Monte Carlo Tree Search. Based on LATS, by Zhou, et. al.\n",
      "Multi-agent Examples​ Multi-agent collaboration: how to create two agents that work together to accomplish a task Multi-agent with supervisor: how to orchestrate individual agents by using an LLM as a \"supervisor\" to distribute work Hierarchical agent teams: how to orchestrate \"teams\" of agents as nested graphs that can collaborate to solve a problem Chatbot Evaluation via Simulation​ It can often be tough to evaluation chat bots in multi-turn situations. One way to do this is with simulations. Chat bot evaluation as multi-agent simulation: how to simulate a dialogue between a \"virtual user\" and your chat bot Multimodal Examples​ WebVoyager: vision-enabled web browsing agent that uses Set-of-marks prompting to navigate a web browser and execute tasks Documentation​ There are only a few new APIs to use. StateGraph​ The main entrypoint is StateGraph. from langgraph graph import StateGraph This class is responsible for constructing the graph. It exposes an interface inspired by NetworkX. This graph is parameterized by a state object that it passes around to each node. __init__​ def __init__ self schema Type Any None When constructing the graph, you need to pass in a schema for a state. Each node then returns operations to update that state. These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute. Whether to set or add is denoted by annotating the state object you construct the graph with.\n",
      "The recommended way to specify the schema is with a typed dictionary: from typing import TypedDict You can then annotate the different attributes using from typing imoport Annotated. Currently, the only supported annotation is import operator; operator.add. This annotation will make it so that any node that returns this attribute ADDS that new result to the existing value.\n",
      "Let's take a look at an example: from typing import TypedDict Annotated Union from langchain_core agents import AgentAction AgentFinish import operator class AgentState TypedDict # The input string input str # The outcome of a given call to the agent # Needs `None` as a valid type, since this is what this will start as agent_outcome Union AgentAction AgentFinish None # List of actions and corresponding observations # Here we annotate this with `operator.add` to indicate that operations to # this state should be ADDED to the existing values (not overwrite it) intermediate_steps Annotated list tuple AgentAction str operator add We can then use this like: # Initialize the StateGraph with this state graph StateGraph AgentState # Create nodes and edges # Compile the graph app graph compile # The inputs should be a dictionary, because the state is a TypedDict inputs # Let's assume this the input \"input\" \"hi\" # Let's assume agent_outcome is set by the graph as some point # It doesn't need to be provided, and it will be None by default # Let's assume `intermediate_steps` is built up over time by the graph # It doesn't need to provided, and it will be empty list by default # The reason `intermediate_steps` is an empty list and not `None` is because # it's annotated with `operator.add` .add_node​ def add_node self key str action RunnableLike None This method adds a node to the graph. It takes two arguments: key: A string representing the name of the node. This must be unique.\n",
      "action: The action to take when this node is called. This should either be a function or a runnable. .add_edge​ def add_edge self start_key str end_key str None Creates an edge from one node to the next. This means that output of the first node will be passed to the next node. It takes two arguments. start_key: A string representing the name of the start node. This key must have already been registered in the graph. end_key: A string representing the name of the end node. This key must have already been registered in the graph. .add_conditional_edges​ def add_conditional_edges self start_key str condition Callable str conditional_edge_mapping Dict str str None This method adds conditional edges. What this means is that only one of the downstream edges will be taken, and which one that is depends on the results of the start node. This takes three arguments: start_key: A string representing the name of the start node. This key must have already been registered in the graph. condition: A function to call to decide what to do next. The input will be the output of the start node. It should return a string that is present in conditional_edge_mapping and represents the edge to take. conditional_edge_mapping: A mapping of string to string. The keys should be strings that may be returned by condition. The values should be the downstream node to call if that condition is returned. .set_entry_point​ def set_entry_point self key str None The entrypoint to the graph.\n",
      "This is the node that is first called. It only takes one argument: key: The name of the node that should be called first. .set_finish_point​ def set_finish_point self key str None This is the exit point of the graph. When this node is called, the results will be the final result from the graph. It only has one argument: key: The name of the node that, when called, will return the results of calling it as the final output Note: This does not need to be called if at any point you previously created an edge (conditional or normal) to END Graph​ from langgraph graph import Graph graph Graph This has the same interface as StateGraph with the exception that it doesn't update a state object over time, and rather relies on passing around the full state from each step. This means that whatever is returned from one node is the input to the next as is. END​ from langgraph graph import END This is a special node representing the end of the graph. This means that anything passed to this node will be the final output of the graph. It can be used in two places: As the end_key in add_edge As a value in conditional_edge_mapping as passed to add_conditional_edges Prebuilt Examples​ There are also a few methods we've added to make it easy to use common, prebuilt graphs and components. ToolExecutor​ from langgraph prebuilt import ToolExecutor This is a simple helper class to help with calling tools.\n",
      "It is parameterized by a list of tools: tools tool_executor ToolExecutor tools It then exposes a runnable interface. It can be used to call tools: you can pass in an AgentAction and it will look up the relevant tool and call it with the appropriate input. chat_agent_executor.create_function_calling_executor​ from langgraph prebuilt import chat_agent_executor This is a helper function for creating a graph that works with a chat model that utilizes function calling. Can be created by passing in a model and a list of tools. The model must be one that supports OpenAI function calling. from langchain_openai import ChatOpenAI from langchain_community tools tavily_search import TavilySearchResults from langgraph prebuilt import chat_agent_executor from langchain_core messages import HumanMessage tools TavilySearchResults max_results model ChatOpenAI app chat_agent_executor create_function_calling_executor model tools inputs \"messages\" HumanMessage content \"what is the weather in sf\" for in app stream inputs print list values print \"----\" create_agent_executor​ from langgraph prebuilt import create_agent_executor This is a helper function for creating a graph that works with LangChain Agents. Can be created by passing in an agent and a list of tools.\n",
      "from langgraph prebuilt import create_agent_executor from langchain_openai import ChatOpenAI from langchain import hub from langchain agents import create_openai_functions_agent from langchain_community tools tavily_search import TavilySearchResults tools TavilySearchResults max_results # Get the prompt to use - you can modify this! prompt hub pull \"hwchase17/openai-functions-agent\" # Choose the LLM that will drive the agent llm ChatOpenAI model \"gpt-3.5-turbo-1106\" # Construct the OpenAI Functions agent agent_runnable create_openai_functions_agent llm tools prompt app create_agent_executor agent_runnable tools inputs \"input\" \"what is the weather in sf\" \"chat_history\" for in app stream inputs print list values print \"----\" PreviousLangSmith Walkthrough Overview Installation Quick StartSet up the toolsSet up the modelDefine the agent stateDefine the nodesDefine the graphUse it! StreamingStreaming Node OutputStreaming LLM Tokens When to Use ExamplesChatAgentExecutor: with function callingAgentExecutorAsyncStreaming TokensPersistenceHuman-in-the-loopPlanning Agent ExamplesReflection / Self-CritiqueMulti-agent ExamplesChatbot Evaluation via SimulationMultimodal Examples DocumentationStateGraphGraphEND Prebuilt ExamplesToolExecutorchat_agent_executor.create_function_calling_executorcreate_agent_executor\n",
      "\n",
      "Source: python.langchain.com_docs_langserve.txt\n",
      "Content: LangServe 🦜️🏓 LangServe 🚩 We will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. Sign up here to get on the waitlist. Overview​ LangServe helps developers deploy LangChain runnables and chains as a REST API. This library is integrated with FastAPI and uses pydantic for data validation. In addition, it provides a client that can be used to call into runnables deployed on a server. A javascript client is available in LangChainJS. Features​ Input and Output schemas automatically inferred from your LangChain object, and enforced on every API call, with rich error messages API docs page with JSONSchema and Swagger (insert example link) Efficient /invoke/, /batch/ and /stream/ endpoints with support for many concurrent requests on a single server /stream_log/ endpoint for streaming all (or some) intermediate steps from your chain/agent new as of 0.0.40, supports astream_events to make it easier to stream without needing to parse the output of stream_log. Playground page at /playground/ with streaming output and intermediate steps Built-in (optional) tracing to LangSmith, just add your API key (see Instructions) All built with battle-tested open-source Python libraries like FastAPI, Pydantic, uvloop and asyncio.\n",
      "Use the client SDK to call a LangServe server as if it was a Runnable running locally (or call the HTTP API directly) LangServe Hub Limitations​ Client callbacks are not yet supported for events that originate on the server OpenAPI docs will not be generated when using Pydantic V2. Fast API does not support mixing pydantic v1 and v2 namespaces. See section below for more details. Hosted LangServe​ We will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. Sign up here to get on the waitlist. Security​ Vulnerability in Versions 0.0.13 - 0.0.15 -- playground endpoint allows accessing arbitrary files on server. Resolved in 0.0.16. Installation​ For both client and server: pip install \"langserve[all]\" or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code. LangChain CLI 🛠️​ Use the LangChain CLI to bootstrap a LangServe project quickly. To use the langchain CLI make sure that you have a recent version of langchain-cli installed. You can install it with pip install -U langchain-cli. langchain app new ../path/to/directory Examples​ Get your LangServe instance started quickly with LangChain Templates. For more examples, see the templates index or the examples directory. Description Links LLMs Minimal example that reserves OpenAI and Anthropic chat models. Uses async, supports batching and streaming. server , client Retriever Simple server that exposes a retriever as a runnable.\n",
      "server , client Conversational Retriever A Conversational Retriever exposed via LangServe server , client Agent without conversation history based on OpenAI tools server , client Agent with conversation history based on OpenAI tools server , client RunnableWithMessageHistory to implement chat persisted on backend, keyed off a session_id supplied by client. server , client RunnableWithMessageHistory to implement chat persisted on backend, keyed off a conversation_id supplied by client, and user_id (see Auth for implementing user_id properly). server , client Configurable Runnable to create a retriever that supports run time configuration of the index name. server , client Configurable Runnable that shows configurable fields and configurable alternatives. server , client APIHandler Shows how to use APIHandler instead of add_routes . This provides more flexibility for developers to define endpoints. Works well with all FastAPI patterns, but takes a bit more effort. server LCEL Example Example that uses LCEL to manipulate a dictionary input. server , client Auth with add_routes : Simple authentication that can be applied across all endpoints associated with app. (Not useful on its own for implementing per user logic.) server Auth with add_routes : Simple authentication mechanism based on path dependencies. (No useful on its own for implementing per user logic.) server Auth with add_routes : Implement per user logic and auth for endpoints that use per request config modifier.\n",
      "( Note : At the moment, does not integrate with OpenAPI docs.) server , client Auth with APIHandler : Implement per user logic and auth that shows how to search only within user owned documents. server , client Widgets Different widgets that can be used with playground (file upload and chat) server Widgets File upload widget used for LangServe playground. server , client Sample Application​ Server​ Here's a server that deploys an OpenAI chat model, an Anthropic chat model, and a chain that uses the Anthropic model to tell a joke about a topic. #!/usr/bin/env python from fastapi import FastAPI from langchain prompts import ChatPromptTemplate from langchain chat_models import ChatAnthropic ChatOpenAI from langserve import add_routes app FastAPI title \"LangChain Server\" version \"1.0\" description \"A simple api server using Langchain's Runnable interfaces\" add_routes app ChatOpenAI path \"/openai\" add_routes app ChatAnthropic path \"/anthropic\" model ChatAnthropic prompt ChatPromptTemplate from_template \"tell me a joke about {topic}\" add_routes app prompt model path \"/joke\" if __name__ == \"__main__\" import uvicorn uvicorn run app host \"localhost\" port 8000 Docs​ If you've deployed the server above, you can view the generated OpenAPI docs using: ⚠️ If using pydantic v2, docs will not be generated for invoke, batch, stream, stream_log. See Pydantic section below for more details. curl localhost:8000/docs make sure to add the /docs suffix.\n",
      "⚠️ Index page / is not defined by design, so curl localhost:8000 or visiting the URL will return a 404. If you want content at / define an endpoint @app.get(\"/\"). Client​ Python SDK from langchain schema import SystemMessage HumanMessage from langchain prompts import ChatPromptTemplate from langchain schema runnable import RunnableMap from langserve import RemoteRunnable openai RemoteRunnable \"http://localhost:8000/openai/\" anthropic RemoteRunnable \"http://localhost:8000/anthropic/\" joke_chain RemoteRunnable \"http://localhost:8000/joke/\" joke_chain invoke \"topic\" \"parrots\" # or async await joke_chain ainvoke \"topic\" \"parrots\" prompt SystemMessage content 'Act like either a cat or a parrot.' HumanMessage content 'Hello!'\n",
      "# Supports astream async for msg in anthropic astream prompt print msg end \"\" flush True prompt ChatPromptTemplate from_messages \"system\" \"Tell me a long story about {topic}\" # Can define custom chains chain prompt RunnableMap \"openai\" openai \"anthropic\" anthropic chain batch \"topic\" \"parrots\" \"topic\" \"cats\" In TypeScript (requires LangChain.js version 0.0.166 or later): import RemoteRunnable from \"langchain/runnables/remote\" const chain new RemoteRunnable url http://localhost:8000/joke/ const result await chain invoke topic \"cats\" Python using requests: import requests response requests post \"http://localhost:8000/joke/invoke\" json 'input' 'topic' 'cats' response json You can also use curl: curl --location --request POST 'http://localhost:8000/joke/invoke' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"input\": { \"topic\": \"cats\" }' Endpoints​ The following code: add_routes app runnable path \"/my_runnable\" adds of these endpoints to the server: POST /my_runnable/invoke - invoke the runnable on a single input POST /my_runnable/batch - invoke the runnable on a batch of inputs POST /my_runnable/stream - invoke on a single input and stream the output POST /my_runnable/stream_log - invoke on a single input and stream the output, including output of intermediate steps as it's generated POST /my_runnable/astream_events - invoke on a single input and stream events as they are generated, including from intermediate steps.\n",
      "GET /my_runnable/input_schema - json schema for input to the runnable GET /my_runnable/output_schema - json schema for output of the runnable GET /my_runnable/config_schema - json schema for config of the runnable These endpoints match the LangChain Expression Language interface -- please reference this documentation for more details. Playground​ You can find a playground page for your runnable at /my_runnable/playground/. This exposes a simple UI to configure and invoke your runnable with streaming output and intermediate steps. Widgets​ The playground supports widgets and can be used to test your runnable with different inputs. See the widgets section below for more details. Sharing​ In addition, for configurable runnables, the playground will allow you to configure the runnable and share a link with the configuration: Legacy Chains​ LangServe works with both Runnables (constructed via LangChain Expression Language) and legacy chains (inheriting from Chain). However, some of the input schemas for legacy chains may be incomplete/incorrect, leading to errors. This can be fixed by updating the input_schema property of those chains in LangChain. If you encounter any errors, please open an issue on THIS repo, and we will work to address it. Deployment​ Deploy to AWS​ You can deploy to AWS using the AWS Copilot CLI copilot init --app application-name --name service-name --type 'Load Balanced Web Service' --dockerfile './Dockerfile' --deploy Click here to learn more.\n",
      "Deploy to Azure​ You can deploy to Azure using Azure Container Apps (Serverless): az containerapp up --name [container-app-name] --source . --resource-group [resource-group-name] --environment [environment-name] --ingress external --target-port 8001 --env-vars=OPENAI_API_KEY=your_key You can find more info here Deploy to GCP​ You can deploy to GCP Cloud Run using the following command: gcloud run deploy [your-service-name] --source . --port 8001 --allow-unauthenticated --region us-central1 --set-env-vars=OPENAI_API_KEY=your_key Community Contributed​ Deploy to Railway​ Example Railway Repo Pydantic​ LangServe provides support for Pydantic 2 with some limitations. OpenAPI docs will not be generated for invoke/batch/stream/stream_log when using Pydantic V2. Fast API does not support [mixing pydantic v1 and v2 namespaces]. LangChain uses the v1 namespace in Pydantic v2. Please read the following guidelines to ensure compatibility with LangChain Except for these limitations, we expect the API endpoints, the playground and any other features to work as expected. Advanced​ Handling Authentication​ If you need to add authentication to your server, please read Fast API's documentation about dependencies and security. The below examples show how to wire up authentication logic LangServe endpoints using FastAPI primitives. You are responsible for providing the actual authentication logic, the users table etc.\n",
      "If you're not sure what you're doing, you could try using an existing solution Auth0. Using add_routes​ If you're using add_routes, see examples here. Description Links Auth with add_routes : Simple authentication that can be applied across all endpoints associated with app. (Not useful on its own for implementing per user logic.) server Auth with add_routes : Simple authentication mechanism based on path dependencies. (No useful on its own for implementing per user logic.) server Auth with add_routes : Implement per user logic and auth for endpoints that use per request config modifier. ( Note : At the moment, does not integrate with OpenAPI docs.) server , client Alternatively, you can use FastAPI's middleware. Using global dependencies and path dependencies has the advantage that auth will be properly supported in the OpenAPI docs page, but these are not sufficient for implement per user logic (e.g., making an application that can search only within user owned documents). If you need to implement per user logic, you can use the per_req_config_modifier or APIHandler (below) to implement this logic. Per User If you need authorization or logic that is user dependent, specify per_req_config_modifier when using add_routes. Use a callable receives the raw Request object and can extract relevant information from it for authentication and authorization purposes. Using APIHandler​ If you feel comfortable with FastAPI and python, you can use LangServe's APIHandler.\n",
      "Description Links Auth with APIHandler : Implement per user logic and auth that shows how to search only within user owned documents. server , client APIHandler Shows how to use APIHandler instead of add_routes . This provides more flexibility for developers to define endpoints. Works well with all FastAPI patterns, but takes a bit more effort. server , client It's a bit more work, but gives you complete control over the endpoint definitions, so you can do whatever custom logic you need for auth. Files​ LLM applications often deal with files. There are different architectures that can be made to implement file processing; at a high level: The file may be uploaded to the server via a dedicated endpoint and processed using a separate endpoint The file may be uploaded by either value (bytes of file) or reference (e.g., s3 url to file content) The processing endpoint may be blocking or non-blocking If significant processing is required, the processing may be offloaded to a dedicated process pool You should determine what is the appropriate architecture for your application. Currently, to upload files by value to a runnable, use base64 encoding for the file (multipart/form-data is not supported yet). Here's an example that shows how to use base64 encoding to send a file to a remote runnable. Remember, you can always upload files by reference (e.g., s3 url) or upload them as multipart/form-data to a dedicated endpoint.\n",
      "Custom Input and Output Types​ Input and Output types are defined on all runnables. You can access them via the input_schema and output_schema properties. LangServe uses these types for validation and documentation. If you want to override the default inferred types, you can use the with_types method. Here's a toy example to illustrate the idea: from typing import Any from fastapi import FastAPI from langchain schema runnable import RunnableLambda app FastAPI def func Any int \"\"\"Mistyped function that should accept an int but accepts anything.\"\"\" return runnable RunnableLambda func with_types input_type int add_routes app runnable Custom User Types​ Inherit from CustomUserType if you want the data to de-serialize into a pydantic model rather than the equivalent dict representation. At the moment, this type only works server side and is used to specify desired decoding behavior. If inheriting from this type the server will keep the decoded type as a pydantic model instead of converting it into a dict. from fastapi import FastAPI from langchain schema runnable import RunnableLambda from langserve import add_routes from langserve schema import CustomUserType app FastAPI class Foo CustomUserType bar int def func foo Foo int \"\"\"Sample function that expects a Foo type which is a pydantic model\"\"\" assert isinstance foo Foo return foo bar # Note that the input and output type are automatically inferred! # You do not need to specify them.\n",
      "# runnable = RunnableLambda(func).with_types( # <-- Not needed in this case # input_type=Foo, # output_type=int, add_routes app RunnableLambda func path \"/foo\" Playground Widgets​ The playground allows you to define custom widgets for your runnable from the backend. Here are a few examples: Description Links Widgets Different widgets that can be used with playground (file upload and chat) server , client Widgets File upload widget used for LangServe playground. server , client Schema​ A widget is specified at the field level and shipped as part of the JSON schema of the input type A widget must contain a key called type with the value being one of a well known list of widgets Other widget keys will be associated with values that describe paths in a JSON object type JsonPath number string number string type NameSpacedPath title string path JsonPath // Using title to mimick json schema, but can use namespace type OneOfPath oneOf JsonPath type Widget type string // Some well known type (e.g., base64file, chat etc.) key string JsonPath NameSpacedPath OneOfPath Available Widgets​ There are only two widgets that the user can specify manually right now: File Upload Widget Chat History Widget See below more information about these widgets. All other widgets on the playground UI are created and managed automatically by the UI based on the config schema of the Runnable.\n",
      "When you create Configurable Runnables, the playground should create appropriate widgets for you to control the behavior. File Upload Widget​ Allows creation of a file upload input in the UI playground for files that are uploaded as base64 encoded strings. Here's the full example. Snippet: try from pydantic v1 import Field except ImportError from pydantic import Field from langserve import CustomUserType # ATTENTION: Inherit from CustomUserType instead of BaseModel otherwise # the server will decode it into a dict instead of a pydantic model. class FileProcessingRequest CustomUserType \"\"\"Request including a base64 encoded file.\"\"\" # The extra field is used to specify a widget for the playground UI. file str Field extra \"widget\" \"type\" \"base64file\" num_chars int 100 Example widget: Chat Widget​ Look at widget example. To define a chat widget, make sure that you pass \"type\": \"chat\". \"input\" is JSONPath to the field in the Request that has the new input message. \"output\" is JSONPath to the field in the Response that has new output message(s). Don't specify these fields if the entire input or output should be used as they are ( e.g., if the output is a list of chat messages.) Here's a snippet: class ChatHistory CustomUserType chat_history List Tuple str str Field examples \"human input\" \"ai response\" extra \"widget\" \"type\" \"chat\" \"input\" \"question\" \"output\" \"answer\" question str def _format_to_messages input ChatHistory List BaseMessage \"\"\"Format the input to a list of messages.\"\"\"\n",
      "history input chat_history user_input input question messages for human ai in history messages append HumanMessage content human messages append AIMessage content ai messages append HumanMessage content user_input return messages model ChatOpenAI chat_model RunnableParallel \"answer\" RunnableLambda _format_to_messages model add_routes app chat_model with_types input_type ChatHistory config_keys \"configurable\" path \"/chat\" Example widget: Enabling / Disabling Endpoints (LangServe >=0.0.33)​ You can enable / disable which endpoints are exposed when adding routes for a given chain. Use enabled_endpoints if you want to make sure to never get a new endpoint when upgrading langserve to a newer verison. Enable: The code below will only enable invoke, batch and the corresponding config_hash endpoint variants.\n",
      "add_routes app chain enabled_endpoints \"invoke\" \"batch\" \"config_hashes\" path \"/mychain\" Disable: The code below will disable the playground for the chain add_routes app chain disabled_endpoints \"playground\" path \"/mychain\" PreviousToken counting NextLangSmith Overview Features Limitations Hosted LangServe Security Installation LangChain CLI 🛠️ Examples Sample ApplicationServerDocsClient Endpoints PlaygroundWidgetsSharing Legacy Chains DeploymentDeploy to AWSDeploy to AzureDeploy to GCPCommunity Contributed Pydantic AdvancedHandling AuthenticationFilesCustom Input and Output TypesCustom User TypesPlayground WidgetsAvailable WidgetsChat WidgetEnabling / Disabling Endpoints (LangServe >=0.0.33)\n",
      "\n",
      "Source: python.langchain.com_docs_expression_language_interface.txt\n",
      "Content: LangChain Expression Language Interface Interface To make it as easy as possible to create custom chains, we’ve implemented a “Runnable” protocol. The Runnable protocol is implemented for most components. This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way.\n",
      "The standard interface includes: stream: stream back chunks of the response invoke: call the chain on an input batch: call the chain on a list of inputs These also have corresponding async methods: astream: stream back chunks of the response async ainvoke: call the chain on an input async abatch: call the chain on a list of inputs async astream_log: stream back intermediate steps as they happen, in addition to the final response astream_events: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14) The input type and output type varies by component: Component Input Type Output Type Prompt Dictionary PromptValue ChatModel Single string, list of chat messages or a PromptValue ChatMessage LLM Single string, list of chat messages or a PromptValue String OutputParser The output of an LLM or ChatModel Depends on the parser Retriever Single string List of Documents Tool Single string or dictionary, depending on the tool Depends on the tool All runnables expose input and output schemas to inspect the inputs and outputs: - input_schema: an input Pydantic model auto-generated from the structure of the Runnable - output_schema: an output Pydantic model auto-generated from the structure of the Runnable Let’s take a look at these methods. To do so, we’ll create a super simple PromptTemplate + ChatModel chain.\n",
      "%pip install –upgrade –quiet langchain-core langchain-community langchain-openai from langchain_core prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI model ChatOpenAI prompt ChatPromptTemplate from_template \"tell me a joke about {topic}\" chain prompt model Input Schema​ A description of the inputs accepted by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation. # The input schema of the chain is the input schema of its first part, the prompt. chain input_schema schema {'title': 'PromptInput', 'type': 'object', 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}} prompt input_schema schema {'title': 'PromptInput', 'type': 'object', 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}} model input_schema schema {'title': 'ChatOpenAIInput', 'anyOf': [{'type': 'string'}, {'$ref': '#/definitions/StringPromptValue'}, {'$ref': '#/definitions/ChatPromptValueConcrete'}, {'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}, {'$ref': '#/definitions/ToolMessage'}]}}], 'definitions': {'StringPromptValue': {'title': 'StringPromptValue', 'description': 'String prompt value.\n",
      "', 'type': 'object', 'properties': {'text': {'title': 'Text', 'type': 'string'}, 'type': {'title': 'Type', 'default': 'StringPromptValue', 'enum': ['StringPromptValue'], 'type': 'string'}}, 'required': ['text']}, 'AIMessage': {'title': 'AIMessage', 'description': 'A Message from an AI. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'HumanMessage': {'title': 'HumanMessage', 'description': 'A Message from a human. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'human', 'enum': ['human'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'ChatMessage': {'title': 'ChatMessage', 'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).\n",
      "', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'chat', 'enum': ['chat'], 'type': 'string'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role']}, 'SystemMessage': {'title': 'SystemMessage', 'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'system', 'enum': ['system'], 'type': 'string'}}, 'required': ['content']}, 'FunctionMessage': {'title': 'FunctionMessage', 'description': 'A Message for passing the result of executing a function back to a model.\n",
      "', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'function', 'enum': ['function'], 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['content', 'name']}, 'ToolMessage': {'title': 'ToolMessage', 'description': 'A Message for passing the result of executing a tool back to a model. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'tool', 'enum': ['tool'], 'type': 'string'}, 'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}}, 'required': ['content', 'tool_call_id']}, 'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete', 'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.\n",
      "', 'type': 'object', 'properties': {'messages': {'title': 'Messages', 'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}, {'$ref': '#/definitions/ToolMessage'}]}}, 'type': {'title': 'Type', 'default': 'ChatPromptValueConcrete', 'enum': ['ChatPromptValueConcrete'], 'type': 'string'}}, 'required': ['messages']}}} Output Schema​ A description of the outputs produced by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation. # The output schema of the chain is the output schema of its last part, in this case a ChatModel, which outputs a ChatMessage chain output_schema schema {'title': 'ChatOpenAIOutput', 'anyOf': [{'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}, {'$ref': '#/definitions/ToolMessage'}], 'definitions': {'AIMessage': {'title': 'AIMessage', 'description': 'A Message from an AI.\n",
      "', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'HumanMessage': {'title': 'HumanMessage', 'description': 'A Message from a human. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'human', 'enum': ['human'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'ChatMessage': {'title': 'ChatMessage', 'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).\n",
      "', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'chat', 'enum': ['chat'], 'type': 'string'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role']}, 'SystemMessage': {'title': 'SystemMessage', 'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'system', 'enum': ['system'], 'type': 'string'}}, 'required': ['content']}, 'FunctionMessage': {'title': 'FunctionMessage', 'description': 'A Message for passing the result of executing a function back to a model.\n",
      "', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'function', 'enum': ['function'], 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['content', 'name']}, 'ToolMessage': {'title': 'ToolMessage', 'description': 'A Message for passing the result of executing a tool back to a model. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'anyOf': [{'type': 'string'}, {'type': 'array', 'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'tool', 'enum': ['tool'], 'type': 'string'}, 'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}}, 'required': ['content', 'tool_call_id']}}} Stream​ for in chain stream \"topic\" \"bears\" print content end \"\" flush True Sure, here's a bear-themed joke for you: Why don't bears wear shoes? Because they already have bear feet! Invoke​ chain invoke \"topic\" \"bears\" AIMessage(content=\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\") Batch​ chain batch \"topic\" \"bears\" \"topic\" \"cats\" [AIMessage(content=\"Sure, here's a bear joke for you:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they already have bear feet!\n",
      "\"), AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\")] You can set the number of concurrent requests by using the max_concurrency parameter chain batch \"topic\" \"bears\" \"topic\" \"cats\" config \"max_concurrency\" [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet! \"), AIMessage(content=\"Why don't cats play poker in the wild? Too many cheetahs!\")] Async Stream​ async for in chain astream \"topic\" \"bears\" print content end \"\" flush True Why don't bears wear shoes? Because they have bear feet! Async Invoke​ await chain ainvoke \"topic\" \"bears\" AIMessage(content=\"Why don't bears ever wear shoes?\\n\\nBecause they already have bear feet!\") Async Batch​ await chain abatch \"topic\" \"bears\" [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\")] Async Stream Events (beta)​ Event Streaming is a beta API, and may change a bit based on feedback. Note: Introduced in langchain-core 0.2.0 For now, when using the astream_events API, for everything to work properly please: Use async throughout the code (including async tools etc) Propagate callbacks if defining custom functions / runnables. Whenever using runnables without LCEL, make sure to call .astream() on LLMs rather than .ainvoke to force the LLM to stream tokens. Event Reference​ Here is a reference table that shows some events that might be emitted by the various Runnable objects. Definitions for some of the Runnable are included after the table.\n",
      "⚠️ When streaming the inputs for the runnable will not be available until the input stream has been entirely consumed This means that the inputs will be available at for the corresponding end hook rather than start event. event name chunk input output on_chat_model_start [model name] {“messages”: [[SystemMessage, HumanMessage]]} on_chat_model_stream [model name] AIMessageChunk(content=“hello”) on_chat_model_end [model name] {“messages”: [[SystemMessage, HumanMessage]]} {“generations”: […], “llm_output”: None, …} on_llm_start [model name] {‘input’: ‘hello’} on_llm_stream [model name] ‘Hello’ on_llm_end [model name] ‘Hello human!’ on_chain_start format_docs on_chain_stream format_docs “hello world!, goodbye world!” on_chain_end format_docs [Document(…)] “hello world!, goodbye world!” on_tool_start some_tool {“x”: 1, “y”: “2”} on_tool_stream some_tool {“x”: 1, “y”: “2”} on_tool_end some_tool {“x”: 1, “y”: “2”} on_retriever_start [retriever name] {“query”: “hello”} on_retriever_chunk [retriever name] {documents: […]} on_retriever_end [retriever name] {“query”: “hello”} {documents: […]} on_prompt_start [template_name] {“question”: “hello”} on_prompt_end [template_name] {“question”: “hello”} ChatPromptValue(messages: [SystemMessage, …]) Here are declarations associated with the events shown above: format_docs: def format_docs docs List Document str '''Format the docs.'''\n",
      "return \", \" join doc page_content for doc in docs format_docs RunnableLambda format_docs some_tool: @tool def some_tool int str dict '''Some_tool.''' return \"x\" \"y\" prompt: template ChatPromptTemplate from_messages \"system\" \"You are Cat Agent 007\" \"human\" \"{question}\" with_config \"run_name\" \"my_template\" \"tags\" \"my_template\" Let’s define a new chain to make it more interesting to show off the astream_events interface (and later the astream_log interface). from langchain_community vectorstores import FAISS from langchain_core output_parsers import StrOutputParser from langchain_core runnables import RunnablePassthrough from langchain_openai import OpenAIEmbeddings template \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\" prompt ChatPromptTemplate from_template template vectorstore FAISS from_texts \"harrison worked at kensho\" embedding OpenAIEmbeddings retriever vectorstore as_retriever retrieval_chain \"context\" retriever with_config run_name \"Docs\" \"question\" RunnablePassthrough prompt model with_config run_name \"my_llm\" StrOutputParser Now let’s use astream_events to get events from the retriever and the LLM. async for event in retrieval_chain astream_events \"where did harrison work?\"\n",
      "version \"v1\" include_names \"Docs\" \"my_llm\" kind event \"event\" if kind == \"on_chat_model_stream\" print event \"data\" \"chunk\" content end \"|\" elif kind in \"on_chat_model_start\" print print \"Streaming LLM:\" elif kind in \"on_chat_model_end\" print print \"Done streaming LLM.\" elif kind == \"on_retriever_end\" print \"--\" print \"Retrieved the following documents:\" print event \"data\" \"output\" \"documents\" elif kind == \"on_tool_end\" print f\"Ended tool: event 'name' else pass /home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future. warn_beta( -- Retrieved the following documents: [Document(page_content='harrison worked at kensho')] Streaming LLM: |H|arrison| worked| at| Kens|ho|.|| Done streaming LLM. Async Stream Intermediate Steps​ All runnables also have a method .astream_log() which is used to stream (as they happen) all or part of the intermediate steps of your chain/sequence. This is useful to show progress to the user, to use intermediate results, or to debug your chain. You can stream all steps (default) or include/exclude steps by name, tags or metadata. This method yields JSONPatch ops that when applied in the same order as received build up the RunState. class LogEntry TypedDict id str \"\"\"ID of the sub-run.\"\"\" name str \"\"\"Name of the object being run.\"\"\" type str \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\" tags List str \"\"\"List of tags for the run.\"\"\"\n",
      "metadata Dict str Any \"\"\"Key-value pairs of metadata for the run.\"\"\" start_time str \"\"\"ISO-8601 timestamp of when the run started.\"\"\" streamed_output_str List str \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\" final_output Optional Any \"\"\"Final output of this run. Only available after the run has finished successfully.\"\"\" end_time Optional str \"\"\"ISO-8601 timestamp of when the run ended. Only available after the run has finished.\"\"\" class RunState TypedDict id str \"\"\"ID of the run.\"\"\" streamed_output List Any \"\"\"List of output chunks streamed by Runnable.stream()\"\"\" final_output Optional Any \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output. Only available after the run has finished successfully.\"\"\" logs Dict str LogEntry \"\"\"Map of run names to sub-runs. If filters were supplied, this list will contain only the runs that matched the filters.\"\"\" Streaming JSONPatch chunks​ This is useful eg. to stream the JSONPatch in an HTTP server, and then apply the ops on the client to rebuild the run state there. See LangServe for tooling to make it easier to build a webserver from any Runnable. async for chunk in retrieval_chain astream_log \"where did harrison work?\"\n",
      "include_names \"Docs\" print \"-\" 40 print chunk ---------------------------------------- RunLogPatch({'op': 'replace', 'path': '', 'value': {'final_output': None, 'id': '82e9b4b1-3dd6-4732-8db9-90e79c4da48c', 'logs': {}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': 'chain'}}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/logs/Docs', 'value': {'end_time': None, 'final_output': None, 'id': '9206e94a-57bd-48ee-8c5e-fdd1c52a6da2', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:55.902+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/logs/Docs/final_output', 'value': {'documents': [Document(page_content='harrison worked at kensho')]}}, {'op': 'add', 'path': '/logs/Docs/end_time', 'value': '2024-01-19T22:33:56.064+00:00'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''}, {'op': 'replace', 'path': '/final_output', 'value': ''}) ---------------------------------------- RunLogPatch({'op':\n",
      "'add', 'path': '/streamed_output/-', 'value': 'H'}, {'op': 'replace', 'path': '/final_output', 'value': 'H'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'arrison'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' worked'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' at'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked at'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Kens'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked at Kens'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ho'}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked at Kensho'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '. '}, {'op': 'replace', 'path': '/final_output', 'value': 'Harrison worked at Kensho.'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''}) Streaming the incremental RunState​ You can simply pass diff=False to get incremental values of RunState.\n",
      "You get more verbose output with more repetitive parts. async for chunk in retrieval_chain astream_log \"where did harrison work?\"\n",
      "include_names \"Docs\" diff False print \"-\" 70 print chunk ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': None, 'final_output': None, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [], 'type': 'chain'})\n",
      "---------------------------------------------------------------------- RunLog({'final_output': '', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': [''], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'H', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H'], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents':\n",
      "[Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison'], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison worked', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked'], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison worked at', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00',\n",
      "'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at'], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison worked at Kens', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens'], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison worked at Kensho', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}},\n",
      "'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho'], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison worked at Kensho. ', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho', '. '], 'type': 'chain'}) ---------------------------------------------------------------------- RunLog({'final_output': 'Harrison worked at Kensho.\n",
      "', 'id': '431d1c55-7c50-48ac-b3a2-2f5ba5f35172', 'logs': {'Docs': {'end_time': '2024-01-19T22:33:57.120+00:00', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '8de10b49-d6af-4cb7-a4e7-fbadf6efa01e', 'metadata': {}, 'name': 'Docs', 'start_time': '2024-01-19T22:33:56.939+00:00', 'streamed_output': [], 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'], 'type': 'retriever'}}, 'name': 'RunnableSequence', 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho', '. ', ''], 'type': 'chain'}) Parallelism​ Let’s take a look at how LangChain Expression Language supports parallel requests. For example, when using a RunnableParallel (often written as a dictionary) it executes each element in parallel. from langchain_core runnables import RunnableParallel chain1 ChatPromptTemplate from_template \"tell me a joke about {topic}\" model chain2 ChatPromptTemplate from_template \"write a short (2 line) poem about {topic}\" model combined RunnableParallel joke chain1 poem chain2 time chain1 invoke \"topic\" \"bears\" CPU times: user 18 ms, sys: 1.27 ms, total: 19.3 ms Wall time: 692 ms AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\") time chain2 invoke \"topic\" \"bears\" CPU times: user 10.5 ms, sys: 166 µs, total: 10.7 ms Wall time: 579 ms AIMessage(content=\"In forest's embrace,\\nMajestic bears pace.\")\n",
      "time combined invoke \"topic\" \"bears\" CPU times: user 32 ms, sys: 2.59 ms, total: 34.6 ms Wall time: 816 ms {'joke': AIMessage(content=\"Sure, here's a bear-related joke for you:\\n\\nWhy did the bear bring a ladder to the bar?\\n\\nBecause he heard the drinks were on the house! \"), 'poem': AIMessage(content=\"In wilderness they roam,\\nMajestic strength, nature's throne.\")} Parallelism on batches​ Parallelism can be combined with other runnables. Let’s try to use parallelism with batches. time chain1 batch \"topic\" \"bears\" \"topic\" \"cats\" CPU times: user 17.3 ms, sys: 4.84 ms, total: 22.2 ms Wall time: 628 ms [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet! \"), AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\")] time chain2 batch \"topic\" \"bears\" \"topic\" \"cats\" CPU times: user 15.8 ms, sys: 3.83 ms, total: 19.7 ms Wall time: 718 ms [AIMessage(content='In the wild, bears roam,\\nMajestic guardians of ancient home. '), AIMessage(content='Whiskers grace, eyes gleam,\\nCats dance through the moonbeam.')] time combined batch \"topic\" \"bears\" \"topic\" \"cats\" CPU times: user 44.8 ms, sys: 3.17 ms, total: 48 ms Wall time: 721 ms [{'joke': AIMessage(content=\"Sure, here's a bear joke for you:\\n\\nWhy don't bears wear shoes?\\n\\nBecause they have bear feet! \"), 'poem': AIMessage(content=\"Majestic bears roam,\\nNature's strength, beauty shown. \")}, {'joke': AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\n",
      "\"), 'poem': AIMessage(content=\"Whiskers dance, eyes aglow,\\nCats embrace the night's gentle flow.\")}] PreviousWhy use LCEL NextStreaming Input Schema Output Schema Stream Invoke Batch Async Stream Async Invoke Async Batch Async Stream Events (beta)Event Reference Async Stream Intermediate StepsStreaming JSONPatch chunksStreaming the incremental RunState ParallelismParallelism on batches\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query #2\n",
    "query_result = client.query.get(\n",
    "    \"Document\",\n",
    "    [\"source\", \"content\"]\n",
    ").with_near_text({\n",
    "    \"concepts\": [\"some text your interested in\"],\n",
    "    \"certainty\": 0.5\n",
    "}).do()\n",
    "\n",
    "for result in query_result['data']['Get']['Document']:\n",
    "    print(f\"Source: {result['source']}\")\n",
    "    print(f\"Content: {result['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c6acd",
   "metadata": {},
   "source": [
    "# Combining the MinIO Dataset Population with the Weaviate Hydration Method (v1)\n",
    "This script does the following:\n",
    "\n",
    "1. Defines helper functions to sanitize URLs and prepare text for tokenization.\n",
    "2. Sets up clients for both MinIO and Weaviate.\n",
    "3. Processes a list of URLs by fetching their content, using the Unstructured library to partition the content into text elements, and storing this text in a MinIO bucket.\n",
    "4. Lists the stored text files in the MinIO bucket, processes each text file to extract the text content again (assuming they need reprocessing, though this might be redundant if they were already processed before storing), and then stores the content in Weaviate.\n",
    "\n",
    "This combined script handles the entire workflow from fetching URL content to storing processed text in Weaviate, making it a comprehensive solution. Remember to replace placeholder URLs and configurations with your actual data and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c0b5c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\weaviate\\warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from minio import Minio\n",
    "import weaviate\n",
    "import os\n",
    "import tempfile\n",
    "import re\n",
    "from unstructured.partition.auto import partition\n",
    "import io\n",
    "\n",
    "# Setup for MinIO and Weaviate\n",
    "minio_client = Minio(\"192.168.0.25:9000\", access_key=\"cda_cdaprod\", secret_key=\"cda_cdaprod\", secure=False)\n",
    "client = weaviate.Client(\"http://192.168.0.25:8080\")\n",
    "bucket_name = \"testtesttest\"\n",
    "\n",
    "def sanitize_url_to_object_name(url):\n",
    "    clean_url = re.sub(r'^https?://', '', url)\n",
    "    clean_url = re.sub(r'[^\\w\\-_\\.]', '_', clean_url)\n",
    "    return clean_url[:250] + '.txt'\n",
    "\n",
    "def prepare_text_for_tokenization(text):\n",
    "    clean_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return clean_text\n",
    "\n",
    "urls = [\n",
    "    \"https://nanonets.com/blog/langchain/amp/\",\n",
    "    \"https://www.sitepoint.com/langchain-python-complete-guide/\",\n",
    "    \"https://medium.com/@aisagescribe/langchain-101-a-comprehensive-introduction-guide-7a5db81afa49\",\n",
    "    \"https://blog.min.io/minio-langchain-tool\",\n",
    "    \"https://quickaitutorial.com/langgraph-create-your-hyper-ai-agent/\",\n",
    "    \"https://python.langchain.com/docs/langserve\",\n",
    "    \"https://python.langchain.com/docs/expression_language/interface\",\n",
    "    \"https://blog.min.io/minio-langchain-tool\",\n",
    "    \"https://python.langchain.com/docs/langgraph\",\n",
    "    \"https://www.33rdsquare.com/langchain/\",\n",
    "    \"https://medium.com/widle-studio/building-ai-solutions-with-langchain-and-node-js-a-comprehensive-guide-widle-studio-4812753aedff\", \"https://blog.min.io/\", \"https://sanity.cdaprod.dev/\"]\n",
    "\n",
    "# Ensure the bucket exists\n",
    "if not minio_client.bucket_exists(bucket_name):\n",
    "    minio_client.make_bucket(bucket_name)\n",
    "\n",
    "# Process URLs: Fetch content, partition, and store in MinIO\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        html_content = io.BytesIO(response.content)\n",
    "        elements = partition(file=html_content, content_type=\"text/html\")\n",
    "        combined_text = \"\\n\".join([e.text for e in elements if hasattr(e, 'text')])\n",
    "        combined_text = prepare_text_for_tokenization(combined_text)\n",
    "        object_name = sanitize_url_to_object_name(url)\n",
    "        # Temporary storage\n",
    "        with tempfile.NamedTemporaryFile(delete=False, mode=\"w\", encoding=\"utf-8\", suffix=\".txt\") as tmp_file:\n",
    "            tmp_file.write(combined_text)\n",
    "            tmp_file_path = tmp_file.name\n",
    "        # Upload to MinIO and remove the temporary file\n",
    "        minio_client.fput_object(bucket_name, object_name, tmp_file_path)\n",
    "        os.remove(tmp_file_path)\n",
    "\n",
    "# List, process, and store in Weaviate\n",
    "for obj in minio_client.list_objects(bucket_name, recursive=True):\n",
    "    if obj.object_name.endswith('.txt'):\n",
    "        file_path = obj.object_name\n",
    "        minio_client.fget_object(bucket_name, obj.object_name, file_path)\n",
    "        elements = partition(filename=file_path)\n",
    "        text_content = \"\\n\".join([e.text for e in elements if hasattr(e, 'text')])\n",
    "        # Store in Weaviate\n",
    "        data_object = {\"source\": obj.object_name, \"content\": text_content}\n",
    "        client.data_object.create(data_object, \"Document\")\n",
    "        os.remove(file_path)  # Clean up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92c4d0",
   "metadata": {},
   "source": [
    "# Combining the MinIO Dataset Population with the Weaviate Hydration Method (v2)\n",
    "### This enhanced version provides clear feedback at each:\n",
    "- initializing clients\n",
    "- processing URLs\n",
    "- storing data in MinIO\n",
    "- inserting documents into Weaviate. \n",
    "\n",
    "It’s designed to keep you informed about the script’s progress and any issues encountered along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "697843f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinIO client initialized.\n",
      "Weaviate client initialized.\n",
      "Fetching URL: https://nanonets.com/blog/langchain/amp/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\weaviate\\warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'nanonets.com_blog_langchain_amp_.txt' in MinIO bucket 'cda-datasets'.\n",
      "Fetching URL: https://www.sitepoint.com/langchain-python-complete-guide/\n",
      "Stored 'www.sitepoint.com_langchain-python-complete-guide_.txt' in MinIO bucket 'cda-datasets'.\n",
      "Fetching URL: https://medium.com/@aisagescribe/langchain-101-a-comprehensive-introduction-guide-7a5db81afa49\n",
      "Stored 'medium.com__aisagescribe_langchain-101-a-comprehensive-introduction-guide-7a5db81afa49.txt' in MinIO bucket 'cda-datasets'.\n",
      "Fetching URL: https://blog.min.io/minio-langchain-tool\n",
      "Stored 'blog.min.io_minio-langchain-tool.txt' in MinIO bucket 'cda-datasets'.\n",
      "Fetching URL: https://quickaitutorial.com/langgraph-create-your-hyper-ai-agent/\n",
      "Stored 'quickaitutorial.com_langgraph-create-your-hyper-ai-agent_.txt' in MinIO bucket 'cda-datasets'.\n",
      "Fetching URL: https://python.langchain.com/docs/langserve\n",
      "Stored 'python.langchain.com_docs_langserve.txt' in MinIO bucket 'cda-datasets'.\n",
      "Fetching URL: https://python.langchain.com/docs/expression_language/interface\n",
      "Stored 'python.langchain.com_docs_expression_language_interface.txt' in MinIO bucket 'cda-datasets'.\n",
      "Fetching URL: https://blog.min.io/minio-langchain-tool\n",
      "Stored 'blog.min.io_minio-langchain-tool.txt' in MinIO bucket 'cda-datasets'.\n",
      "Fetching URL: https://python.langchain.com/docs/langgraph\n",
      "Stored 'python.langchain.com_docs_langgraph.txt' in MinIO bucket 'cda-datasets'.\n",
      "Fetching URL: https://www.33rdsquare.com/langchain/\n",
      "Stored 'www.33rdsquare.com_langchain_.txt' in MinIO bucket 'cda-datasets'.\n",
      "Fetching URL: https://medium.com/widle-studio/building-ai-solutions-with-langchain-and-node-js-a-comprehensive-guide-widle-studio-4812753aedff\n",
      "Stored 'medium.com_widle-studio_building-ai-solutions-with-langchain-and-node-js-a-comprehensive-guide-widle-studio-4812753aedff.txt' in MinIO bucket 'cda-datasets'.\n",
      "Fetching URL: https://blog.min.io/\n",
      "Stored 'blog.min.io_.txt' in MinIO bucket 'cda-datasets'.\n",
      "Fetching URL: https://sanity.cdaprod.dev/\n",
      "Stored 'sanity.cdaprod.dev_.txt' in MinIO bucket 'cda-datasets'.\n",
      "Processing document: blog.min.io_.txt\n",
      "Inserted document 'blog.min.io_.txt' into Weaviate.\n",
      "Processing document: blog.min.io_author_david-cannan_.txt\n",
      "Inserted document 'blog.min.io_author_david-cannan_.txt' into Weaviate.\n",
      "Processing document: blog.min.io_minio-langchain-tool.txt\n",
      "Inserted document 'blog.min.io_minio-langchain-tool.txt' into Weaviate.\n",
      "Processing document: medium.com__aisagescribe_langchain-101-a-comprehensive-introduction-guide-7a5db81afa49.txt\n",
      "Inserted document 'medium.com__aisagescribe_langchain-101-a-comprehensive-introduction-guide-7a5db81afa49.txt' into Weaviate.\n",
      "Processing document: medium.com_widle-studio_building-ai-solutions-with-langchain-and-node-js-a-comprehensive-guide-widle-studio-4812753aedff.txt\n",
      "Inserted document 'medium.com_widle-studio_building-ai-solutions-with-langchain-and-node-js-a-comprehensive-guide-widle-studio-4812753aedff.txt' into Weaviate.\n",
      "Processing document: nanonets.com_blog_langchain_amp_.txt\n",
      "Inserted document 'nanonets.com_blog_langchain_amp_.txt' into Weaviate.\n",
      "Processing document: python.langchain.com_docs_expression_language_interface.txt\n",
      "Inserted document 'python.langchain.com_docs_expression_language_interface.txt' into Weaviate.\n",
      "Processing document: python.langchain.com_docs_langgraph.txt\n",
      "Inserted document 'python.langchain.com_docs_langgraph.txt' into Weaviate.\n",
      "Processing document: python.langchain.com_docs_langserve.txt\n",
      "Inserted document 'python.langchain.com_docs_langserve.txt' into Weaviate.\n",
      "Processing document: quickaitutorial.com_langgraph-create-your-hyper-ai-agent_.txt\n",
      "Inserted document 'quickaitutorial.com_langgraph-create-your-hyper-ai-agent_.txt' into Weaviate.\n",
      "Processing document: sanity.cdaprod.dev_.txt\n",
      "Inserted document 'sanity.cdaprod.dev_.txt' into Weaviate.\n",
      "Processing document: www.33rdsquare.com_langchain_.txt\n",
      "Inserted document 'www.33rdsquare.com_langchain_.txt' into Weaviate.\n",
      "Processing document: www.sitepoint.com_langchain-python-complete-guide_.txt\n",
      "Inserted document 'www.sitepoint.com_langchain-python-complete-guide_.txt' into Weaviate.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from minio import Minio\n",
    "import weaviate\n",
    "import os\n",
    "import tempfile\n",
    "import re\n",
    "from unstructured.partition.auto import partition\n",
    "import io\n",
    "\n",
    "# Setup for MinIO and Weaviate\n",
    "minio_client = Minio(\"192.168.0.25:9000\", access_key=\"cda_cdaprod\", secret_key=\"cda_cdaprod\", secure=False)\n",
    "print(\"MinIO client initialized.\")\n",
    "\n",
    "client = weaviate.Client(\"http://192.168.0.25:8080\")\n",
    "print(\"Weaviate client initialized.\")\n",
    "\n",
    "bucket_name = \"cda-datasets\"\n",
    "\n",
    "def sanitize_url_to_object_name(url):\n",
    "    clean_url = re.sub(r'^https?://', '', url)\n",
    "    clean_url = re.sub(r'[^\\w\\-_\\.]', '_', clean_url)\n",
    "    return clean_url[:250] + '.txt'\n",
    "\n",
    "def prepare_text_for_tokenization(text):\n",
    "    clean_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return clean_text\n",
    "\n",
    "urls = [\n",
    "    \"https://nanonets.com/blog/langchain/amp/\",\n",
    "    \"https://www.sitepoint.com/langchain-python-complete-guide/\",\n",
    "    \"https://medium.com/@aisagescribe/langchain-101-a-comprehensive-introduction-guide-7a5db81afa49\",\n",
    "    \"https://blog.min.io/minio-langchain-tool\",\n",
    "    \"https://quickaitutorial.com/langgraph-create-your-hyper-ai-agent/\",\n",
    "    \"https://python.langchain.com/docs/langserve\",\n",
    "    \"https://python.langchain.com/docs/expression_language/interface\",\n",
    "    \"https://blog.min.io/minio-langchain-tool\",\n",
    "    \"https://python.langchain.com/docs/langgraph\",\n",
    "    \"https://www.33rdsquare.com/langchain/\",\n",
    "    \"https://medium.com/widle-studio/building-ai-solutions-with-langchain-and-node-js-a-comprehensive-guide-widle-studio-4812753aedff\", \"https://blog.min.io/\", \"https://sanity.cdaprod.dev/\"]\n",
    "\n",
    "if not minio_client.bucket_exists(bucket_name):\n",
    "    minio_client.make_bucket(bucket_name)\n",
    "    print(f\"Bucket '{bucket_name}' created.\")\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"Fetching URL: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for HTTP issues\n",
    "\n",
    "        html_content = io.BytesIO(response.content)\n",
    "        elements = partition(file=html_content, content_type=\"text/html\")\n",
    "        combined_text = \"\\n\".join([e.text for e in elements if hasattr(e, 'text')])\n",
    "        combined_text = prepare_text_for_tokenization(combined_text)\n",
    "        object_name = sanitize_url_to_object_name(url)\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(delete=False, mode=\"w\", encoding=\"utf-8\", suffix=\".txt\") as tmp_file:\n",
    "            tmp_file.write(combined_text)\n",
    "            tmp_file_path = tmp_file.name\n",
    "        \n",
    "        minio_client.fput_object(bucket_name, object_name, tmp_file_path)\n",
    "        print(f\"Stored '{object_name}' in MinIO bucket '{bucket_name}'.\")\n",
    "        os.remove(tmp_file_path)  # Clean up\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch URL {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "for obj in minio_client.list_objects(bucket_name, recursive=True):\n",
    "    if obj.object_name.endswith('.txt'):\n",
    "        print(f\"Processing document: {obj.object_name}\")\n",
    "        file_path = obj.object_name\n",
    "        minio_client.fget_object(bucket_name, obj.object_name, file_path)\n",
    "        \n",
    "        elements = partition(filename=file_path)\n",
    "        text_content = \"\\n\".join([e.text for e in elements if hasattr(e, 'text')])\n",
    "        \n",
    "        data_object = {\"source\": obj.object_name, \"content\": text_content}\n",
    "        client.data_object.create(data_object, \"Document\")\n",
    "        print(f\"Inserted document '{obj.object_name}' into Weaviate.\")\n",
    "        \n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce6259",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Markdown Graveyard\n",
    "\n",
    "---\n",
    "\n",
    "# Contextual Stuff\n",
    "\n",
    "## Integrating Cloud Storage and Knowledge Graphs for Enhanced Data Management\n",
    "\n",
    "In the rapidly evolving digital landscape, the efficient management and processing of data have become paramount for businesses and researchers alike. This article explores a proof of concept (POC) that showcases the integration of cloud storage solutions and knowledge graphs to streamline the management of unstructured data. By leveraging technologies such as MinIO, Weaviate, and advanced text processing libraries, this POC demonstrates a scalable and effective approach to data handling.\n",
    "\n",
    "## Overview of the Solution\n",
    "\n",
    "The POC employs a Python script to fetch, process, and store textual data from specified URLs into MinIO, an open-source object storage service, and subsequently indexes this data in Weaviate, a knowledge graph designed for scalable and fast data storage and retrieval. The process involves several steps, starting from data acquisition to its transformation and storage, highlighting the potential of integrating different technologies to enhance data management capabilities.\n",
    "\n",
    "### Step 1: Setting Up MinIO and Weaviate Clients\n",
    "\n",
    "The initial step involves initializing clients for both MinIO and Weaviate, connecting to their respective servers. This setup is crucial for enabling subsequent operations like data storage and indexing.\n",
    "\n",
    "```python\n",
    "minio_client = Minio(\"192.168.0.25:9000\", access_key=\"cda_cdaprod\", secret_key=\"cda_cdaprod\", secure=False)\n",
    "client = weaviate.Client(\"http://192.168.0.25:8080\")\n",
    "```\n",
    "\n",
    "### Step 2: Data Fetching and Pre-processing\n",
    "\n",
    "The script fetches textual content from predefined URLs, processes this content to remove HTML tags, and prepares it for storage. This step is vital for converting raw HTML data into a more structured and usable text format.\n",
    "\n",
    "### Step 3: Storage in MinIO\n",
    "\n",
    "Once the data is processed, it's stored in a specified bucket in MinIO. This involves sanitizing the URL to a valid object name, writing the processed text to a temporary file, and then uploading this file to MinIO. This step demonstrates the flexibility and ease of using MinIO for handling large volumes of data.\n",
    "\n",
    "### Step 4: Indexing in Weaviate\n",
    "\n",
    "After storing the documents in MinIO, the script indexes them in Weaviate. This involves reading the stored documents, further processing the text, and creating data objects in Weaviate's knowledge graph. This step highlights Weaviate's capability to manage and search through large datasets efficiently.\n",
    "\n",
    "## Benefits and Applications\n",
    "\n",
    "The integration of MinIO and Weaviate offers numerous benefits, including scalable storage, efficient data retrieval, and the ability to handle unstructured data effectively. This POC illustrates not just a technical implementation but also a strategic approach to managing data in a way that enhances accessibility, searchability, and usability.\n",
    "\n",
    "Such a system could be invaluable for organizations dealing with large datasets, researchers requiring efficient data retrieval methods, or businesses looking to implement advanced data analysis and management solutions.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This proof of concept highlights the synergy between cloud storage and knowledge graphs, offering a glimpse into the future of data management. By leveraging the strengths of MinIO for storage and Weaviate for data indexing and retrieval, organizations can achieve a more streamlined and efficient data management process. This POC not only demonstrates the technical feasibility of such an integration but also underscores the potential benefits for businesses and researchers in managing and analyzing data at scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
